#!/usr/bin/env python3
"""
ê³µê²©ì  ìµœì í™” ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ - ë°ì´í„°ë² ì´ìŠ¤ ì—†ì´ ìˆœìˆ˜ í¬ë¡¤ë§ ì„±ëŠ¥ë§Œ ì¸¡ì •
"""

import asyncio
import logging
import time
from datetime import datetime
from typing import List
import sys

# Config import
try:
    from config.settings import (
        GLOBAL_SEMAPHORE_LIMIT,
        TCP_CONNECTOR_LIMIT,
        TCP_CONNECTOR_LIMIT_PER_HOST,
        DEFAULT_CRAWL_DELAY,
        REQUEST_TIMEOUT
    )
except ImportError:
    GLOBAL_SEMAPHORE_LIMIT = 200
    TCP_CONNECTOR_LIMIT = 300
    TCP_CONNECTOR_LIMIT_PER_HOST = 20
    DEFAULT_CRAWL_DELAY = 0.5
    REQUEST_TIMEOUT = 10

from polite_crawler import PoliteCrawler
from tranco_manager import TrancoManager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

async def run_performance_test(url_count: int):
    """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    logger.info("="*80)
    logger.info(f"ê³µê²©ì  ìµœì í™” ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘ - {url_count}ê°œ URL")
    logger.info("="*80)

    # ì„¤ì • ì¶œë ¥
    logger.info("\ní˜„ì¬ ìµœì í™” ì„¤ì •:")
    logger.info(f"  - Global Semaphore: {GLOBAL_SEMAPHORE_LIMIT}")
    logger.info(f"  - TCP Connector Limit: {TCP_CONNECTOR_LIMIT}")
    logger.info(f"  - TCP Limit Per Host: {TCP_CONNECTOR_LIMIT_PER_HOST}")
    logger.info(f"  - Default Crawl Delay: {DEFAULT_CRAWL_DELAY}s")
    logger.info(f"  - Request Timeout: {REQUEST_TIMEOUT}s")
    logger.info("")

    # URL ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„ (ë¡œì»¬ íŒŒì¼ ì‚¬ìš©)
    logger.info("1. URL ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„ ì¤‘...")
    url_file = "data/tranco_top_500_urls.txt"

    try:
        with open(url_file, 'r') as f:
            all_urls = [line.strip() for line in f if line.strip()]

        # ìš”ì²­ëœ ìˆ˜ë§Œí¼ë§Œ ì„ íƒ (ìµœëŒ€ 500ê°œ)
        urls = all_urls[:min(url_count, len(all_urls))]
        logger.info(f"   âœ“ {len(urls)}ê°œ URL ì¤€ë¹„ ì™„ë£Œ (from {url_file})")
    except Exception as e:
        logger.error(f"URL íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        return None

    # PoliteCrawler ì´ˆê¸°í™”
    logger.info("2. ê³µê²©ì  í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì¤‘...")
    async with PoliteCrawler(respect_robots_txt=False) as crawler:
        logger.info("   âœ“ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")

        # í¬ë¡¤ë§ ì‹œì‘
        logger.info(f"\n3. í¬ë¡¤ë§ ì‹œì‘ ({len(urls)}ê°œ URL)...")
        start_time = time.time()
        start_datetime = datetime.now()

        results = await crawler.crawl_batch_politely(urls)

        end_time = time.time()
        end_datetime = datetime.now()
        duration = end_time - start_time

        # í†µê³„ ê³„ì‚°
        successful = sum(1 for r in results if r['success'])
        failed = len(results) - successful
        success_rate = (successful / len(results)) * 100 if results else 0
        pages_per_second = len(results) / duration if duration > 0 else 0
        pages_per_minute = pages_per_second * 60

        # ê²°ê³¼ ì¶œë ¥
        logger.info("\n" + "="*80)
        logger.info("ê³µê²©ì  ìµœì í™” ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼")
        logger.info("="*80)
        logger.info(f"\nğŸ“Š ê¸°ë³¸ í†µê³„:")
        logger.info(f"  ì‹œì‘ ì‹œê°„         : {start_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"  ì¢…ë£Œ ì‹œê°„         : {end_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"  ì´ ì‹¤í–‰ ì‹œê°„      : {duration:.2f}ì´ˆ ({duration/60:.2f}ë¶„)")
        logger.info(f"  ì´ URL ìˆ˜         : {len(results)}ê°œ")
        logger.info(f"  ì„±ê³µ              : {successful}ê°œ")
        logger.info(f"  ì‹¤íŒ¨              : {failed}ê°œ")
        logger.info(f"  ì„±ê³µë¥             : {success_rate:.1f}%")

        logger.info(f"\nğŸš€ ì„±ëŠ¥ ì§€í‘œ:")
        logger.info(f"  ì²˜ë¦¬ ì†ë„         : {pages_per_second:.2f} pages/sec")
        logger.info(f"  ì²˜ë¦¬ ì†ë„         : {pages_per_minute:.1f} pages/min")
        logger.info(f"  í‰ê·  ì‘ë‹µ ì‹œê°„    : {duration/len(results):.3f}ì´ˆ/URL")

        # í¬ë¡¤ëŸ¬ í†µê³„
        crawler_summary = crawler.get_crawling_summary()
        logger.info(f"\nğŸŒ í¬ë¡¤ëŸ¬ í†µê³„:")
        logger.info(f"  ì²˜ë¦¬ ë„ë©”ì¸ ìˆ˜    : {crawler_summary.get('total_domains', 0)}ê°œ")
        logger.info(f"  ì´ ìš”ì²­ ìˆ˜        : {crawler_summary.get('total_requests', 0)}ê°œ")
        logger.info(f"  ì´ ì—ëŸ¬ ìˆ˜        : {crawler_summary.get('total_errors', 0)}ê°œ")
        logger.info(f"  ì°¨ë‹¨ëœ ë„ë©”ì¸     : {crawler_summary.get('blocked_domains', 0)}ê°œ")
        logger.info(f"  ì—ëŸ¬ìœ¨            : {crawler_summary.get('error_rate', 0):.1%}")
        logger.info(f"  í‰ê·  ë”œë ˆì´       : {crawler_summary.get('average_delay', 0):.2f}ì´ˆ")

        # ì—ëŸ¬ ë¶„ì„
        if failed > 0:
            logger.info(f"\nâš ï¸  ì—ëŸ¬ ë¶„ì„:")
            error_types = {}
            for result in results:
                if not result['success']:
                    error = result.get('error', 'Unknown')
                    error_type = error.split(':')[0] if ':' in error else error
                    error_types[error_type] = error_types.get(error_type, 0) + 1

            for error_type, count in sorted(error_types.items(), key=lambda x: x[1], reverse=True):
                logger.info(f"  {error_type}: {count}ê°œ ({count/failed*100:.1f}%)")

        logger.info("\n" + "="*80)

        return {
            'url_count': len(results),
            'successful': successful,
            'failed': failed,
            'success_rate': success_rate,
            'duration_seconds': duration,
            'pages_per_second': pages_per_second,
            'pages_per_minute': pages_per_minute,
            'total_domains': crawler_summary.get('total_domains', 0),
            'settings': {
                'semaphore': GLOBAL_SEMAPHORE_LIMIT,
                'tcp_limit': TCP_CONNECTOR_LIMIT,
                'tcp_per_host': TCP_CONNECTOR_LIMIT_PER_HOST,
                'crawl_delay': DEFAULT_CRAWL_DELAY,
                'timeout': REQUEST_TIMEOUT
            }
        }

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(description='Aggressive Performance Test')
    parser.add_argument('--count', type=int, default=100,
                       help='í…ŒìŠ¤íŠ¸í•  URL ê°œìˆ˜ (ê¸°ë³¸: 100)')

    args = parser.parse_args()

    try:
        result = await run_performance_test(args.count)

        if result:
            # ê°„ë‹¨í•œ ìš”ì•½
            print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
            print(f"   {result['url_count']}ê°œ URLì„ {result['duration_seconds']:.1f}ì´ˆì— ì²˜ë¦¬")
            print(f"   ì„±ëŠ¥: {result['pages_per_second']:.2f} pages/sec")
            print(f"   ì„±ê³µë¥ : {result['success_rate']:.1f}%")

            # JSON í˜•íƒœë¡œë„ ì¶œë ¥
            import json
            with open(f'aggressive_test_{args.count}_urls.json', 'w') as f:
                json.dump(result, f, indent=2)
            print(f"\nğŸ“ ìƒì„¸ ê²°ê³¼ê°€ aggressive_test_{args.count}_urls.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

            return 0
        else:
            return 1

    except Exception as e:
        logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}", exc_info=True)
        return 1

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
