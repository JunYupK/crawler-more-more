import asyncio
import aiohttp
import time
from typing import Dict, List, Optional, Tuple, Set
from urllib.parse import urljoin, urlparse
from urllib.robotparser import RobotFileParser
from datetime import datetime, timedelta
import logging
import re
from dataclasses import dataclass, field

logger = logging.getLogger(__name__)

@dataclass
class RobotRules:
    """robots.txt ê·œì¹™"""
    can_fetch: bool = True
    crawl_delay: int = 1  # ê¸°ë³¸ 1ì´ˆ
    last_accessed: datetime = field(default_factory=datetime.now)
    rules_text: str = ""
    user_agent: str = "*"

@dataclass
class DomainState:
    """ë„ë©”ì¸ë³„ ìƒíƒœ ê´€ë¦¬"""
    last_access: datetime = field(default_factory=datetime.now)
    request_count: int = 0
    error_count: int = 0
    crawl_delay: int = 1
    robots_checked: bool = False
    robots_rules: Optional[RobotRules] = None
    is_blocked: bool = False
    next_allowed_time: datetime = field(default_factory=datetime.now)

class PoliteCrawler:
    """ì •ì¤‘í•œ í¬ë¡¤ë§ì„ ìœ„í•œ robots.txt ì¤€ìˆ˜ í¬ë¡¤ëŸ¬"""

    def __init__(self):
        self.domain_states: Dict[str, DomainState] = {}
        self.user_agent = "PoliteCrawler/1.0 (+https://example.com/bot)"

        # ê¸°ë³¸ í¬ë¡¤ë§ ì •ì±…
        self.default_delay = 2  # ê¸°ë³¸ 2ì´ˆ ë”œë ˆì´
        self.max_delay = 30     # ìµœëŒ€ 30ì´ˆ ë”œë ˆì´
        self.respect_robots = True
        self.max_errors_per_domain = 10

        # ê¸€ë¡œë²Œ ë ˆì´íŠ¸ ë¦¬ë¯¸í„°
        self.global_semaphore = asyncio.Semaphore(20)  # ë™ì‹œ ìš”ì²­ 20ê°œ
        self.session: Optional[aiohttp.ClientSession] = None

    async def __aenter__(self):
        """Context manager ì§„ì…"""
        connector = aiohttp.TCPConnector(
            limit=100,
            limit_per_host=5,  # ë„ë©”ì¸ë‹¹ ìµœëŒ€ 5ê°œ ì—°ê²°
            ttl_dns_cache=300,
            use_dns_cache=True,
            keepalive_timeout=60,
            enable_cleanup_closed=True
        )

        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=aiohttp.ClientTimeout(total=30),
            headers={
                'User-Agent': self.user_agent,
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'DNT': '1'
            }
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Context manager ì¢…ë£Œ"""
        if self.session:
            await self.session.close()

    async def check_robots_txt(self, domain: str) -> RobotRules:
        """robots.txt í™•ì¸"""
        try:
            robots_url = f"https://{domain}/robots.txt"

            # HTTP ì—°ê²° ì‹¤íŒ¨ì‹œ HTTPSë¡œ ì‹œë„
            try:
                async with self.session.get(robots_url) as response:
                    if response.status == 200:
                        robots_content = await response.text()
                    else:
                        robots_content = ""
            except:
                # HTTPS ì‹¤íŒ¨ì‹œ HTTPë¡œ ì‹œë„
                robots_url = f"http://{domain}/robots.txt"
                try:
                    async with self.session.get(robots_url) as response:
                        if response.status == 200:
                            robots_content = await response.text()
                        else:
                            robots_content = ""
                except:
                    robots_content = ""

            # robots.txt íŒŒì‹±
            rules = self._parse_robots_txt(robots_content, domain)
            logger.debug(f"robots.txt í™•ì¸ ì™„ë£Œ: {domain} (ë”œë ˆì´: {rules.crawl_delay}ì´ˆ)")

            return rules

        except Exception as e:
            logger.warning(f"robots.txt í™•ì¸ ì‹¤íŒ¨ ({domain}): {e}")
            return RobotRules(can_fetch=True, crawl_delay=self.default_delay)

    def _parse_robots_txt(self, robots_content: str, domain: str) -> RobotRules:
        """robots.txt ë‚´ìš© íŒŒì‹±"""
        try:
            if not robots_content.strip():
                return RobotRules(can_fetch=True, crawl_delay=self.default_delay)

            # Pythonì˜ robotparser ì‚¬ìš©
            rp = RobotFileParser()
            rp.set_url(f"https://{domain}/robots.txt")
            rp.read()

            # í¬ë¡¤ë§ í—ˆìš© ì—¬ë¶€ í™•ì¸
            can_fetch = rp.can_fetch(self.user_agent, "/")

            # Crawl-delay ì¶”ì¶œ
            crawl_delay = self.default_delay

            # robots.txtì—ì„œ crawl-delay ì§ì ‘ íŒŒì‹±
            crawl_delay_match = re.search(r'crawl-delay:\s*(\d+)', robots_content.lower())
            if crawl_delay_match:
                crawl_delay = min(int(crawl_delay_match.group(1)), self.max_delay)

            # ì¼ë¶€ ì‚¬ì´íŠ¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë‹¤ë¥¸ í˜•ì‹ë“¤
            delay_patterns = [
                r'request-rate:\s*1/(\d+)',  # request-rate: 1/10 (10ì´ˆë§ˆë‹¤ 1íšŒ)
                r'visit-time:\s*(\d+)',     # visit-time: 5
            ]

            for pattern in delay_patterns:
                match = re.search(pattern, robots_content.lower())
                if match:
                    crawl_delay = max(crawl_delay, min(int(match.group(1)), self.max_delay))

            return RobotRules(
                can_fetch=can_fetch,
                crawl_delay=crawl_delay,
                rules_text=robots_content[:500],  # ì²˜ìŒ 500ìë§Œ ì €ì¥
                user_agent=self.user_agent
            )

        except Exception as e:
            logger.warning(f"robots.txt íŒŒì‹± ì‹¤íŒ¨ ({domain}): {e}")
            return RobotRules(can_fetch=True, crawl_delay=self.default_delay)

    def _get_domain_state(self, domain: str) -> DomainState:
        """ë„ë©”ì¸ ìƒíƒœ íšë“"""
        if domain not in self.domain_states:
            self.domain_states[domain] = DomainState()
        return self.domain_states[domain]

    async def is_allowed_to_fetch(self, url: str) -> Tuple[bool, str]:
        """URL í¬ë¡¤ë§ í—ˆìš© ì—¬ë¶€ í™•ì¸"""
        try:
            parsed_url = urlparse(url)
            domain = parsed_url.netloc

            domain_state = self._get_domain_state(domain)

            # ë„ˆë¬´ ë§ì€ ì—ëŸ¬ ë°œìƒì‹œ ì°¨ë‹¨
            if domain_state.error_count >= self.max_errors_per_domain:
                return False, f"ë„ë©”ì¸ ì—ëŸ¬ í•œë„ ì´ˆê³¼ ({domain_state.error_count})"

            # ì´ë¯¸ ì°¨ë‹¨ëœ ë„ë©”ì¸
            if domain_state.is_blocked:
                return False, "ë„ë©”ì¸ ì°¨ë‹¨ë¨"

            # robots.txt ë¯¸í™•ì¸ì‹œ í™•ì¸
            if not domain_state.robots_checked:
                robots_rules = await self.check_robots_txt(domain)
                domain_state.robots_rules = robots_rules
                domain_state.robots_checked = True
                domain_state.crawl_delay = robots_rules.crawl_delay

            # robots.txt ê·œì¹™ í™•ì¸
            if (domain_state.robots_rules and
                not domain_state.robots_rules.can_fetch):
                domain_state.is_blocked = True
                return False, "robots.txtì— ì˜í•´ ì°¨ë‹¨ë¨"

            # ë”œë ˆì´ í™•ì¸
            now = datetime.now()
            if now < domain_state.next_allowed_time:
                wait_time = (domain_state.next_allowed_time - now).total_seconds()
                return False, f"ë”œë ˆì´ ëŒ€ê¸° ì¤‘ ({wait_time:.1f}ì´ˆ ë‚¨ìŒ)"

            return True, "í—ˆìš©"

        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ í—ˆìš© í™•ì¸ ì‹¤íŒ¨ ({url}): {e}")
            return False, f"í™•ì¸ ì‹¤íŒ¨: {e}"

    async def wait_for_domain_delay(self, domain: str):
        """ë„ë©”ì¸ë³„ ë”œë ˆì´ ëŒ€ê¸°"""
        domain_state = self._get_domain_state(domain)

        now = datetime.now()
        if now < domain_state.next_allowed_time:
            wait_time = (domain_state.next_allowed_time - now).total_seconds()
            logger.debug(f"ë„ë©”ì¸ ë”œë ˆì´ ëŒ€ê¸°: {domain} ({wait_time:.1f}ì´ˆ)")
            await asyncio.sleep(wait_time)

        # ë‹¤ìŒ í—ˆìš© ì‹œê°„ ì„¤ì •
        domain_state.next_allowed_time = datetime.now() + timedelta(seconds=domain_state.crawl_delay)

    async def fetch_url_politely(self, url: str) -> Dict[str, any]:
        """ì •ì¤‘í•œ ë°©ì‹ìœ¼ë¡œ URL í¬ë¡¤ë§"""
        parsed_url = urlparse(url)
        domain = parsed_url.netloc

        try:
            # í¬ë¡¤ë§ í—ˆìš© í™•ì¸
            allowed, reason = await self.is_allowed_to_fetch(url)
            if not allowed:
                return {
                    'url': url,
                    'success': False,
                    'error': f"í¬ë¡¤ë§ ë¶ˆí—ˆ: {reason}",
                    'status': None,
                    'content': None
                }

            # ê¸€ë¡œë²Œ ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ìš”ì²­ ì œí•œ
            async with self.global_semaphore:
                # ë„ë©”ì¸ë³„ ë”œë ˆì´ ëŒ€ê¸°
                await self.wait_for_domain_delay(domain)

                domain_state = self._get_domain_state(domain)

                # ìš”ì²­ ì‹¤í–‰
                async with self.session.get(url) as response:
                    content = await response.text()

                    # ì„±ê³µ ì²˜ë¦¬
                    domain_state.last_access = datetime.now()
                    domain_state.request_count += 1

                    # ìƒíƒœ ì½”ë“œë³„ ì²˜ë¦¬
                    if response.status == 429:  # Too Many Requests
                        # ë”œë ˆì´ ì¦ê°€
                        domain_state.crawl_delay = min(domain_state.crawl_delay * 2, self.max_delay)
                        logger.warning(f"429 ì—ëŸ¬ë¡œ ì¸í•œ ë”œë ˆì´ ì¦ê°€: {domain} ({domain_state.crawl_delay}ì´ˆ)")

                    return {
                        'url': url,
                        'success': True,
                        'status': response.status,
                        'content': content,
                        'content_length': len(content),
                        'domain': domain,
                        'crawl_delay': domain_state.crawl_delay,
                        'headers': dict(response.headers)
                    }

        except asyncio.TimeoutError:
            domain_state = self._get_domain_state(domain)
            domain_state.error_count += 1
            return {
                'url': url,
                'success': False,
                'error': 'Timeout',
                'status': None,
                'content': None,
                'domain': domain
            }

        except aiohttp.ClientError as e:
            domain_state = self._get_domain_state(domain)
            domain_state.error_count += 1

            # ì—°ê²° ì˜¤ë¥˜ê°€ ë§ì„ ê²½ìš° ë”œë ˆì´ ì¦ê°€
            if domain_state.error_count % 3 == 0:
                domain_state.crawl_delay = min(domain_state.crawl_delay * 1.5, self.max_delay)

            return {
                'url': url,
                'success': False,
                'error': f'Client error: {str(e)}',
                'status': None,
                'content': None,
                'domain': domain
            }

        except Exception as e:
            domain_state = self._get_domain_state(domain)
            domain_state.error_count += 1

            logger.error(f"í¬ë¡¤ë§ ì‹¤íŒ¨ ({url}): {e}")
            return {
                'url': url,
                'success': False,
                'error': f'Unexpected error: {str(e)}',
                'status': None,
                'content': None,
                'domain': domain
            }

    async def crawl_batch_politely(self, urls: List[str]) -> List[Dict[str, any]]:
        """URL ë°°ì¹˜ë¥¼ ì •ì¤‘í•˜ê²Œ í¬ë¡¤ë§"""
        logger.info(f"ì •ì¤‘í•œ ë°°ì¹˜ í¬ë¡¤ë§ ì‹œì‘: {len(urls)}ê°œ URL")

        # ë„ë©”ì¸ë³„ ê·¸ë£¹í™”
        domain_groups = {}
        for url in urls:
            domain = urlparse(url).netloc
            if domain not in domain_groups:
                domain_groups[domain] = []
            domain_groups[domain].append(url)

        logger.info(f"ë„ë©”ì¸ë³„ ë¶„ì‚°: {len(domain_groups)}ê°œ ë„ë©”ì¸")

        # ë„ë©”ì¸ë³„ë¡œ ìˆœì°¨ ì²˜ë¦¬, ì „ì²´ì ìœ¼ë¡œëŠ” ë³‘ë ¬ ì²˜ë¦¬
        tasks = []
        for domain, domain_urls in domain_groups.items():
            task = asyncio.create_task(self._crawl_domain_urls(domain, domain_urls))
            tasks.append(task)

        # ëª¨ë“  ë„ë©”ì¸ í¬ë¡¤ë§ ì™„ë£Œ ëŒ€ê¸°
        domain_results = await asyncio.gather(*tasks)

        # ê²°ê³¼ í•©ì¹˜ê¸°
        all_results = []
        for domain_result in domain_results:
            all_results.extend(domain_result)

        logger.info(f"ì •ì¤‘í•œ ë°°ì¹˜ í¬ë¡¤ë§ ì™„ë£Œ: {len(all_results)}ê°œ ê²°ê³¼")
        return all_results

    async def _crawl_domain_urls(self, domain: str, urls: List[str]) -> List[Dict[str, any]]:
        """ë‹¨ì¼ ë„ë©”ì¸ì˜ URLë“¤ì„ ìˆœì°¨ í¬ë¡¤ë§"""
        results = []

        logger.debug(f"ë„ë©”ì¸ í¬ë¡¤ë§ ì‹œì‘: {domain} ({len(urls)}ê°œ URL)")

        for url in urls:
            try:
                result = await self.fetch_url_politely(url)
                results.append(result)

                # ì„±ê³µ/ì‹¤íŒ¨ ë¡œê¹…
                if result['success']:
                    logger.debug(f"âœ… {url} - {result['status']}")
                else:
                    logger.debug(f"âŒ {url} - {result['error']}")

            except Exception as e:
                logger.error(f"ë„ë©”ì¸ í¬ë¡¤ë§ ì˜¤ë¥˜ ({url}): {e}")
                results.append({
                    'url': url,
                    'success': False,
                    'error': f'Domain crawling error: {str(e)}',
                    'status': None,
                    'content': None,
                    'domain': domain
                })

        logger.debug(f"ë„ë©”ì¸ í¬ë¡¤ë§ ì™„ë£Œ: {domain}")
        return results

    def get_domain_stats(self) -> Dict[str, Dict[str, any]]:
        """ë„ë©”ì¸ë³„ í†µê³„"""
        stats = {}
        for domain, state in self.domain_states.items():
            stats[domain] = {
                'request_count': state.request_count,
                'error_count': state.error_count,
                'crawl_delay': state.crawl_delay,
                'is_blocked': state.is_blocked,
                'robots_checked': state.robots_checked,
                'can_fetch': state.robots_rules.can_fetch if state.robots_rules else True,
                'last_access': state.last_access.isoformat() if state.last_access else None
            }
        return stats

    def get_crawling_summary(self) -> Dict[str, any]:
        """í¬ë¡¤ë§ ìš”ì•½ í†µê³„"""
        total_requests = sum(state.request_count for state in self.domain_states.values())
        total_errors = sum(state.error_count for state in self.domain_states.values())
        blocked_domains = sum(1 for state in self.domain_states.values() if state.is_blocked)

        return {
            'total_domains': len(self.domain_states),
            'total_requests': total_requests,
            'total_errors': total_errors,
            'blocked_domains': blocked_domains,
            'error_rate': total_errors / total_requests if total_requests > 0 else 0,
            'average_delay': sum(state.crawl_delay for state in self.domain_states.values()) / len(self.domain_states) if self.domain_states else 0
        }

async def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    logging.basicConfig(level=logging.INFO)

    test_urls = [
        "https://google.com/",
        "https://github.com/",
        "https://stackoverflow.com/",
        "https://reddit.com/",
        "https://wikipedia.org/"
    ]

    async with PoliteCrawler() as crawler:
        print("ğŸ¤– ì •ì¤‘í•œ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹œì‘")

        results = await crawler.crawl_batch_politely(test_urls)

        print(f"\nğŸ“Š í¬ë¡¤ë§ ê²°ê³¼:")
        successful = sum(1 for r in results if r['success'])
        print(f"  ì„±ê³µ: {successful}/{len(results)}")

        for result in results:
            status = "âœ…" if result['success'] else "âŒ"
            print(f"  {status} {result['url']} - {result.get('status', result.get('error'))}")

        print(f"\nğŸŒ ë„ë©”ì¸ í†µê³„:")
        domain_stats = crawler.get_domain_stats()
        for domain, stats in domain_stats.items():
            print(f"  {domain}: {stats['request_count']}íšŒ ìš”ì²­, {stats['crawl_delay']}ì´ˆ ë”œë ˆì´")

        print(f"\nğŸ“ˆ ì „ì²´ ìš”ì•½:")
        summary = crawler.get_crawling_summary()
        for key, value in summary.items():
            print(f"  {key}: {value}")

if __name__ == "__main__":
    asyncio.run(main())