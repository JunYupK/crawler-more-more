#!/usr/bin/env python3
"""
Fast Processor Runner - BeautifulSoup ì›Œì»¤ ì‹¤í–‰ê¸°
=================================================

Desktopì—ì„œ ì‹¤í–‰í•˜ì—¬ process.fast í† í”½ì˜ ì •ì  í˜ì´ì§€ë¥¼ ì²˜ë¦¬

Usage:
    # ë‹¨ì¼ ì›Œì»¤ ì‹¤í–‰
    python runners/fast_processor_runner.py

    # ë‹¤ì¤‘ ì›Œì»¤ ì‹¤í–‰ (4ê°œ)
    python runners/fast_processor_runner.py --workers 4

    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ
    python runners/fast_processor_runner.py --test --max-messages 100

    # HTML íŒŒì¼ ì§ì ‘ ì²˜ë¦¬
    python runners/fast_processor_runner.py --process-file page.html
"""

import asyncio
import argparse
import logging
import signal
import sys
import time
from pathlib import Path
from typing import Optional

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from src.processor.fast_worker import FastWorker, FastWorkerPool
from src.processor.base_worker import ProcessedResult
from config.kafka_config import get_config

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
)
logger = logging.getLogger(__name__)


class FastProcessorRunner:
    """Fast Processor ì‹¤í–‰ê¸°"""

    def __init__(
        self,
        kafka_servers: Optional[str] = None,
        num_workers: int = 1,
    ):
        self.kafka_servers = kafka_servers or get_config().kafka.bootstrap_servers
        self.num_workers = num_workers

        self._running = False
        self._pool: Optional[FastWorkerPool] = None
        self._worker: Optional[FastWorker] = None

        # ì¢…ë£Œ ì‹œê·¸ë„ í•¸ë“¤ëŸ¬
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)

    def _signal_handler(self, signum, frame):
        """ì¢…ë£Œ ì‹œê·¸ë„ ì²˜ë¦¬"""
        logger.info(f"Received signal {signum}, shutting down...")
        self._running = False

        # ì›Œì»¤ ì¤‘ì§€ í”Œë˜ê·¸ ì„¤ì •
        if self._worker:
            self._worker._running = False
        if self._pool:
            for worker in self._pool.workers:
                worker._running = False

    async def run(self, max_messages: Optional[int] = None) -> None:
        """
        í”„ë¡œì„¸ì„œ ì‹¤í–‰

        Args:
            max_messages: ìµœëŒ€ ì²˜ë¦¬ ë©”ì‹œì§€ ìˆ˜
        """
        self._running = True
        start_time = time.time()
        processed_count = 0

        def on_processed(result: ProcessedResult):
            nonlocal processed_count
            processed_count += 1

            if processed_count % 50 == 0:
                elapsed = time.time() - start_time
                logger.info(
                    f"[Progress] Processed: {processed_count:,} | "
                    f"Rate: {processed_count/elapsed:.1f} msg/s | "
                    f"Success: {result.success}"
                )

        try:
            logger.info(f"Connecting to Kafka: {self.kafka_servers}")
            logger.info(f"Number of workers: {self.num_workers}")

            if self.num_workers > 1:
                # ë‹¤ì¤‘ ì›Œì»¤ ëª¨ë“œ
                self._pool = FastWorkerPool(
                    num_workers=self.num_workers,
                    bootstrap_servers=self.kafka_servers,
                )
                await self._pool.start()

                # ì›Œì»¤ë³„ max_messages ê³„ì‚°
                per_worker = max_messages // self.num_workers if max_messages else None
                await self._pool.run(max_messages_per_worker=per_worker)

                # ìµœì¢… í†µê³„
                stats = self._pool.get_combined_stats()

            else:
                # ë‹¨ì¼ ì›Œì»¤ ëª¨ë“œ
                self._worker = FastWorker(
                    bootstrap_servers=self.kafka_servers,
                    worker_id=0,
                )

                async with self._worker:
                    await self._worker.run(
                        max_messages=max_messages,
                        callback=on_processed,
                    )

                stats = self._worker.get_stats()

            # ìµœì¢… í†µê³„ ì¶œë ¥
            elapsed = time.time() - start_time
            logger.info("=" * 60)
            logger.info("Fast Processor stopped!")
            logger.info(f"Total time: {elapsed:.1f}s")
            logger.info(f"Stats: {stats}")
            logger.info("=" * 60)

        except Exception as e:
            logger.error(f"Error running fast processor: {e}", exc_info=True)

        finally:
            if self._pool:
                await self._pool.stop()

    async def test_connection(self) -> bool:
        """Kafka ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            from aiokafka import AIOKafkaConsumer
            config = get_config()

            consumer = AIOKafkaConsumer(
                config.topics.process_fast,
                bootstrap_servers=self.kafka_servers,
            )
            await consumer.start()
            await consumer.stop()
            logger.info("Kafka connection test: SUCCESS")
            return True
        except Exception as e:
            logger.error(f"Kafka connection test: FAILED - {e}")
            return False


def process_file(file_path: str) -> None:
    """
    HTML íŒŒì¼ ì§ì ‘ ì²˜ë¦¬ (ì˜¤í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ìš©)

    Args:
        file_path: HTML íŒŒì¼ ê²½ë¡œ
    """
    import asyncio

    logger.info(f"Processing file: {file_path}")

    with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
        html = f.read()

    async def process():
        worker = FastWorker(worker_id=0)

        result = await worker.process_html(
            html=html,
            url=f"file://{file_path}",
            metadata={},
        )

        return result

    result = asyncio.run(process())

    print("\n" + "=" * 60)
    print("PROCESSING RESULT")
    print("=" * 60)

    print(f"\nğŸ“Š Success: {result.success}")
    print(f"ğŸ”§ Processor: {result.processor_type.value}")
    print(f"â±ï¸ Processing time: {result.processing_time_ms:.1f}ms")

    print(f"\nğŸ“ Metadata:")
    print(f"   Title: {result.title or 'N/A'}")
    print(f"   Description: {(result.description or 'N/A')[:100]}...")
    print(f"   Language: {result.language or 'N/A'}")

    print(f"\nğŸ“ˆ Content Stats:")
    print(f"   Original HTML: {result.content_length:,} bytes")
    print(f"   Markdown: {result.markdown_length:,} bytes")
    print(f"   Links: {len(result.links or [])}")
    print(f"   Images: {len(result.images or [])}")
    print(f"   Headings: {len(result.headings or [])}")

    if result.headings:
        print(f"\nğŸ“‘ Headings:")
        for heading in result.headings[:5]:
            print(f"   â€¢ {heading}")

    print(f"\nğŸ“„ Markdown Preview (first 500 chars):")
    print("-" * 40)
    print((result.markdown or "")[:500])
    print("-" * 40)

    if not result.success:
        print(f"\nâŒ Error: {result.error_type}: {result.error_message}")

    print("\n" + "=" * 60)


def demo_processing():
    """ì²˜ë¦¬ ë°ëª¨"""
    import asyncio

    test_html = """
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <title>Sample Article</title>
        <meta name="description" content="This is a sample article for testing.">
        <meta name="keywords" content="sample, test, article">
    </head>
    <body>
        <nav>Navigation here</nav>
        <article>
            <h1>Welcome to the Sample Article</h1>
            <p>This is the first paragraph with some <strong>important</strong> content.
            It demonstrates how the fast worker processes static HTML pages.</p>

            <h2>Section One</h2>
            <p>More content here with a <a href="https://example.com">link</a> to another page.
            The worker will extract this link and include it in the results.</p>

            <img src="https://example.com/image.jpg" alt="Sample image">

            <h2>Section Two</h2>
            <ul>
                <li>First item</li>
                <li>Second item</li>
                <li>Third item</li>
            </ul>

            <h3>Subsection</h3>
            <p>Final paragraph with concluding thoughts about the article topic.</p>
        </article>
        <footer>Footer content</footer>
    </body>
    </html>
    """

    async def process():
        worker = FastWorker(worker_id=0)
        return await worker.process_html(
            html=test_html,
            url="https://example.com/article",
            metadata={},
        )

    result = asyncio.run(process())

    print("\n" + "=" * 60)
    print("FAST WORKER DEMO")
    print("=" * 60)

    print(f"\nSuccess: {result.success}")
    print(f"Title: {result.title}")
    print(f"Description: {result.description}")

    print(f"\nExtracted:")
    print(f"  Links: {len(result.links or [])} found")
    print(f"  Images: {len(result.images or [])} found")
    print(f"  Headings: {result.headings}")

    print(f"\nMarkdown output:")
    print("-" * 40)
    print(result.markdown)
    print("-" * 40)

    print("\n" + "=" * 60)


def parse_args():
    """ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±"""
    parser = argparse.ArgumentParser(
        description='Fast Processor (BeautifulSoup) for Stream Pipeline',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    # Kafka ì„¤ì •
    parser.add_argument(
        '--kafka-servers',
        type=str,
        default=None,
        help='Kafka ë¸Œë¡œì»¤ ì£¼ì†Œ',
    )

    # ì›Œì»¤ ì„¤ì •
    parser.add_argument(
        '--workers',
        type=int,
        default=1,
        help='ì›Œì»¤ ìˆ˜',
    )

    # ì‹¤í–‰ ëª¨ë“œ
    parser.add_argument(
        '--max-messages',
        type=int,
        default=None,
        help='ìµœëŒ€ ì²˜ë¦¬ ë©”ì‹œì§€ ìˆ˜',
    )
    parser.add_argument(
        '--test',
        action='store_true',
        help='í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ê¸°ë³¸ 100ê°œ)',
    )
    parser.add_argument(
        '--test-connection',
        action='store_true',
        help='Kafka ì—°ê²°ë§Œ í…ŒìŠ¤íŠ¸',
    )

    # ì˜¤í”„ë¼ì¸
    parser.add_argument(
        '--process-file',
        type=str,
        help='HTML íŒŒì¼ ì§ì ‘ ì²˜ë¦¬',
    )
    parser.add_argument(
        '--demo',
        action='store_true',
        help='ì²˜ë¦¬ ë°ëª¨ ì‹¤í–‰',
    )

    # ë¡œê¹…
    parser.add_argument(
        '--debug',
        action='store_true',
        help='ë””ë²„ê·¸ ë¡œê¹… í™œì„±í™”',
    )

    return parser.parse_args()


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    args = parse_args()

    # ë””ë²„ê·¸ ë¡œê¹…
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)

    # ë°ëª¨ ëª¨ë“œ
    if args.demo:
        demo_processing()
        return

    # íŒŒì¼ ì²˜ë¦¬ ëª¨ë“œ
    if args.process_file:
        process_file(args.process_file)
        return

    # Runner ìƒì„±
    runner = FastProcessorRunner(
        kafka_servers=args.kafka_servers,
        num_workers=args.workers,
    )

    # ì—°ê²° í…ŒìŠ¤íŠ¸
    if args.test_connection:
        await runner.test_connection()
        return

    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ
    max_messages = args.max_messages
    if args.test and not max_messages:
        max_messages = 100

    # í”„ë¡œì„¸ì„œ ì‹¤í–‰
    await runner.run(max_messages=max_messages)


if __name__ == '__main__':
    asyncio.run(main())
