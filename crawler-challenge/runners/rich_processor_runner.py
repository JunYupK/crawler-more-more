#!/usr/bin/env python3
"""
Rich Processor Runner - Crawl4AI ì›Œì»¤ ì‹¤í–‰ê¸°
=============================================

Desktopì—ì„œ ì‹¤í–‰í•˜ì—¬ process.rich í† í”½ì˜ ë™ì  í˜ì´ì§€ë¥¼ ì²˜ë¦¬

Usage:
    # ë‹¨ì¼ ì›Œì»¤ ì‹¤í–‰
    python runners/rich_processor_runner.py

    # ë‹¤ì¤‘ ì›Œì»¤ ì‹¤í–‰ (2ê°œ - ë¸Œë¼ìš°ì € ë¦¬ì†ŒìŠ¤ ì œí•œ)
    python runners/rich_processor_runner.py --workers 2

    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ
    python runners/rich_processor_runner.py --test --max-messages 50

    # Crawl4AI ì„¤ì¹˜ ìƒíƒœ í™•ì¸
    python runners/rich_processor_runner.py --check-crawl4ai

    # URL ì§ì ‘ ì²˜ë¦¬ (ì‹¤ì œ ë¸Œë¼ìš°ì € ì‚¬ìš©)
    python runners/rich_processor_runner.py --process-url https://example.com
"""

import asyncio
import argparse
import logging
import signal
import sys
import time
from pathlib import Path
from typing import Optional

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from src.processor.rich_worker import RichWorker, RichWorkerPool, check_crawl4ai_installation
from src.processor.base_worker import ProcessedResult
from src.common.kafka_config import get_config

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
)
logger = logging.getLogger(__name__)


class RichProcessorRunner:
    """Rich Processor ì‹¤í–‰ê¸°"""

    def __init__(
        self,
        kafka_servers: Optional[str] = None,
        num_workers: int = 1,
        headless: bool = True,
    ):
        self.kafka_servers = kafka_servers or get_config().kafka.bootstrap_servers
        self.num_workers = num_workers
        self.headless = headless

        self._running = False
        self._pool: Optional[RichWorkerPool] = None
        self._worker: Optional[RichWorker] = None

        # ì¢…ë£Œ ì‹œê·¸ë„ í•¸ë“¤ëŸ¬
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)

    def _signal_handler(self, signum, frame):
        """ì¢…ë£Œ ì‹œê·¸ë„ ì²˜ë¦¬"""
        logger.info(f"Received signal {signum}, shutting down...")
        self._running = False

        # ì›Œì»¤ ì¤‘ì§€ í”Œë˜ê·¸ ì„¤ì •
        if self._worker:
            self._worker._running = False
        if self._pool:
            for worker in self._pool.workers:
                worker._running = False

    async def run(self, max_messages: Optional[int] = None) -> None:
        """
        í”„ë¡œì„¸ì„œ ì‹¤í–‰

        Args:
            max_messages: ìµœëŒ€ ì²˜ë¦¬ ë©”ì‹œì§€ ìˆ˜
        """
        self._running = True
        start_time = time.time()
        processed_count = 0

        def on_processed(result: ProcessedResult):
            nonlocal processed_count
            processed_count += 1

            if processed_count % 10 == 0:  # RichëŠ” ëŠë¦¬ë¯€ë¡œ 10ê°œë§ˆë‹¤ ë¡œê¹…
                elapsed = time.time() - start_time
                logger.info(
                    f"[Progress] Processed: {processed_count:,} | "
                    f"Rate: {processed_count/elapsed:.2f} msg/s | "
                    f"Success: {result.success}"
                )

        try:
            # Crawl4AI ìƒíƒœ í™•ì¸
            crawl4ai_status = check_crawl4ai_installation()
            logger.info(f"Crawl4AI status: {crawl4ai_status}")

            if not crawl4ai_status['installed']:
                logger.warning("Crawl4AI not installed. Using fallback mode (BeautifulSoup).")
                logger.warning("Install with: pip install crawl4ai && crawl4ai-setup")

            logger.info(f"Connecting to Kafka: {self.kafka_servers}")
            logger.info(f"Number of workers: {self.num_workers}")
            logger.info(f"Headless mode: {self.headless}")

            if self.num_workers > 1:
                # ë‹¤ì¤‘ ì›Œì»¤ ëª¨ë“œ
                self._pool = RichWorkerPool(
                    num_workers=self.num_workers,
                    bootstrap_servers=self.kafka_servers,
                    headless=self.headless,
                )
                await self._pool.start()

                # ì›Œì»¤ë³„ max_messages ê³„ì‚°
                per_worker = max_messages // self.num_workers if max_messages else None
                await self._pool.run(max_messages_per_worker=per_worker)

                # ìµœì¢… í†µê³„
                stats = self._pool.get_combined_stats()

            else:
                # ë‹¨ì¼ ì›Œì»¤ ëª¨ë“œ
                self._worker = RichWorker(
                    bootstrap_servers=self.kafka_servers,
                    worker_id=0,
                    headless=self.headless,
                )

                async with self._worker:
                    await self._worker.run(
                        max_messages=max_messages,
                        callback=on_processed,
                    )

                stats = self._worker.get_stats()

            # ìµœì¢… í†µê³„ ì¶œë ¥
            elapsed = time.time() - start_time
            logger.info("=" * 60)
            logger.info("Rich Processor stopped!")
            logger.info(f"Total time: {elapsed:.1f}s")
            logger.info(f"Stats: {stats}")
            logger.info("=" * 60)

        except Exception as e:
            logger.error(f"Error running rich processor: {e}", exc_info=True)

        finally:
            if self._pool:
                await self._pool.stop()

    async def test_connection(self) -> bool:
        """Kafka ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            from aiokafka import AIOKafkaConsumer
            config = get_config()

            consumer = AIOKafkaConsumer(
                config.topics.process_rich,
                bootstrap_servers=self.kafka_servers,
            )
            await consumer.start()
            await consumer.stop()
            logger.info("Kafka connection test: SUCCESS")
            return True
        except Exception as e:
            logger.error(f"Kafka connection test: FAILED - {e}")
            return False


async def process_url(url: str, headless: bool = True) -> None:
    """
    URL ì§ì ‘ ì²˜ë¦¬ (ì‹¤ì œ ë¸Œë¼ìš°ì € ì‚¬ìš©)

    Args:
        url: ì²˜ë¦¬í•  URL
        headless: í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ
    """
    logger.info(f"Processing URL: {url}")
    logger.info(f"Headless: {headless}")

    worker = RichWorker(
        worker_id=0,
        headless=headless,
    )

    # ë¸Œë¼ìš°ì € ì´ˆê¸°í™”ë¥¼ ìœ„í•´ ì‹œì‘ í•„ìš”
    await worker.start()

    try:
        result = await worker.process_html(
            html="",  # URLì„ ì§ì ‘ ë°©ë¬¸í•˜ë¯€ë¡œ ë¹„ì–´ ìˆìŒ
            url=url,
            metadata={},
        )

        print("\n" + "=" * 60)
        print("PROCESSING RESULT")
        print("=" * 60)

        print(f"\nğŸ“Š Success: {result.success}")
        print(f"ğŸ”§ Processor: {result.processor_type.value}")
        print(f"â±ï¸ Processing time: {result.processing_time_ms:.1f}ms")

        if result.success:
            print(f"\nğŸ“ Metadata:")
            print(f"   Title: {result.title or 'N/A'}")
            print(f"   Description: {(result.description or 'N/A')[:100]}...")

            print(f"\nğŸ“ˆ Content Stats:")
            print(f"   Markdown: {result.markdown_length:,} chars")
            print(f"   Links: {len(result.links or [])}")
            print(f"   Images: {len(result.images or [])}")

            if result.links:
                print(f"\nğŸ”— Sample Links:")
                for link in result.links[:5]:
                    print(f"   â€¢ {link[:80]}...")

            print(f"\nğŸ“„ Markdown Preview (first 1000 chars):")
            print("-" * 40)
            print((result.markdown or "")[:1000])
            print("-" * 40)

        else:
            print(f"\nâŒ Error: {result.error_type}")
            print(f"   Message: {result.error_message}")

        print("\n" + "=" * 60)

    finally:
        await worker.stop()


def check_crawl4ai() -> None:
    """Crawl4AI ì„¤ì¹˜ ìƒíƒœ ìƒì„¸ í™•ì¸"""
    print("\n" + "=" * 60)
    print("CRAWL4AI INSTALLATION CHECK")
    print("=" * 60)

    status = check_crawl4ai_installation()

    if status['installed']:
        print("\nâœ… Crawl4AI is installed")
        print(f"   Version: {status.get('version', 'unknown')}")
        print(f"   Browser: {status.get('browser_available', 'unknown')}")

        if status.get('browser_available') == True:
            print("\nâœ… Browser is available and working")
        elif status.get('browser_available') == False:
            print("\nâš ï¸ Browser not available")
            print("   Run: crawl4ai-setup")
        else:
            print(f"\nâ“ Browser status: {status.get('browser_available')}")

    else:
        print("\nâŒ Crawl4AI is NOT installed")
        print("\n   To install:")
        print("   1. pip install crawl4ai")
        print("   2. crawl4ai-setup")
        print("\n   Or with Docker:")
        print("   docker pull unclecode/crawl4ai:latest")

    if 'error' in status:
        print(f"\nâš ï¸ Error during check: {status['error']}")

    print("\n" + "=" * 60)


def demo_processing():
    """ì²˜ë¦¬ ë°ëª¨ (í´ë°± ëª¨ë“œ)"""
    test_html = """
    <!DOCTYPE html>
    <html>
    <head>
        <title>Dynamic Page Demo</title>
        <meta name="description" content="A page that would normally need JavaScript">
    </head>
    <body>
        <div id="root">
            <h1>Dynamic Content</h1>
            <p>This content would normally be loaded by JavaScript,
            but we're demonstrating the fallback processing mode.</p>

            <div class="content">
                <h2>Section with Links</h2>
                <p>Here's a <a href="https://example.com/page1">link to page 1</a>
                and another <a href="https://example.com/page2">link to page 2</a>.</p>

                <h2>Section with Images</h2>
                <img src="https://example.com/image1.jpg" alt="Image 1">
                <img src="https://example.com/image2.jpg" alt="Image 2">
            </div>
        </div>
    </body>
    </html>
    """

    async def process():
        worker = RichWorker(worker_id=0)
        return await worker._process_fallback(
            html=test_html,
            url="https://example.com/dynamic",
            metadata={},
        )

    result = asyncio.run(process())

    print("\n" + "=" * 60)
    print("RICH WORKER DEMO (Fallback Mode)")
    print("=" * 60)

    print(f"\nSuccess: {result.success}")
    print(f"Title: {result.title}")
    print(f"Description: {result.description}")

    print(f"\nExtracted:")
    print(f"  Links: {result.links}")
    print(f"  Images: {result.images}")

    print(f"\nMarkdown (first 500 chars):")
    print("-" * 40)
    print((result.markdown or "")[:500])
    print("-" * 40)

    print("\n" + "=" * 60)


def parse_args():
    """ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±"""
    parser = argparse.ArgumentParser(
        description='Rich Processor (Crawl4AI) for Stream Pipeline',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    # Kafka ì„¤ì •
    parser.add_argument(
        '--kafka-servers',
        type=str,
        default=None,
        help='Kafka ë¸Œë¡œì»¤ ì£¼ì†Œ',
    )

    # ì›Œì»¤ ì„¤ì •
    parser.add_argument(
        '--workers',
        type=int,
        default=1,
        help='ì›Œì»¤ ìˆ˜ (ë¸Œë¼ìš°ì € ë¦¬ì†ŒìŠ¤ ì œí•œìœ¼ë¡œ 2-3 ê¶Œì¥)',
    )
    parser.add_argument(
        '--no-headless',
        action='store_true',
        help='ë¸Œë¼ìš°ì € UI í‘œì‹œ (ë””ë²„ê¹…ìš©)',
    )

    # ì‹¤í–‰ ëª¨ë“œ
    parser.add_argument(
        '--max-messages',
        type=int,
        default=None,
        help='ìµœëŒ€ ì²˜ë¦¬ ë©”ì‹œì§€ ìˆ˜',
    )
    parser.add_argument(
        '--test',
        action='store_true',
        help='í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ê¸°ë³¸ 50ê°œ)',
    )
    parser.add_argument(
        '--test-connection',
        action='store_true',
        help='Kafka ì—°ê²°ë§Œ í…ŒìŠ¤íŠ¸',
    )

    # Crawl4AI ê´€ë ¨
    parser.add_argument(
        '--check-crawl4ai',
        action='store_true',
        help='Crawl4AI ì„¤ì¹˜ ìƒíƒœ í™•ì¸',
    )
    parser.add_argument(
        '--process-url',
        type=str,
        help='URL ì§ì ‘ ì²˜ë¦¬ (ì‹¤ì œ ë¸Œë¼ìš°ì € ì‚¬ìš©)',
    )
    parser.add_argument(
        '--demo',
        action='store_true',
        help='ì²˜ë¦¬ ë°ëª¨ ì‹¤í–‰ (í´ë°± ëª¨ë“œ)',
    )

    # ë¡œê¹…
    parser.add_argument(
        '--debug',
        action='store_true',
        help='ë””ë²„ê·¸ ë¡œê¹… í™œì„±í™”',
    )

    return parser.parse_args()


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    args = parse_args()

    # ë””ë²„ê·¸ ë¡œê¹…
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)

    # Crawl4AI í™•ì¸
    if args.check_crawl4ai:
        check_crawl4ai()
        return

    # URL ì²˜ë¦¬ ëª¨ë“œ
    if args.process_url:
        await process_url(args.process_url, headless=not args.no_headless)
        return

    # ë°ëª¨ ëª¨ë“œ
    if args.demo:
        demo_processing()
        return

    # Runner ìƒì„±
    runner = RichProcessorRunner(
        kafka_servers=args.kafka_servers,
        num_workers=args.workers,
        headless=not args.no_headless,
    )

    # ì—°ê²° í…ŒìŠ¤íŠ¸
    if args.test_connection:
        await runner.test_connection()
        return

    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ
    max_messages = args.max_messages
    if args.test and not max_messages:
        max_messages = 50  # RichëŠ” ëŠë¦¬ë¯€ë¡œ 50ê°œ

    # í”„ë¡œì„¸ì„œ ì‹¤í–‰
    await runner.run(max_messages=max_messages)


if __name__ == '__main__':
    asyncio.run(main())
