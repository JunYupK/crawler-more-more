#!/usr/bin/env python3
"""
Sharded Distributed Crawler - Master Node
"""
import sys
import os
import asyncio
import logging
import signal
import argparse
import subprocess
import time
from datetime import datetime
from typing import Optional
from src.monitoring.metrics import MetricsManager
# Add src to path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from src.managers.tranco_manager import TrancoManager
from src.managers.sharded_queue_manager import ShardedRedisQueueManager
from src.managers.progress_tracker import ProgressTracker

# ë¡œê¹… ì„¤ì •
def setup_logging():
    log_dir = os.path.join(os.path.dirname(__file__), '../logs')
    os.makedirs(log_dir, exist_ok=True)
    log_file = os.path.join(log_dir, 'sharded_crawler_master.log')
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )

class ShardedCrawlerMaster:
    """ìƒ¤ë”©ëœ ë¶„ì‚° í¬ë¡¤ëŸ¬ ë§ˆìŠ¤í„° ë…¸ë“œ"""
    def __init__(self, worker_count: int = 4):
        self.worker_count = worker_count
        self.should_stop = False

        # ìƒ¤ë”©ëœ Redis ì—°ê²° ì„¤ì • (3ê°œ DB ìƒ¤ë“œ)
        redis_host = os.getenv('REDIS_HOST', 'localhost')
        shard_configs = [
            {'host': redis_host, 'port': 6379, 'db': 1},
            {'host': redis_host, 'port': 6379, 'db': 2},
            {'host': redis_host, 'port': 6379, 'db': 3}
        ]
        self.queue_manager = ShardedRedisQueueManager(shard_configs)

        # ì»´í¬ë„ŒíŠ¸ë“¤
        self.tranco_manager: Optional[TrancoManager] = None
        self.progress_tracker: Optional[ProgressTracker] = None
        
        # [Metric] 2. MetricsManager ì´ˆê¸°í™” (í¬íŠ¸ 8000, Master ì—­í• )
        self.metrics = MetricsManager(port=8000, role='master')
        # í†µê³„
        self.start_time = datetime.now()

        # ì‹ í˜¸ í•¸ë“¤ëŸ¬
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)


    def _signal_handler(self, signum, frame):
        """ì¢…ë£Œ ì‹ í˜¸ ì²˜ë¦¬"""
        logging.info(f"ìƒ¤ë”© ë§ˆìŠ¤í„°: ì¢…ë£Œ ì‹ í˜¸ ìˆ˜ì‹ : {signum}")
        self.should_stop = True

    async def initialize(self) -> bool:
        """ë§ˆìŠ¤í„° ì´ˆê¸°í™”"""
        try:
            logging.info("=== ìƒ¤ë”©ëœ ë¶„ì‚° í¬ë¡¤ëŸ¬ ë§ˆìŠ¤í„° ì´ˆê¸°í™” ì‹œì‘ ===")
            # [Metric] 3. ë©”íŠ¸ë¦­ ì„œë²„ ì‹œì‘
            self.metrics.start_server()

            # 1. Tranco Manager
            logging.info("1. Tranco Manager ì´ˆê¸°í™”...")
            self.tranco_manager = TrancoManager()

            # 2. Sharded Redis Queue Manager
            logging.info("2. Sharded Redis Queue Manager ì´ˆê¸°í™”...")
            if not self.queue_manager.test_connection():
                logging.error("Redis ìƒ¤ë“œ ì—°ê²° ì‹¤íŒ¨")
                return False

            # 3. Progress Tracker
            logging.info("3. Progress Tracker ì´ˆê¸°í™”...")
            self.progress_tracker = ProgressTracker()
            if not self.progress_tracker.test_connection():
                logging.error("Progress Tracker Redis ì—°ê²° ì‹¤íŒ¨")
                return False

            logging.info(f"[OK] ìƒ¤ë”© ë§ˆìŠ¤í„° ì´ˆê¸°í™” ì™„ë£Œ (ì›Œì»¤ {self.worker_count}ê°œ, ìƒ¤ë“œ {self.queue_manager.num_shards}ê°œ)")
            return True

        except Exception as e:
            logging.error(f"ìƒ¤ë”© ë§ˆìŠ¤í„° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    async def prepare_work(self, url_count: int = 400) -> bool:
        """ì‘ì—… ì¤€ë¹„ ë° ìƒ¤ë”©ëœ íì— ë°°í¬"""
        try:
            logging.info(f"=== ìƒ¤ë”©ëœ ì‘ì—… ì¤€ë¹„ ({url_count}ê°œ URL) ===")

            # URL ë°ì´í„°ì…‹ ì¤€ë¹„
            urls = await self.tranco_manager.prepare_url_dataset(initial_count=url_count)
            if not urls:
                logging.error("URL ë°ì´í„°ì…‹ ì¤€ë¹„ ì‹¤íŒ¨")
                return False

            # ìƒ¤ë”©ëœ Redis íì— ë¡œë“œ
            if not self.queue_manager.initialize_queues(urls):
                logging.error("ìƒ¤ë”©ëœ í ì´ˆê¸°í™” ì‹¤íŒ¨")
                return False
            # í†µê³„ ì¶œë ¥
            queue_stats = self.queue_manager.get_queue_stats()
            # [Metric] 4. ì´ˆê¸° ìƒíƒœ ë°˜ì˜
            self.metrics.update_queue_stats(queue_stats)

            # [Metric] í¬ë¡¤ë§ ì‹œì‘ ì‹œê°„ ê¸°ë¡
            self.metrics.session_start_timestamp.set(time.time())
            logging.info(f"âœ… í¬ë¡¤ë§ ì„¸ì…˜ ì‹œì‘ ì‹œê°„ ê¸°ë¡: {datetime.now().isoformat()}")

            # í†µê³„ ì¶œë ¥
            queue_stats = self.queue_manager.get_queue_stats()
            logging.info(f"ìƒ¤ë”©ëœ í ë¡œë”© ì™„ë£Œ:")
            logging.info(f"  - ì´ URL: {queue_stats.get('total_urls', 0)}ê°œ")
            logging.info(f"  - ìƒ¤ë“œ ìˆ˜: {queue_stats.get('num_shards', 0)}ê°œ")
            logging.info(f"  - ê³ ìš°ì„ ìˆœìœ„: {queue_stats.get('queue_priority_high', 0)}")
            logging.info(f"  - ì¤‘ìš°ì„ ìˆœìœ„: {queue_stats.get('queue_priority_medium', 0)}")
            logging.info(f"  - ì¼ë°˜ìš°ì„ ìˆœìœ„: {queue_stats.get('queue_priority_normal', 0)}")
            logging.info(f"  - ì €ìš°ì„ ìˆœìœ„: {queue_stats.get('queue_priority_low', 0)}")

            # ìƒ¤ë“œë³„ ë¶„ì‚° í˜„í™©
            load_balance = self.queue_manager.get_shard_load_balance()
            logging.info(f"ìƒ¤ë“œ ë¡œë“œ ë¶„ì‚°:")
            for shard_id, load in load_balance.items():
                logging.info(f"  - ìƒ¤ë“œ {shard_id}: {load}ê°œ ì‘ì—…")

            return True

        except Exception as e:
            logging.error(f"ìƒ¤ë”©ëœ ì‘ì—… ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            return False

    async def monitor_workers(self):
        """ì›Œì»¤ë“¤ ëª¨ë‹ˆí„°ë§ ë° ë™ì  ë¦¬ë°¸ëŸ°ì‹±"""
        logging.info("=== ìƒ¤ë”©ëœ ì›Œì»¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘ ===")

        last_report = datetime.now()
        last_rebalance = datetime.now()

        while not self.should_stop:
            try:
                # í ìƒíƒœ í™•ì¸
                queue_stats = self.queue_manager.get_queue_stats()

                # [Metric] 5. ì‹¤ì‹œê°„ í†µê³„ ì—…ë°ì´íŠ¸ (ì¤‘ìš”!)
                self.metrics.update_queue_stats(queue_stats)

                # 5ë¶„ë§ˆë‹¤ ë¦¬í¬íŠ¸
                if (datetime.now() - last_report).total_seconds() > 300:
                    logging.info("ğŸ“Š ìƒ¤ë”© ë§ˆìŠ¤í„° ë¦¬í¬íŠ¸:")
                    logging.info(f"  - ì™„ë£Œ: {queue_stats.get('completed', 0)}ê°œ")
                    logging.info(f"  - ëŒ€ê¸°: {queue_stats.get('total_pending', 0)}ê°œ")
                    logging.info(f"  - ì²˜ë¦¬ ì¤‘: {queue_stats.get('processing', 0)}ê°œ")
                    logging.info(f"  - ì‹¤íŒ¨: {queue_stats.get('failed', 0)}ê°œ")
                    logging.info(f"  - ì™„ë£Œìœ¨: {queue_stats.get('completion_rate', 0):.1%}")
                    

                    # ìƒ¤ë“œë³„ ìƒì„¸ ì •ë³´
                    for shard in queue_stats.get('shard_details', []):
                        shard_id = shard['shard_id']
                        pending = shard['total_pending']
                        completed = shard['completed']
                        logging.info(f"  - ìƒ¤ë“œ {shard_id}: ëŒ€ê¸° {pending}, ì™„ë£Œ {completed}")

                    last_report = datetime.now()

                # 10ë¶„ë§ˆë‹¤ ìƒ¤ë“œ ë¦¬ë°¸ëŸ°ì‹±
                if (datetime.now() - last_rebalance).total_seconds() > 600:
                    logging.info("ğŸ”„ ìƒ¤ë“œ ë¦¬ë°¸ëŸ°ì‹± í™•ì¸...")
                    self.queue_manager.rebalance_shards()
                    last_rebalance = datetime.now()

                # ëª¨ë“  ì‘ì—… ì™„ë£Œ í™•ì¸
                if (queue_stats.get('total_pending', 0) == 0 and
                    queue_stats.get('processing', 0) == 0 and
                    queue_stats.get('completed', 0) > 0):
                    logging.info("âœ… ëª¨ë“  ìƒ¤ë”©ëœ ì‘ì—… ì™„ë£Œ")

                    # [Metric] í¬ë¡¤ë§ ì¢…ë£Œ ì‹œê°„ ê¸°ë¡
                    self.metrics.session_end_timestamp.set(time.time())
                    logging.info(f"âœ… í¬ë¡¤ë§ ì„¸ì…˜ ì¢…ë£Œ ì‹œê°„ ê¸°ë¡: {datetime.now().isoformat()}")

                    # Prometheus scrape ë°˜ì˜ì„ ìœ„í•œ ëŒ€ê¸° (30ì´ˆ)
                    logging.info("â³ Prometheus ë©”íŠ¸ë¦­ ë°˜ì˜ ëŒ€ê¸° ì¤‘ (30ì´ˆ)...")
                    await asyncio.sleep(30)

                    # ìë™ ë¦¬í¬íŠ¸ ìƒì„±
                    await self.generate_completion_report()

                    # ëŒ€ê¸° ëª¨ë“œ ì§„ì…
                    logging.info("=" * 60)
                    logging.info("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ ë° ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")
                    logging.info("ğŸ“Š ë©”íŠ¸ë¦­ ì„œë²„ê°€ ê³„ì† ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤ (port 8000)")
                    logging.info("ğŸ’¡ ìœˆë„ìš°ì—ì„œ ì¶”ê°€ ë³´ê³ ì„œë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
                    logging.info("   python scripts/generate_ai_report.py")
                    logging.info("â¹ï¸  ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”")
                    logging.info("=" * 60)

                    # ëŒ€ê¸° ëª¨ë“œ (SIGINT/SIGTERM ìˆ˜ì‹  ì‹œê¹Œì§€)
                    while not self.should_stop:
                        await asyncio.sleep(10)

                    logging.info("ğŸ‘‹ ë§ˆìŠ¤í„° ì¢…ë£Œ ì¤‘...")
                    break

                await asyncio.sleep(30)  # 30ì´ˆë§ˆë‹¤ í™•ì¸

            except Exception as e:
                logging.error(f"ìƒ¤ë”© ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                self.metrics.inc_error(error_type='monitor_loop_error')
                await asyncio.sleep(30)

    async def run_tests_and_generate_report(self):
        """í¬ë¡¤ë§ ì™„ë£Œ í›„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±"""
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))

        try:
            logging.info("ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë° ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘...")

            # pytest ì‹¤í–‰í•˜ì—¬ test_report.json ìƒì„±
            process = await asyncio.create_subprocess_exec(
                sys.executable, '-m', 'pytest',
                '--json-report',
                '--json-report-file=test_report.json',
                '--json-report-indent=2',
                '-v',
                cwd=project_root,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )

            stdout, stderr = await process.communicate()

            if process.returncode == 0:
                logging.info("âœ… í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì™„ë£Œ (ëª¨ë‘ í†µê³¼)")
            else:
                # í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨í•´ë„ ë¦¬í¬íŠ¸ëŠ” ìƒì„±ë¨
                logging.warning(f"âš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ (ì¢…ë£Œ ì½”ë“œ: {process.returncode})")

            # test_report.jsonì´ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
            test_report_path = os.path.join(project_root, 'test_report.json')
            if os.path.exists(test_report_path):
                logging.info(f"ğŸ“„ í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±ë¨: {test_report_path}")
            else:
                logging.warning("í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        except Exception as e:
            logging.error(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            # í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨í•´ë„ AI ë¦¬í¬íŠ¸ ìƒì„±ì€ ê³„ì† ì§„í–‰

    async def generate_completion_report(self):
        """í¬ë¡¤ë§ ì™„ë£Œ í›„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë° AI ë¦¬í¬íŠ¸ ìë™ ìƒì„±"""
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))

        # 1ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë° test_report.json ìƒì„±
        await self.run_tests_and_generate_report()

        # 2ë‹¨ê³„: AI ë¦¬í¬íŠ¸ ìƒì„±
        try:
            logging.info("ğŸ“Š AI ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘...")

            report_script = os.path.join(project_root, 'scripts', 'generate_ai_report.py')

            if not os.path.exists(report_script):
                logging.warning(f"ë¦¬í¬íŠ¸ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {report_script}")
                return

            # ë¹„ë™ê¸°ë¡œ ë¦¬í¬íŠ¸ ìƒì„± ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
            process = await asyncio.create_subprocess_exec(
                sys.executable, report_script,
                cwd=project_root,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )

            stdout, stderr = await process.communicate()

            if process.returncode == 0:
                logging.info("âœ… AI ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")
                if stdout:
                    logging.info(stdout.decode('utf-8', errors='ignore'))
            else:
                logging.warning(f"AI ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨ (ì¢…ë£Œ ì½”ë“œ: {process.returncode})")
                if stderr:
                    logging.warning(stderr.decode('utf-8', errors='ignore'))

        except Exception as e:
            logging.error(f"AI ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            # ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨í•´ë„ í¬ë¡¤ë§ì€ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬

    async def run(self, url_count: int = 400):
        """ìƒ¤ë”© ë§ˆìŠ¤í„° ì‹¤í–‰"""
        try:
            # ì´ˆê¸°í™”
            if not await self.initialize():
                logging.error("ìƒ¤ë”© ë§ˆìŠ¤í„° ì´ˆê¸°í™” ì‹¤íŒ¨")
                return False

            # ì‘ì—… ì¤€ë¹„
            if not await self.prepare_work(url_count):
                logging.error("ìƒ¤ë”©ëœ ì‘ì—… ì¤€ë¹„ ì‹¤íŒ¨")
                return False

            # ì›Œì»¤ ëª¨ë‹ˆí„°ë§
            await self.monitor_workers()

            return True

        except Exception as e:
            logging.error(f"ìƒ¤ë”© ë§ˆìŠ¤í„° ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return False

async def main():
    parser = argparse.ArgumentParser(description='Sharded Distributed Crawler - Master')
    parser.add_argument('--count', type=int, default=400, help='í¬ë¡¤ë§í•  URL ê°œìˆ˜')
    parser.add_argument('--workers', type=int, default=4, help='ì›Œì»¤ ìˆ˜')
    
    args = parser.parse_args()

    setup_logging()
    
    print(f"ìƒ¤ë”©ëœ ë¶„ì‚° í¬ë¡¤ëŸ¬ ë§ˆìŠ¤í„° ì‹œì‘ (ì›Œì»¤ {args.workers}ê°œ, URL {args.count}ê°œ)")
    master = ShardedCrawlerMaster(worker_count=args.workers)
    success = await master.run(url_count=args.count)
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    asyncio.run(main())
