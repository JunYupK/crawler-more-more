#!/usr/bin/env python3
"""
Sharded Distributed Crawler - Redis ìƒ¤ë”©ì„ í™œìš©í•œ ê³ ì„±ëŠ¥ ë¶„ì‚° í¬ë¡¤ëŸ¬
"""

import asyncio
import logging
import sys
import signal
import os
import time
from datetime import datetime
from typing import List, Dict, Any, Optional
import argparse

from tranco_manager import TrancoManager
from sharded_queue_manager import ShardedRedisQueueManager
from database import DatabaseManager
from progress_tracker import ProgressTracker
from monitoring.metrics import MetricsMonitor

# ë¡œê¹… ì„¤ì •
def setup_logging(worker_id: Optional[int] = None):
    log_file = f'sharded_crawler_worker_{worker_id}.log' if worker_id else 'sharded_crawler_master.log'
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(f'logs/{log_file}', encoding='utf-8'),
            logging.StreamHandler()
        ]
    )

class ShardedCrawlerMaster:
    """ìƒ¤ë”©ëœ ë¶„ì‚° í¬ë¡¤ëŸ¬ ë§ˆìŠ¤í„° ë…¸ë“œ"""

    def __init__(self, worker_count: int = 4):
        self.worker_count = worker_count
        self.should_stop = False

        # ìƒ¤ë”©ëœ Redis ì—°ê²° ì„¤ì • (3ê°œ DB ìƒ¤ë“œ)
        redis_host = os.getenv('REDIS_HOST', 'localhost')
        shard_configs = [
            {'host': redis_host, 'port': 6379, 'db': 1},
            {'host': redis_host, 'port': 6379, 'db': 2},
            {'host': redis_host, 'port': 6379, 'db': 3}
        ]
        self.queue_manager = ShardedRedisQueueManager(shard_configs)

        # ì»´í¬ë„ŒíŠ¸ë“¤
        self.tranco_manager: Optional[TrancoManager] = None
        self.progress_tracker: Optional[ProgressTracker] = None

        # í†µê³„
        self.start_time = datetime.now()

        # ì‹ í˜¸ í•¸ë“¤ëŸ¬
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)

    def _signal_handler(self, signum, frame):
        """ì¢…ë£Œ ì‹ í˜¸ ì²˜ë¦¬"""
        logging.info(f"ìƒ¤ë”© ë§ˆìŠ¤í„°: ì¢…ë£Œ ì‹ í˜¸ ìˆ˜ì‹ : {signum}")
        self.should_stop = True

    async def initialize(self) -> bool:
        """ë§ˆìŠ¤í„° ì´ˆê¸°í™”"""
        try:
            logging.info("=== ìƒ¤ë”©ëœ ë¶„ì‚° í¬ë¡¤ëŸ¬ ë§ˆìŠ¤í„° ì´ˆê¸°í™” ì‹œì‘ ===")

            # 1. Tranco Manager
            logging.info("1. Tranco Manager ì´ˆê¸°í™”...")
            self.tranco_manager = TrancoManager()

            # 2. Sharded Redis Queue Manager
            logging.info("2. Sharded Redis Queue Manager ì´ˆê¸°í™”...")
            if not self.queue_manager.test_connection():
                logging.error("Redis ìƒ¤ë“œ ì—°ê²° ì‹¤íŒ¨")
                return False

            # 3. Progress Tracker
            logging.info("3. Progress Tracker ì´ˆê¸°í™”...")
            self.progress_tracker = ProgressTracker()
            if not self.progress_tracker.test_connection():
                logging.error("Progress Tracker Redis ì—°ê²° ì‹¤íŒ¨")
                return False

            logging.info(f"[OK] ìƒ¤ë”© ë§ˆìŠ¤í„° ì´ˆê¸°í™” ì™„ë£Œ (ì›Œì»¤ {self.worker_count}ê°œ, ìƒ¤ë“œ {self.queue_manager.num_shards}ê°œ)")
            return True

        except Exception as e:
            logging.error(f"ìƒ¤ë”© ë§ˆìŠ¤í„° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    async def prepare_work(self, url_count: int = 400) -> bool:
        """ì‘ì—… ì¤€ë¹„ ë° ìƒ¤ë”©ëœ íì— ë°°í¬"""
        try:
            logging.info(f"=== ìƒ¤ë”©ëœ ì‘ì—… ì¤€ë¹„ ({url_count}ê°œ URL) ===")

            # URL ë°ì´í„°ì…‹ ì¤€ë¹„
            urls = await self.tranco_manager.prepare_url_dataset(initial_count=url_count)
            if not urls:
                logging.error("URL ë°ì´í„°ì…‹ ì¤€ë¹„ ì‹¤íŒ¨")
                return False

            # ìƒ¤ë”©ëœ Redis íì— ë¡œë“œ
            if not self.queue_manager.initialize_queues(urls):
                logging.error("ìƒ¤ë”©ëœ í ì´ˆê¸°í™” ì‹¤íŒ¨")
                return False

            # í†µê³„ ì¶œë ¥
            queue_stats = self.queue_manager.get_queue_stats()
            logging.info(f"ìƒ¤ë”©ëœ í ë¡œë”© ì™„ë£Œ:")
            logging.info(f"  - ì´ URL: {queue_stats.get('total_urls', 0)}ê°œ")
            logging.info(f"  - ìƒ¤ë“œ ìˆ˜: {queue_stats.get('num_shards', 0)}ê°œ")
            logging.info(f"  - ê³ ìš°ì„ ìˆœìœ„: {queue_stats.get('queue_priority_high', 0)}")
            logging.info(f"  - ì¤‘ìš°ì„ ìˆœìœ„: {queue_stats.get('queue_priority_medium', 0)}")
            logging.info(f"  - ì¼ë°˜ìš°ì„ ìˆœìœ„: {queue_stats.get('queue_priority_normal', 0)}")
            logging.info(f"  - ì €ìš°ì„ ìˆœìœ„: {queue_stats.get('queue_priority_low', 0)}")

            # ìƒ¤ë“œë³„ ë¶„ì‚° í˜„í™©
            load_balance = self.queue_manager.get_shard_load_balance()
            logging.info(f"ìƒ¤ë“œ ë¡œë“œ ë¶„ì‚°:")
            for shard_id, load in load_balance.items():
                logging.info(f"  - ìƒ¤ë“œ {shard_id}: {load}ê°œ ì‘ì—…")

            return True

        except Exception as e:
            logging.error(f"ìƒ¤ë”©ëœ ì‘ì—… ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            return False

    async def monitor_workers(self):
        """ì›Œì»¤ë“¤ ëª¨ë‹ˆí„°ë§ ë° ë™ì  ë¦¬ë°¸ëŸ°ì‹±"""
        logging.info("=== ìƒ¤ë”©ëœ ì›Œì»¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘ ===")

        last_report = datetime.now()
        last_rebalance = datetime.now()

        while not self.should_stop:
            try:
                # í ìƒíƒœ í™•ì¸
                queue_stats = self.queue_manager.get_queue_stats()

                # 5ë¶„ë§ˆë‹¤ ë¦¬í¬íŠ¸
                if (datetime.now() - last_report).total_seconds() > 300:
                    logging.info("ğŸ“Š ìƒ¤ë”© ë§ˆìŠ¤í„° ë¦¬í¬íŠ¸:")
                    logging.info(f"  - ì™„ë£Œ: {queue_stats.get('completed', 0)}ê°œ")
                    logging.info(f"  - ëŒ€ê¸°: {queue_stats.get('total_pending', 0)}ê°œ")
                    logging.info(f"  - ì²˜ë¦¬ ì¤‘: {queue_stats.get('processing', 0)}ê°œ")
                    logging.info(f"  - ì‹¤íŒ¨: {queue_stats.get('failed', 0)}ê°œ")
                    logging.info(f"  - ì™„ë£Œìœ¨: {queue_stats.get('completion_rate', 0):.1%}")

                    # ìƒ¤ë“œë³„ ìƒì„¸ ì •ë³´
                    for shard in queue_stats.get('shard_details', []):
                        shard_id = shard['shard_id']
                        pending = shard['total_pending']
                        completed = shard['completed']
                        logging.info(f"  - ìƒ¤ë“œ {shard_id}: ëŒ€ê¸° {pending}, ì™„ë£Œ {completed}")

                    last_report = datetime.now()

                # 10ë¶„ë§ˆë‹¤ ìƒ¤ë“œ ë¦¬ë°¸ëŸ°ì‹±
                if (datetime.now() - last_rebalance).total_seconds() > 600:
                    logging.info("ğŸ”„ ìƒ¤ë“œ ë¦¬ë°¸ëŸ°ì‹± í™•ì¸...")
                    self.queue_manager.rebalance_shards()
                    last_rebalance = datetime.now()

                # ëª¨ë“  ì‘ì—… ì™„ë£Œ í™•ì¸
                if (queue_stats.get('total_pending', 0) == 0 and
                    queue_stats.get('processing', 0) == 0 and
                    queue_stats.get('completed', 0) > 0):
                    logging.info("âœ… ëª¨ë“  ìƒ¤ë”©ëœ ì‘ì—… ì™„ë£Œ")
                    break

                await asyncio.sleep(30)  # 30ì´ˆë§ˆë‹¤ í™•ì¸

            except Exception as e:
                logging.error(f"ìƒ¤ë”© ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(30)

    async def run(self, url_count: int = 400):
        """ìƒ¤ë”© ë§ˆìŠ¤í„° ì‹¤í–‰"""
        try:
            # ì´ˆê¸°í™”
            if not await self.initialize():
                logging.error("ìƒ¤ë”© ë§ˆìŠ¤í„° ì´ˆê¸°í™” ì‹¤íŒ¨")
                return False

            # ì‘ì—… ì¤€ë¹„
            if not await self.prepare_work(url_count):
                logging.error("ìƒ¤ë”©ëœ ì‘ì—… ì¤€ë¹„ ì‹¤íŒ¨")
                return False

            # ì›Œì»¤ ëª¨ë‹ˆí„°ë§
            await self.monitor_workers()

            return True

        except Exception as e:
            logging.error(f"ìƒ¤ë”© ë§ˆìŠ¤í„° ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return False

class ShardedCrawlerWorker:
    """ìƒ¤ë”©ëœ ë¶„ì‚° í¬ë¡¤ëŸ¬ ì›Œì»¤ ë…¸ë“œ"""

    def __init__(self, worker_id: int, batch_size: int = 25, preferred_shard: Optional[int] = None):
        self.worker_id = worker_id
        self.batch_size = batch_size
        self.preferred_shard = preferred_shard  # ì„ í˜¸í•˜ëŠ” ìƒ¤ë“œ (ë¡œë“œ ë°¸ëŸ°ì‹±ìš©)
        self.should_stop = False

        # ìƒ¤ë”©ëœ Redis ì—°ê²° ì„¤ì •
        redis_host = os.getenv('REDIS_HOST', 'localhost')
        postgres_host = os.getenv('POSTGRES_HOST', 'localhost')

        shard_configs = [
            {'host': redis_host, 'port': 6379, 'db': 1},
            {'host': redis_host, 'port': 6379, 'db': 2},
            {'host': redis_host, 'port': 6379, 'db': 3}
        ]

        # ì»´í¬ë„ŒíŠ¸ë“¤
        self.queue_manager = ShardedRedisQueueManager(shard_configs)
        self.db_manager = DatabaseManager(host=postgres_host)

        # í†µê³„
        self.total_processed = 0
        self.total_successful = 0
        self.total_failed = 0
        self.start_time = datetime.now()
        self.shard_distribution = {}  # ìƒ¤ë“œë³„ ì²˜ë¦¬ í†µê³„

        # ì‹ í˜¸ í•¸ë“¤ëŸ¬
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)

    def _signal_handler(self, signum, frame):
        """ì¢…ë£Œ ì‹ í˜¸ ì²˜ë¦¬"""
        logging.info(f"ìƒ¤ë”© ì›Œì»¤ {self.worker_id}: ì¢…ë£Œ ì‹ í˜¸ ìˆ˜ì‹ : {signum}")
        self.should_stop = True

    async def initialize(self) -> bool:
        """ì›Œì»¤ ì´ˆê¸°í™”"""
        try:
            logging.info(f"=== ìƒ¤ë”© ì›Œì»¤ {self.worker_id} ì´ˆê¸°í™” ì‹œì‘ ===")

            # Redis ìƒ¤ë“œ ì—°ê²° í™•ì¸
            if not self.queue_manager.test_connection():
                logging.error("Redis ìƒ¤ë“œ ì—°ê²° ì‹¤íŒ¨")
                return False

            # ì„ í˜¸ ìƒ¤ë“œ ì„¤ì • (ì›Œì»¤ ID ê¸°ë°˜)
            if self.preferred_shard is None:
                self.preferred_shard = self.worker_id % self.queue_manager.num_shards

            logging.info(f"[OK] ìƒ¤ë”© ì›Œì»¤ {self.worker_id} ì´ˆê¸°í™” ì™„ë£Œ (ì„ í˜¸ ìƒ¤ë“œ: {self.preferred_shard})")
            return True

        except Exception as e:
            logging.error(f"ìƒ¤ë”© ì›Œì»¤ {self.worker_id} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    async def process_batch(self, batch: List[Dict]) -> bool:
        """ë°°ì¹˜ ì²˜ë¦¬ (ìƒ¤ë“œ ì •ë³´ í¬í•¨)"""
        try:
            if not batch:
                return False

            batch_urls = [item['url'] for item in batch]
            logging.info(f"ìƒ¤ë”© ì›Œì»¤ {self.worker_id}: ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘ ({len(batch_urls)}ê°œ URL)")

            # ìƒ¤ë“œë³„ ë¶„í¬ ì¶”ì 
            shard_counts = {}
            for item in batch:
                shard_id = item.get('shard_id', -1)
                shard_counts[shard_id] = shard_counts.get(shard_id, 0) + 1

            logging.debug(f"ë°°ì¹˜ ìƒ¤ë“œ ë¶„í¬: {shard_counts}")

            # ì„ì‹œ í¬ë¡¤ëŸ¬ ìƒì„± (ë°°ì¹˜ë³„ë¡œ)
            from polite_crawler import PoliteCrawler

            async with PoliteCrawler(respect_robots_txt=False) as crawler:
                results = await crawler.crawl_batch_politely(batch_urls)

                # ê²°ê³¼ ì²˜ë¦¬
                successful_count = 0
                failed_count = 0

                for i, result in enumerate(results):
                    original_item = batch[i] if i < len(batch) else {}
                    shard_id = original_item.get('shard_id', -1)

                    try:
                        if result['success'] and result.get('content'):
                            # ì„±ê³µ - ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                            self.db_manager.add_to_batch(result['url'], result['content'])
                            self.queue_manager.mark_completed(result['url'], success=True)
                            successful_count += 1

                            # ìƒ¤ë“œë³„ ì„±ê³µ í†µê³„
                            if shard_id not in self.shard_distribution:
                                self.shard_distribution[shard_id] = {'success': 0, 'failed': 0}
                            self.shard_distribution[shard_id]['success'] += 1

                        else:
                            # ì‹¤íŒ¨ ì²˜ë¦¬
                            error_message = result.get('error', 'Unknown error')
                            error_info = {
                                'type': 'crawl_error',
                                'message': error_message,
                                'recoverable': 'timeout' in error_message.lower(),
                                'worker_id': self.worker_id,
                                'shard_id': shard_id
                            }

                            self.queue_manager.mark_completed(
                                result['url'],
                                success=False,
                                error_info=error_info
                            )
                            failed_count += 1

                            # ìƒ¤ë“œë³„ ì‹¤íŒ¨ í†µê³„
                            if shard_id not in self.shard_distribution:
                                self.shard_distribution[shard_id] = {'success': 0, 'failed': 0}
                            self.shard_distribution[shard_id]['failed'] += 1

                    except Exception as e:
                        logging.error(f"ìƒ¤ë”© ì›Œì»¤ {self.worker_id} ê²°ê³¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                        failed_count += 1

                # í†µê³„ ì—…ë°ì´íŠ¸
                self.total_processed += len(results)
                self.total_successful += successful_count
                self.total_failed += failed_count

                logging.info(f"ìƒ¤ë”© ì›Œì»¤ {self.worker_id} ë°°ì¹˜ ì™„ë£Œ: {successful_count}ê°œ ì„±ê³µ, {failed_count}ê°œ ì‹¤íŒ¨")
                return True

        except Exception as e:
            logging.error(f"ìƒ¤ë”© ì›Œì»¤ {self.worker_id} ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return False

    async def run(self):
        """ìƒ¤ë”© ì›Œì»¤ ì‹¤í–‰"""
        try:
            # ì´ˆê¸°í™”
            if not await self.initialize():
                logging.error(f"ìƒ¤ë”© ì›Œì»¤ {self.worker_id} ì´ˆê¸°í™” ì‹¤íŒ¨")
                return False

            logging.info(f"ìƒ¤ë”© ì›Œì»¤ {self.worker_id} ì‘ì—… ì‹œì‘ (ì„ í˜¸ ìƒ¤ë“œ: {self.preferred_shard})")

            consecutive_empty = 0

            while not self.should_stop:
                try:
                    # ìƒ¤ë”©ëœ íì—ì„œ ë°°ì¹˜ ê°€ì ¸ì˜¤ê¸° (ì„ í˜¸ ìƒ¤ë“œ ìš°ì„ )
                    batch = self.queue_manager.get_next_batch(
                        batch_size=self.batch_size,
                        shard_preference=self.preferred_shard
                    )

                    if not batch:
                        consecutive_empty += 1
                        if consecutive_empty >= 10:  # 10íšŒ ì—°ì† ë¹ˆ ë°°ì¹˜ë©´ ì¢…ë£Œ
                            logging.info(f"ìƒ¤ë”© ì›Œì»¤ {self.worker_id}: ë” ì´ìƒ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤")
                            break
                        await asyncio.sleep(5)
                        continue

                    consecutive_empty = 0

                    # ë°°ì¹˜ ì²˜ë¦¬
                    await self.process_batch(batch)

                    # ì§§ì€ ëŒ€ê¸°
                    await asyncio.sleep(1)

                except Exception as e:
                    logging.error(f"ìƒ¤ë”© ì›Œì»¤ {self.worker_id} ì‹¤í–‰ ì˜¤ë¥˜: {e}")
                    await asyncio.sleep(5)

            # ìµœì¢… í†µê³„
            duration = datetime.now() - self.start_time
            duration_minutes = duration.total_seconds() / 60

            logging.info(f"=== ìƒ¤ë”© ì›Œì»¤ {self.worker_id} ì™„ë£Œ ===")
            logging.info(f"ì‹¤í–‰ ì‹œê°„: {duration_minutes:.1f}ë¶„")
            logging.info(f"ì´ ì²˜ë¦¬: {self.total_processed}ê°œ")
            logging.info(f"ì„±ê³µ: {self.total_successful}ê°œ")
            logging.info(f"ì‹¤íŒ¨: {self.total_failed}ê°œ")

            if self.total_processed > 0:
                success_rate = self.total_successful / self.total_processed * 100
                pages_per_minute = self.total_processed / duration_minutes if duration_minutes > 0 else 0
                logging.info(f"ì„±ê³µë¥ : {success_rate:.1f}%")
                logging.info(f"ì²˜ë¦¬ì†ë„: {pages_per_minute:.1f} pages/ë¶„")

            # ìƒ¤ë“œë³„ ì²˜ë¦¬ ë¶„í¬
            logging.info(f"ìƒ¤ë“œë³„ ì²˜ë¦¬ ë¶„í¬:")
            for shard_id, stats in self.shard_distribution.items():
                total = stats['success'] + stats['failed']
                logging.info(f"  ìƒ¤ë“œ {shard_id}: {total}ê°œ ({stats['success']}ì„±ê³µ, {stats['failed']}ì‹¤íŒ¨)")

            # DB ì •ë¦¬
            if self.db_manager:
                self.db_manager.flush_batch()
                self.db_manager.close_all_connections()

            return True

        except Exception as e:
            logging.error(f"ìƒ¤ë”© ì›Œì»¤ {self.worker_id} ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return False

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='Sharded Distributed Crawler - Master/Worker')
    parser.add_argument('--mode', choices=['master', 'worker'], required=True,
                       help='ì‹¤í–‰ ëª¨ë“œ')
    parser.add_argument('--worker-id', type=int, default=1,
                       help='ì›Œì»¤ ID (worker ëª¨ë“œì¼ ë•Œ)')
    parser.add_argument('--count', type=int, default=400,
                       help='í¬ë¡¤ë§í•  URL ê°œìˆ˜ (master ëª¨ë“œì¼ ë•Œ)')
    parser.add_argument('--workers', type=int, default=4,
                       help='ì›Œì»¤ ìˆ˜ (master ëª¨ë“œì¼ ë•Œ)')
    parser.add_argument('--batch-size', type=int, default=25,
                       help='ë°°ì¹˜ í¬ê¸° (worker ëª¨ë“œì¼ ë•Œ)')
    parser.add_argument('--preferred-shard', type=int, default=None,
                       help='ì„ í˜¸ ìƒ¤ë“œ ID (worker ëª¨ë“œì¼ ë•Œ)')

    args = parser.parse_args()

    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('logs', exist_ok=True)

    # ë¡œê¹… ì„¤ì •
    setup_logging(args.worker_id if args.mode == 'worker' else None)

    try:
        if args.mode == 'master':
            print(f"ìƒ¤ë”©ëœ ë¶„ì‚° í¬ë¡¤ëŸ¬ ë§ˆìŠ¤í„° ì‹œì‘ (ì›Œì»¤ {args.workers}ê°œ, URL {args.count}ê°œ)")
            master = ShardedCrawlerMaster(worker_count=args.workers)
            success = await master.run(url_count=args.count)
            sys.exit(0 if success else 1)

        elif args.mode == 'worker':
            print(f"ìƒ¤ë”©ëœ ë¶„ì‚° í¬ë¡¤ëŸ¬ ì›Œì»¤ {args.worker_id} ì‹œì‘")
            worker = ShardedCrawlerWorker(
                worker_id=args.worker_id,
                batch_size=args.batch_size,
                preferred_shard=args.preferred_shard
            )
            success = await worker.run()
            sys.exit(0 if success else 1)

    except KeyboardInterrupt:
        logging.info("ì‚¬ìš©ìì— ì˜í•œ ì¤‘ë‹¨")
        sys.exit(0)

    except Exception as e:
        logging.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())