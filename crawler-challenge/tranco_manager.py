import asyncio
import aiohttp
import csv
import os
import gzip
import io
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple
from urllib.parse import urlparse, urljoin
import logging
import hashlib
from pathlib import Path

# work_logger import ì¶”ê°€
try:
    from work_logger import WorkLogger
except ImportError:
    WorkLogger = None

logger = logging.getLogger(__name__)

class TrancoManager:
    """Tranco Top 1M ë¦¬ìŠ¤íŠ¸ ê´€ë¦¬ì"""

    def __init__(self, data_dir="data"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)

        # Tranco ì„¤ì •
        self.base_url = "https://tranco-list.eu"
        self.current_list_url = f"{self.base_url}/list/NQKMX/full"
        self.cache_duration = timedelta(hours=24)  # 24ì‹œê°„ ìºì‹œ

        # íŒŒì¼ ê²½ë¡œ
        self.csv_file = self.data_dir / "tranco_top1m.csv"
        self.processed_file = self.data_dir / "tranco_urls.txt"
        self.metadata_file = self.data_dir / "tranco_metadata.txt"

        # ì‘ì—… ë¡œê±° ì´ˆê¸°í™”
        self.work_logger = WorkLogger() if WorkLogger else None

    async def download_latest_list(self, force_update=False) -> bool:
        """ìµœì‹  Tranco ë¦¬ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ"""
        try:
            # ìºì‹œ í™•ì¸
            if not force_update and self._is_cache_valid():
                logger.info("ìœ íš¨í•œ ìºì‹œ íŒŒì¼ ì¡´ì¬, ë‹¤ìš´ë¡œë“œ ê±´ë„ˆë›°ê¸°")
                return True

            logger.info("Tranco Top 1M ë¦¬ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")

            async with aiohttp.ClientSession() as session:
                # ìµœì‹  ë¦¬ìŠ¤íŠ¸ ID í™•ì¸
                list_id = await self._get_latest_list_id(session)
                if not list_id:
                    logger.error("ìµœì‹  ë¦¬ìŠ¤íŠ¸ ID íšë“ ì‹¤íŒ¨")
                    return False

                download_url = f"{self.base_url}/list/{list_id}/1000000"
                logger.info(f"ë‹¤ìš´ë¡œë“œ URL: {download_url}")

                async with session.get(download_url) as response:
                    if response.status != 200:
                        logger.error(f"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: HTTP {response.status}")
                        return False

                    content = await response.read()

                    # ì••ì¶•ëœ ë°ì´í„°ì¸ì§€ í™•ì¸
                    if content.startswith(b'\x1f\x8b'):  # gzip magic number
                        content = gzip.decompress(content)

                    # CSV íŒŒì¼ë¡œ ì €ì¥
                    with open(self.csv_file, 'wb') as f:
                        f.write(content)

                    # ë©”íƒ€ë°ì´í„° ì €ì¥
                    self._save_metadata(list_id, len(content))

                    logger.info(f"Tranco ë¦¬ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {len(content)} bytes")
                    return True

        except Exception as e:
            logger.error(f"Tranco ë¦¬ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    async def _get_latest_list_id(self, session: aiohttp.ClientSession) -> Optional[str]:
        """ìµœì‹  ë¦¬ìŠ¤íŠ¸ ID íšë“"""
        try:
            async with session.get(f"{self.base_url}/list/NQKMX") as response:
                if response.status == 200:
                    return "NQKMX"  # ê³ ì • ID ì‚¬ìš©
                return None
        except Exception as e:
            logger.error(f"ë¦¬ìŠ¤íŠ¸ ID íšë“ ì‹¤íŒ¨: {e}")
            return None

    def _is_cache_valid(self) -> bool:
        """ìºì‹œ ìœ íš¨ì„± ê²€ì‚¬"""
        if not self.csv_file.exists():
            return False

        file_time = datetime.fromtimestamp(self.csv_file.stat().st_mtime)
        return datetime.now() - file_time < self.cache_duration

    def _save_metadata(self, list_id: str, file_size: int):
        """ë©”íƒ€ë°ì´í„° ì €ì¥"""
        metadata = {
            'list_id': list_id,
            'download_time': datetime.now().isoformat(),
            'file_size': file_size,
            'file_path': str(self.csv_file)
        }

        with open(self.metadata_file, 'w') as f:
            for key, value in metadata.items():
                f.write(f"{key}: {value}\n")

    def parse_csv_to_urls(self, limit: Optional[int] = None,
                         start_rank: int = 1, add_www: bool = True) -> List[Dict[str, any]]:
        """CSVë¥¼ íŒŒì‹±í•˜ì—¬ URL ë¦¬ìŠ¤íŠ¸ ìƒì„±"""
        try:
            if not self.csv_file.exists():
                logger.error("Tranco CSV íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
                return []

            urls = []
            protocols = ['https://', 'http://']

            logger.info(f"CSV íŒŒì‹± ì‹œì‘ (ìˆœìœ„ {start_rank}ë¶€í„° {limit or 'ë'}ê¹Œì§€)")

            with open(self.csv_file, 'r', encoding='utf-8') as f:
                csv_reader = csv.reader(f)

                for row_num, row in enumerate(csv_reader, 1):
                    if row_num < start_rank:
                        continue

                    if limit and len(urls) >= limit:
                        break

                    if len(row) >= 2:
                        rank = int(row[0])
                        domain = row[1].strip()

                        # ê¸°ë³¸ ë„ë©”ì¸ URLë“¤ ìƒì„±
                        domain_urls = []

                        # í”„ë¡œí† ì½œë³„ ê¸°ë³¸ URL
                        for protocol in protocols:
                            domain_urls.append({
                                'url': f"{protocol}{domain}/",
                                'rank': rank,
                                'domain': domain,
                                'priority': self._calculate_priority(rank),
                                'url_type': 'root'
                            })

                            # www ë²„ì „ë„ ì¶”ê°€ (ì˜µì…˜)
                            if add_www and not domain.startswith('www.'):
                                domain_urls.append({
                                    'url': f"{protocol}www.{domain}/",
                                    'rank': rank,
                                    'domain': f"www.{domain}",
                                    'priority': self._calculate_priority(rank) - 1,
                                    'url_type': 'www'
                                })

                        urls.extend(domain_urls)

            logger.info(f"URL ìƒì„± ì™„ë£Œ: {len(urls)}ê°œ")
            return urls

        except Exception as e:
            logger.error(f"CSV íŒŒì‹± ì‹¤íŒ¨: {e}")
            return []

    def _calculate_priority(self, rank: int) -> int:
        """ìˆœìœ„ ê¸°ë°˜ ìš°ì„ ìˆœìœ„ ê³„ì‚°"""
        if rank <= 100:
            return 1000  # ìµœê³  ìš°ì„ ìˆœìœ„
        elif rank <= 1000:
            return 900
        elif rank <= 10000:
            return 800
        elif rank <= 100000:
            return 700
        else:
            return 600  # ê¸°ë³¸ ìš°ì„ ìˆœìœ„

    def generate_extended_urls(self, domain_data: List[Dict],
                              common_paths: Optional[List[str]] = None) -> List[Dict]:
        """ë„ë©”ì¸ì—ì„œ í™•ì¥ URL ìƒì„±"""
        if common_paths is None:
            common_paths = [
                '',  # ë£¨íŠ¸
                'about/',
                'contact/',
                'products/',
                'services/',
                'news/',
                'blog/',
                'support/',
                'help/',
                'privacy/',
                'terms/',
                'sitemap.xml',
                'robots.txt'
            ]

        extended_urls = []

        for domain_info in domain_data[:1000]:  # ìƒìœ„ 1000ê°œ ë„ë©”ì¸ë§Œ
            base_url = domain_info['url'].rstrip('/')
            domain = domain_info['domain']
            rank = domain_info['rank']
            base_priority = domain_info['priority']

            for i, path in enumerate(common_paths):
                extended_urls.append({
                    'url': f"{base_url}/{path}",
                    'rank': rank,
                    'domain': domain,
                    'priority': base_priority - i - 10,  # ê²½ë¡œë³„ë¡œ ìš°ì„ ìˆœìœ„ ì¡°ì •
                    'url_type': 'extended',
                    'path': path
                })

        logger.info(f"í™•ì¥ URL ìƒì„± ì™„ë£Œ: {len(extended_urls)}ê°œ")
        return extended_urls

    def save_urls_to_file(self, urls: List[Dict], filename: Optional[str] = None):
        """URL ë¦¬ìŠ¤íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        if filename is None:
            filename = self.processed_file

        try:
            with open(filename, 'w', encoding='utf-8') as f:
                for url_info in urls:
                    line = f"{url_info['priority']},{url_info['rank']},{url_info['domain']},{url_info['url']},{url_info['url_type']}\n"
                    f.write(line)

            logger.info(f"URL ë¦¬ìŠ¤íŠ¸ ì €ì¥ ì™„ë£Œ: {filename} ({len(urls)}ê°œ)")

        except Exception as e:
            logger.error(f"URL ë¦¬ìŠ¤íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")

    def load_urls_from_file(self, filename: Optional[str] = None, limit: Optional[int] = None) -> List[Dict]:
        """íŒŒì¼ì—ì„œ URL ë¦¬ìŠ¤íŠ¸ ë¡œë“œ"""
        if filename is None:
            filename = self.processed_file

        try:
            if not Path(filename).exists():
                logger.error(f"URL íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {filename}")
                return []

            urls = []
            with open(filename, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    if limit and len(urls) >= limit:
                        break

                    line = line.strip()
                    if line:
                        parts = line.split(',', 4)
                        if len(parts) >= 5:
                            urls.append({
                                'priority': int(parts[0]),
                                'rank': int(parts[1]),
                                'domain': parts[2],
                                'url': parts[3],
                                'url_type': parts[4]
                            })

            logger.info(f"URL ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ì™„ë£Œ: {len(urls)}ê°œ")
            return urls

        except Exception as e:
            logger.error(f"URL ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []

    async def prepare_url_dataset(self, initial_count: int = 10000,
                                 force_update: bool = False) -> List[Dict]:
        """URL ë°ì´í„°ì…‹ ì¤€ë¹„"""
        logger.info(f"URL ë°ì´í„°ì…‹ ì¤€ë¹„ ì‹œì‘ (ëª©í‘œ: {initial_count}ê°œ)")

        # 1. Tranco ë¦¬ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ
        if not await self.download_latest_list(force_update):
            logger.error("Tranco ë¦¬ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
            return []

        # 2. ê¸°ë³¸ URL ìƒì„± (ìƒìœ„ Nê°œ ë„ë©”ì¸)
        domain_count = min(initial_count // 4, 2500)  # ë„ë©”ì¸ë‹¹ í‰ê·  4ê°œ URL
        basic_urls = self.parse_csv_to_urls(
            limit=domain_count,
            start_rank=1,
            add_www=True
        )

        # 3. í™•ì¥ URL ìƒì„± (ìƒìœ„ ë„ë©”ì¸ë“¤)
        if len(basic_urls) > 0:
            # ìƒìœ„ 500ê°œ ë„ë©”ì¸ì—ì„œ í™•ì¥ URL ìƒì„±
            top_domains = [url for url in basic_urls if url['rank'] <= 500]
            extended_urls = self.generate_extended_urls(
                top_domains,
                common_paths=['', 'about/', 'contact/', 'products/', 'news/']
            )

            # ê¸°ë³¸ + í™•ì¥ URL í•©ì¹˜ê¸°
            all_urls = basic_urls + extended_urls
        else:
            all_urls = basic_urls

        # 4. ìš°ì„ ìˆœìœ„ë³„ ì •ë ¬
        all_urls.sort(key=lambda x: x['priority'], reverse=True)

        # 5. ëª©í‘œ ê°œìˆ˜ë§Œí¼ ìë¥´ê¸°
        if len(all_urls) > initial_count:
            all_urls = all_urls[:initial_count]

        # 6. íŒŒì¼ë¡œ ì €ì¥
        self.save_urls_to_file(all_urls)

        logger.info(f"URL ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ: {len(all_urls)}ê°œ")
        return all_urls

    def get_domain_stats(self, urls: List[Dict]) -> Dict[str, int]:
        """ë„ë©”ì¸ë³„ í†µê³„"""
        domain_count = {}
        for url_info in urls:
            domain = url_info['domain']
            domain_count[domain] = domain_count.get(domain, 0) + 1

        return dict(sorted(domain_count.items(), key=lambda x: x[1], reverse=True))

async def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    logging.basicConfig(level=logging.INFO)

    manager = TrancoManager()

    # URL ë°ì´í„°ì…‹ ì¤€ë¹„
    urls = await manager.prepare_url_dataset(initial_count=1000, force_update=False)

    if urls:
        print(f"\nâœ… ì´ {len(urls)}ê°œ URL ì¤€ë¹„ ì™„ë£Œ")

        # ìƒìœ„ 10ê°œ ì¶œë ¥
        print("\nğŸ“Š ìƒìœ„ 10ê°œ URL:")
        for i, url_info in enumerate(urls[:10], 1):
            print(f"{i:2d}. [{url_info['priority']:4d}] {url_info['url']} (ìˆœìœ„: {url_info['rank']})")

        # ë„ë©”ì¸ í†µê³„
        domain_stats = manager.get_domain_stats(urls)
        print(f"\nğŸŒ ìƒìœ„ 5ê°œ ë„ë©”ì¸ë³„ URL ìˆ˜:")
        for domain, count in list(domain_stats.items())[:5]:
            print(f"  {domain}: {count}ê°œ")

    else:
        print("âŒ URL ë°ì´í„°ì…‹ ì¤€ë¹„ ì‹¤íŒ¨")

if __name__ == "__main__":
    asyncio.run(main())