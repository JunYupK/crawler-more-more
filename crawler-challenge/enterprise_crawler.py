#!/usr/bin/env python3
"""
Enterprise Crawler - Tranco Top 1M ëŒ€ê·œëª¨ ì •ì¤‘í•œ í¬ë¡¤ëŸ¬
"""

import asyncio
import logging
import sys
import signal
from datetime import datetime
from typing import List, Dict, Any, Optional

from tranco_manager import TrancoManager
from redis_queue_manager import RedisQueueManager
from polite_crawler import PoliteCrawler
from database import DatabaseManager
from progress_tracker import ProgressTracker
from monitoring.metrics import MetricsMonitor

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('enterprise_crawler.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class EnterpriseCrawler:
    """ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ëŒ€ê·œëª¨ í¬ë¡¤ëŸ¬"""

    def __init__(self, initial_url_count: int = 1000):
        self.initial_url_count = initial_url_count
        self.should_stop = False
        self.batch_size = 50

        # ì»´í¬ë„ŒíŠ¸ë“¤
        self.tranco_manager: Optional[TrancoManager] = None
        self.queue_manager: Optional[RedisQueueManager] = None
        self.polite_crawler: Optional[PoliteCrawler] = None
        self.db_manager: Optional[DatabaseManager] = None
        self.progress_tracker: Optional[ProgressTracker] = None
        self.metrics_monitor: Optional[MetricsMonitor] = None

        # í†µê³„
        self.total_processed = 0
        self.total_successful = 0
        self.total_failed = 0
        self.start_time = datetime.now()

        # ì‹ í˜¸ í•¸ë“¤ëŸ¬
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)

    def _signal_handler(self, signum, frame):
        """ì¢…ë£Œ ì‹ í˜¸ ì²˜ë¦¬"""
        logger.info(f"ì¢…ë£Œ ì‹ í˜¸ ìˆ˜ì‹ : {signum}")
        self.should_stop = True

    async def initialize(self) -> bool:
        """ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        try:
            logger.info("=== Enterprise Crawler ì´ˆê¸°í™” ì‹œì‘ ===")

            # 1. Tranco Manager
            logger.info("1. Tranco Manager ì´ˆê¸°í™”...")
            self.tranco_manager = TrancoManager()

            # 2. Redis Queue Manager
            logger.info("2. Redis Queue Manager ì´ˆê¸°í™”...")
            self.queue_manager = RedisQueueManager()
            if not self.queue_manager.test_connection():
                logger.error("Redis ì—°ê²° ì‹¤íŒ¨")
                return False

            # 3. Database Manager
            logger.info("3. Database Manager ì´ˆê¸°í™”...")
            self.db_manager = DatabaseManager()
            self.db_manager.connect()

            # 4. Progress Tracker
            logger.info("4. Progress Tracker ì´ˆê¸°í™”...")
            self.progress_tracker = ProgressTracker()
            if not self.progress_tracker.test_connection():
                logger.error("Progress Tracker Redis ì—°ê²° ì‹¤íŒ¨")
                return False

            # 5. Metrics Monitor
            logger.info("5. Metrics Monitor ì´ˆê¸°í™”...")
            self.metrics_monitor = MetricsMonitor()
            await self.metrics_monitor.start_monitoring()

            # 6. Polite Crawler
            logger.info("6. Polite Crawler ì´ˆê¸°í™”...")
            self.polite_crawler = PoliteCrawler()
            await self.polite_crawler.__aenter__()

            logger.info("âœ… ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            return True

        except Exception as e:
            logger.error(f"ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    async def prepare_url_dataset(self) -> bool:
        """URL ë°ì´í„°ì…‹ ì¤€ë¹„"""
        try:
            logger.info(f"=== URL ë°ì´í„°ì…‹ ì¤€ë¹„ ({self.initial_url_count}ê°œ) ===")

            # Tranco ë¦¬ìŠ¤íŠ¸ì—ì„œ URL ìƒì„±
            urls = await self.tranco_manager.prepare_url_dataset(
                initial_count=self.initial_url_count,
                force_update=False
            )

            if not urls:
                logger.error("URL ë°ì´í„°ì…‹ ì¤€ë¹„ ì‹¤íŒ¨")
                return False

            # Redis íì— ë¡œë“œ
            if not self.queue_manager.initialize_queues(urls):
                logger.error("í ì´ˆê¸°í™” ì‹¤íŒ¨")
                return False

            # í†µê³„ ì¶œë ¥
            queue_stats = self.queue_manager.get_queue_stats()
            logger.info(f"í ë¡œë”© ì™„ë£Œ:")
            logger.info(f"  - ê³ ìš°ì„ ìˆœìœ„: {queue_stats.get('queue_priority_high', 0)}")
            logger.info(f"  - ì¤‘ìš°ì„ ìˆœìœ„: {queue_stats.get('queue_priority_medium', 0)}")
            logger.info(f"  - ì¼ë°˜ìš°ì„ ìˆœìœ„: {queue_stats.get('queue_priority_normal', 0)}")
            logger.info(f"  - ì €ìš°ì„ ìˆœìœ„: {queue_stats.get('queue_priority_low', 0)}")
            logger.info(f"  - ì´ URL: {queue_stats.get('total_urls', 0)}ê°œ")

            return True

        except Exception as e:
            logger.error(f"URL ë°ì´í„°ì…‹ ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            return False

    async def crawl_batch(self) -> bool:
        """ë‹¨ì¼ ë°°ì¹˜ í¬ë¡¤ë§"""
        try:
            # íì—ì„œ ë‹¤ìŒ ë°°ì¹˜ ê°€ì ¸ì˜¤ê¸°
            batch = self.queue_manager.get_next_batch(self.batch_size)
            if not batch:
                logger.info("íì— ë” ì´ìƒ URLì´ ì—†ìŠµë‹ˆë‹¤")
                return False

            batch_urls = [item['url'] for item in batch]
            logger.info(f"ë°°ì¹˜ í¬ë¡¤ë§ ì‹œì‘: {len(batch_urls)}ê°œ URL")

            # ì •ì¤‘í•œ í¬ë¡¤ë§ ì‹¤í–‰
            results = await self.polite_crawler.crawl_batch_politely(batch_urls)

            # ê²°ê³¼ ì²˜ë¦¬
            successful_count = 0
            failed_count = 0

            for result in results:
                try:
                    if result['success'] and result.get('content'):
                        # ì„±ê³µ - ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                        self.db_manager.add_to_batch(result['url'], result['content'])
                        self.queue_manager.mark_completed(result['url'], success=True)
                        successful_count += 1

                        if self.metrics_monitor:
                            self.metrics_monitor.increment_pages()

                    else:
                        # ì‹¤íŒ¨ ì²˜ë¦¬
                        error_info = {
                            'type': 'crawl_error',
                            'message': result.get('error', 'Unknown error'),
                            'recoverable': 'timeout' in result.get('error', '').lower()
                        }

                        self.queue_manager.mark_completed(
                            result['url'],
                            success=False,
                            error_info=error_info
                        )
                        failed_count += 1

                        # Progress trackerì— ì—ëŸ¬ ì €ì¥
                        if self.progress_tracker:
                            self.progress_tracker.save_error(error_info)

                except Exception as e:
                    logger.error(f"ê²°ê³¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    failed_count += 1

            # í†µê³„ ì—…ë°ì´íŠ¸
            self.total_processed += len(results)
            self.total_successful += successful_count
            self.total_failed += failed_count

            logger.info(f"ë°°ì¹˜ ì™„ë£Œ: {successful_count}ê°œ ì„±ê³µ, {failed_count}ê°œ ì‹¤íŒ¨")
            return True

        except Exception as e:
            logger.error(f"ë°°ì¹˜ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return False

    def save_progress(self):
        """ì§„í–‰ ìƒí™© ì €ì¥"""
        try:
            if not self.progress_tracker:
                return

            uptime_minutes = int((datetime.now() - self.start_time).total_seconds() / 60)
            pages_per_minute = self.total_processed / uptime_minutes if uptime_minutes > 0 else 0

            # í ìƒíƒœ
            queue_stats = self.queue_manager.get_queue_stats()

            # ë„ë©”ì¸ í†µê³„
            domain_stats = self.queue_manager.get_domain_stats()

            # í¬ë¡¤ëŸ¬ í†µê³„
            crawler_summary = self.polite_crawler.get_crawling_summary()

            stats = {
                'total_processed': self.total_processed,
                'success_count': self.total_successful,
                'error_count': self.total_failed,
                'error_rate': self.total_failed / self.total_processed if self.total_processed > 0 else 0,
                'pages_per_minute': round(pages_per_minute, 2),
                'uptime_minutes': uptime_minutes,

                # í ìƒíƒœ
                'queue_pending': queue_stats.get('total_pending', 0),
                'queue_completed': queue_stats.get('completed', 0),
                'completion_rate': queue_stats.get('completion_rate', 0),

                # í¬ë¡¤ëŸ¬ ìƒíƒœ
                'crawler_domains': crawler_summary.get('total_domains', 0),
                'crawler_blocked_domains': crawler_summary.get('blocked_domains', 0),
                'crawler_average_delay': crawler_summary.get('average_delay', 0),

                # DB ìƒíƒœ
                'db_pages_count': self.db_manager.get_crawled_count() if self.db_manager else 0
            }

            self.progress_tracker.save_progress(stats)

        except Exception as e:
            logger.error(f"ì§„í–‰ ìƒí™© ì €ì¥ ì‹¤íŒ¨: {e}")

    async def run_continuous(self):
        """ì—°ì† í¬ë¡¤ë§ ì‹¤í–‰"""
        logger.info("=== ì—°ì† í¬ë¡¤ë§ ì‹œì‘ ===")

        cycle_count = 0
        last_progress_save = datetime.now()

        try:
            while not self.should_stop:
                cycle_count += 1
                logger.info(f"\n--- ì‚¬ì´í´ #{cycle_count} ---")

                # ë°°ì¹˜ í¬ë¡¤ë§
                has_more = await self.crawl_batch()

                if not has_more:
                    logger.info("ëª¨ë“  URL í¬ë¡¤ë§ ì™„ë£Œ")
                    break

                # ì¬ì‹œë„ ë°°ì¹˜ ì²˜ë¦¬
                retry_batch = self.queue_manager.get_retry_batch(20)
                if retry_batch:
                    logger.info(f"ì¬ì‹œë„ ë°°ì¹˜ ì²˜ë¦¬: {len(retry_batch)}ê°œ")
                    retry_urls = [item['url'] for item in retry_batch]
                    retry_results = await self.polite_crawler.crawl_batch_politely(retry_urls)

                    # ì¬ì‹œë„ ê²°ê³¼ ì²˜ë¦¬
                    for result in retry_results:
                        if result['success'] and result.get('content'):
                            self.db_manager.add_to_batch(result['url'], result['content'])
                            self.queue_manager.mark_completed(result['url'], success=True)
                            self.total_successful += 1
                        else:
                            self.total_failed += 1

                    self.total_processed += len(retry_results)

                # ì£¼ê¸°ì ìœ¼ë¡œ ì§„í–‰ ìƒí™© ì €ì¥ (5ë¶„ë§ˆë‹¤)
                if (datetime.now() - last_progress_save).total_seconds() > 300:
                    self.save_progress()
                    last_progress_save = datetime.now()

                    # í í†µê³„ ì¶œë ¥
                    queue_stats = self.queue_manager.get_queue_stats()
                    logger.info(f"ğŸ“Š ì§„í–‰ìƒí™©: {queue_stats.get('completed', 0)}ê°œ ì™„ë£Œ, "
                              f"{queue_stats.get('total_pending', 0)}ê°œ ëŒ€ê¸° "
                              f"(ì™„ë£Œìœ¨: {queue_stats.get('completion_rate', 0):.1%})")

                # ì§§ì€ ëŒ€ê¸°
                await asyncio.sleep(2)

        except Exception as e:
            logger.error(f"ì—°ì† í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")

        finally:
            # ìµœì¢… ì§„í–‰ ìƒí™© ì €ì¥
            self.save_progress()

    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            logger.info("=== ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œì‘ ===")

            if self.polite_crawler:
                await self.polite_crawler.__aexit__(None, None, None)

            if self.db_manager:
                self.db_manager.disconnect()

            if self.metrics_monitor:
                await self.metrics_monitor.stop_monitoring()

            if self.progress_tracker:
                self.progress_tracker.mark_session_completed()

            logger.info("âœ… ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")

        except Exception as e:
            logger.error(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì˜¤ë¥˜: {e}")

    def print_final_stats(self):
        """ìµœì¢… í†µê³„ ì¶œë ¥"""
        duration = datetime.now() - self.start_time
        duration_minutes = duration.total_seconds() / 60

        logger.info("\n" + "="*60)
        logger.info("               ìµœì¢… í¬ë¡¤ë§ í†µê³„")
        logger.info("="*60)
        logger.info(f"ì‹¤í–‰ ì‹œê°„      : {duration_minutes:.1f}ë¶„")
        logger.info(f"ì´ ì²˜ë¦¬        : {self.total_processed:,}ê°œ")
        logger.info(f"ì„±ê³µ          : {self.total_successful:,}ê°œ")
        logger.info(f"ì‹¤íŒ¨          : {self.total_failed:,}ê°œ")

        if self.total_processed > 0:
            success_rate = self.total_successful / self.total_processed * 100
            pages_per_minute = self.total_processed / duration_minutes if duration_minutes > 0 else 0

            logger.info(f"ì„±ê³µë¥         : {success_rate:.1f}%")
            logger.info(f"ì²˜ë¦¬ì†ë„      : {pages_per_minute:.1f} pages/ë¶„")

        # ë„ë©”ì¸ í†µê³„
        if self.polite_crawler:
            crawler_summary = self.polite_crawler.get_crawling_summary()
            logger.info(f"ì²˜ë¦¬ ë„ë©”ì¸    : {crawler_summary.get('total_domains', 0)}ê°œ")
            logger.info(f"ì°¨ë‹¨ ë„ë©”ì¸    : {crawler_summary.get('blocked_domains', 0)}ê°œ")
            logger.info(f"í‰ê·  ë”œë ˆì´    : {crawler_summary.get('average_delay', 0):.1f}ì´ˆ")

        # DB í†µê³„
        if self.db_manager:
            db_count = self.db_manager.get_crawled_count()
            logger.info(f"DB ì €ì¥       : {db_count:,}ê°œ")

        logger.info("="*60)

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(description='Enterprise Crawler - Tranco Top 1M')
    parser.add_argument('--count', type=int, default=1000,
                       help='í¬ë¡¤ë§í•  URL ê°œìˆ˜ (ê¸°ë³¸: 1000)')
    parser.add_argument('--batch-size', type=int, default=50,
                       help='ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 50)')

    args = parser.parse_args()

    # í¬ë¡¤ëŸ¬ ìƒì„±
    crawler = EnterpriseCrawler(initial_url_count=args.count)
    crawler.batch_size = args.batch_size

    try:
        # ì´ˆê¸°í™”
        if not await crawler.initialize():
            logger.error("í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì‹¤íŒ¨")
            sys.exit(1)

        # URL ë°ì´í„°ì…‹ ì¤€ë¹„
        if not await crawler.prepare_url_dataset():
            logger.error("URL ë°ì´í„°ì…‹ ì¤€ë¹„ ì‹¤íŒ¨")
            sys.exit(1)

        # ì—°ì† í¬ë¡¤ë§ ì‹¤í–‰
        await crawler.run_continuous()

    except KeyboardInterrupt:
        logger.info("ì‚¬ìš©ìì— ì˜í•œ ì¤‘ë‹¨")

    except Exception as e:
        logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")

    finally:
        # ì •ë¦¬
        await crawler.cleanup()
        crawler.print_final_stats()

if __name__ == "__main__":
    asyncio.run(main())