#!/usr/bin/env python3
"""
Multithreaded Enterprise Crawler - ë©€í‹°ìŠ¤ë ˆë”©ì„ í™œìš©í•œ ê³ ì„±ëŠ¥ í¬ë¡¤ëŸ¬
"""

import asyncio
import logging
import sys
import signal
import threading
import concurrent.futures
from datetime import datetime
from typing import List, Dict, Any, Optional
from queue import Queue, Empty
import time

from tranco_manager import TrancoManager
from redis_queue_manager import RedisQueueManager
from polite_crawler import PoliteCrawler
from database import DatabaseManager
from progress_tracker import ProgressTracker
from monitoring.metrics import MetricsMonitor

# Config import ì¶”ê°€
try:
    from config.settings import WORKER_THREADS, BATCH_SIZE
except ImportError:
    WORKER_THREADS = 16
    BATCH_SIZE = 100

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('multithreaded_crawler.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

class MultithreadedCrawler:
    """ë©€í‹°ìŠ¤ë ˆë”©ì„ í™œìš©í•œ ëŒ€ê·œëª¨ í¬ë¡¤ëŸ¬"""

    def __init__(self, initial_url_count: int = 1000, worker_threads: int = None):
        self.initial_url_count = initial_url_count
        self.worker_threads = worker_threads if worker_threads is not None else WORKER_THREADS  # ê¸°ë³¸ 16ê°œ
        self.should_stop = False
        self.batch_size = BATCH_SIZE  # ê³µê²©ì  ë°°ì¹˜ í¬ê¸° 100

        # ì»´í¬ë„ŒíŠ¸ë“¤
        self.tranco_manager: Optional[TrancoManager] = None
        self.queue_manager: Optional[RedisQueueManager] = None
        self.db_manager: Optional[DatabaseManager] = None
        self.progress_tracker: Optional[ProgressTracker] = None
        self.metrics_monitor: Optional[MetricsMonitor] = None

        # ë©€í‹°ìŠ¤ë ˆë”© ê´€ë ¨
        self.thread_pool = None
        self.worker_queue = Queue()
        self.result_queue = Queue()
        self.active_workers = 0
        self.workers_lock = threading.Lock()

        # í†µê³„
        self.total_processed = 0
        self.total_successful = 0
        self.total_failed = 0
        self.start_time = datetime.now()
        self.stats_lock = threading.Lock()

        # ì‹ í˜¸ í•¸ë“¤ëŸ¬
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)

    def _signal_handler(self, signum, frame):
        """ì¢…ë£Œ ì‹ í˜¸ ì²˜ë¦¬"""
        logger.info(f"ì¢…ë£Œ ì‹ í˜¸ ìˆ˜ì‹ : {signum}")
        self.should_stop = True

    async def initialize(self) -> bool:
        """ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        try:
            logger.info("=== Multithreaded Crawler ì´ˆê¸°í™” ì‹œì‘ ===")

            # 1. Tranco Manager
            logger.info("1. Tranco Manager ì´ˆê¸°í™”...")
            self.tranco_manager = TrancoManager()

            # 2. Redis Queue Manager
            logger.info("2. Redis Queue Manager ì´ˆê¸°í™”...")
            self.queue_manager = RedisQueueManager()
            if not self.queue_manager.test_connection():
                logger.error("Redis ì—°ê²° ì‹¤íŒ¨")
                return False

            # 3. Database Manager
            logger.info("3. Database Manager ì´ˆê¸°í™”...")
            self.db_manager = DatabaseManager()

            # 4. Progress Tracker
            logger.info("4. Progress Tracker ì´ˆê¸°í™”...")
            self.progress_tracker = ProgressTracker()
            if not self.progress_tracker.test_connection():
                logger.error("Progress Tracker Redis ì—°ê²° ì‹¤íŒ¨")
                return False

            # 5. Metrics Monitor
            logger.info("5. Metrics Monitor ì´ˆê¸°í™”...")
            self.metrics_monitor = MetricsMonitor()
            await self.metrics_monitor.start_monitoring()

            # 6. Thread Pool ì´ˆê¸°í™”
            logger.info(f"6. Thread Pool ì´ˆê¸°í™” ({self.worker_threads}ê°œ ì›Œì»¤)...")
            self.thread_pool = concurrent.futures.ThreadPoolExecutor(
                max_workers=self.worker_threads,
                thread_name_prefix="CrawlerWorker"
            )

            logger.info("[OK] ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            return True

        except Exception as e:
            logger.error(f"ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    async def prepare_url_dataset(self) -> bool:
        """URL ë°ì´í„°ì…‹ ì¤€ë¹„"""
        try:
            logger.info(f"=== URL ë°ì´í„°ì…‹ ì¤€ë¹„ ({self.initial_url_count}ê°œ) ===")

            # Tranco ë¦¬ìŠ¤íŠ¸ì—ì„œ URL ìƒì„±
            urls = await self.tranco_manager.prepare_url_dataset(
                initial_count=self.initial_url_count
            )

            if not urls:
                logger.error("URL ë°ì´í„°ì…‹ ì¤€ë¹„ ì‹¤íŒ¨")
                return False

            # Redis íì— ë¡œë“œ
            if not self.queue_manager.initialize_queues(urls):
                logger.error("í ì´ˆê¸°í™” ì‹¤íŒ¨")
                return False

            # í†µê³„ ì¶œë ¥
            queue_stats = self.queue_manager.get_queue_stats()
            logger.info(f"í ë¡œë”© ì™„ë£Œ:")
            logger.info(f"  - ê³ ìš°ì„ ìˆœìœ„: {queue_stats.get('queue_priority_high', 0)}")
            logger.info(f"  - ì¤‘ìš°ì„ ìˆœìœ„: {queue_stats.get('queue_priority_medium', 0)}")
            logger.info(f"  - ì¼ë°˜ìš°ì„ ìˆœìœ„: {queue_stats.get('queue_priority_normal', 0)}")
            logger.info(f"  - ì €ìš°ì„ ìˆœìœ„: {queue_stats.get('queue_priority_low', 0)}")
            logger.info(f"  - ì´ URL: {queue_stats.get('total_urls', 0)}ê°œ")

            return True

        except Exception as e:
            logger.error(f"URL ë°ì´í„°ì…‹ ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            return False

    def crawl_worker(self, worker_id: int):
        """ì›Œì»¤ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ë˜ëŠ” í¬ë¡¤ë§ í•¨ìˆ˜"""
        logger.info(f"Worker {worker_id} ì‹œì‘")

        # ê° ì›Œì»¤ë³„ë¡œ ë…ë¦½ì ì¸ í¬ë¡¤ëŸ¬ ìƒì„±
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        try:
            # PoliteCrawlerë¥¼ ë™ê¸° ë°©ì‹ìœ¼ë¡œ ì´ˆê¸°í™”
            crawler = PoliteCrawler(respect_robots_txt=False)

            # asyncio context manager ì„¤ì •
            async def init_crawler():
                await crawler.__aenter__()
                return crawler

            crawler = loop.run_until_complete(init_crawler())

            while not self.should_stop:
                try:
                    # ì‘ì—… íì—ì„œ ë°°ì¹˜ ê°€ì ¸ì˜¤ê¸°
                    batch = self.worker_queue.get(timeout=5)
                    if batch is None:  # ì¢…ë£Œ ì‹ í˜¸
                        break

                    with self.workers_lock:
                        self.active_workers += 1

                    try:
                        # ë¹„ë™ê¸° í¬ë¡¤ë§ ì‹¤í–‰
                        results = loop.run_until_complete(
                            crawler.crawl_batch_politely([item['url'] for item in batch])
                        )

                        # ê²°ê³¼ë¥¼ ê²°ê³¼ íì— ì¶”ê°€
                        self.result_queue.put({
                            'worker_id': worker_id,
                            'batch': batch,
                            'results': results,
                            'timestamp': datetime.now()
                        })

                    finally:
                        with self.workers_lock:
                            self.active_workers -= 1
                        self.worker_queue.task_done()

                except Empty:
                    continue
                except Exception as e:
                    logger.error(f"Worker {worker_id} ì˜¤ë¥˜: {e}")
                    with self.workers_lock:
                        self.active_workers -= 1

        except Exception as e:
            logger.error(f"Worker {worker_id} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        finally:
            # í¬ë¡¤ëŸ¬ ì •ë¦¬
            try:
                loop.run_until_complete(crawler.__aexit__(None, None, None))
            except:
                pass
            loop.close()
            logger.info(f"Worker {worker_id} ì¢…ë£Œ")

    async def process_results(self):
        """ê²°ê³¼ ì²˜ë¦¬ ì½”ë£¨í‹´"""
        while not self.should_stop:
            try:
                # ê²°ê³¼ íì—ì„œ ì²˜ë¦¬í•  ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
                if self.result_queue.empty():
                    await asyncio.sleep(0.1)
                    continue

                try:
                    result_data = self.result_queue.get_nowait()
                except Empty:
                    continue

                batch = result_data['batch']
                results = result_data['results']
                worker_id = result_data['worker_id']

                # ê²°ê³¼ ì²˜ë¦¬
                successful_count = 0
                failed_count = 0

                for result in results:
                    try:
                        if result['success'] and result.get('content'):
                            # ì„±ê³µ - ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                            self.db_manager.add_to_batch(result['url'], result['content'])
                            self.queue_manager.mark_completed(result['url'], success=True)
                            successful_count += 1

                            if self.metrics_monitor:
                                self.metrics_monitor.increment_pages()

                        else:
                            # ì‹¤íŒ¨ ì²˜ë¦¬
                            error_message = result.get('error', 'Unknown error')
                            error_info = {
                                'type': 'crawl_error',
                                'message': error_message,
                                'recoverable': 'timeout' in error_message.lower()
                            }

                            self.queue_manager.mark_completed(
                                result['url'],
                                success=False,
                                error_info=error_info
                            )
                            failed_count += 1

                            # Progress trackerì— ì—ëŸ¬ ì €ì¥
                            if self.progress_tracker:
                                self.progress_tracker.save_error(error_info)

                    except Exception as e:
                        logger.error(f"ê²°ê³¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                        failed_count += 1

                # í†µê³„ ì—…ë°ì´íŠ¸
                with self.stats_lock:
                    self.total_processed += len(results)
                    self.total_successful += successful_count
                    self.total_failed += failed_count

                logger.info(f"Worker {worker_id} ë°°ì¹˜ ì™„ë£Œ: {successful_count}ê°œ ì„±ê³µ, {failed_count}ê°œ ì‹¤íŒ¨")

            except Exception as e:
                logger.error(f"ê²°ê³¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    async def run_continuous(self):
        """ì—°ì† í¬ë¡¤ë§ ì‹¤í–‰"""
        logger.info("=== ë©€í‹°ìŠ¤ë ˆë”© ì—°ì† í¬ë¡¤ë§ ì‹œì‘ ===")

        # ì›Œì»¤ ìŠ¤ë ˆë“œë“¤ ì‹œì‘
        logger.info(f"{self.worker_threads}ê°œ ì›Œì»¤ ìŠ¤ë ˆë“œ ì‹œì‘...")
        worker_futures = []
        for i in range(self.worker_threads):
            future = self.thread_pool.submit(self.crawl_worker, i)
            worker_futures.append(future)

        # ê²°ê³¼ ì²˜ë¦¬ íƒœìŠ¤í¬ ì‹œì‘
        result_processor = asyncio.create_task(self.process_results())

        cycle_count = 0
        last_progress_save = datetime.now()

        try:
            while not self.should_stop:
                cycle_count += 1
                logger.info(f"\n--- ì‚¬ì´í´ #{cycle_count} ---")

                # íì—ì„œ ë‹¤ìŒ ë°°ì¹˜ë“¤ ê°€ì ¸ì˜¤ê¸°
                total_batch_size = self.batch_size * self.worker_threads
                batch = self.queue_manager.get_next_batch(total_batch_size)

                if not batch:
                    logger.info("íì— ë” ì´ìƒ URLì´ ì—†ìŠµë‹ˆë‹¤")
                    break

                # ë°°ì¹˜ë¥¼ ì›Œì»¤ë“¤ì—ê²Œ ë¶„ë°°
                batch_per_worker = len(batch) // self.worker_threads
                for i in range(self.worker_threads):
                    start_idx = i * batch_per_worker
                    end_idx = start_idx + batch_per_worker if i < self.worker_threads - 1 else len(batch)
                    worker_batch = batch[start_idx:end_idx]

                    if worker_batch:
                        self.worker_queue.put(worker_batch)

                logger.info(f"ë°°ì¹˜ ë¶„ë°° ì™„ë£Œ: {len(batch)}ê°œ URLì„ {self.worker_threads}ê°œ ì›Œì»¤ì—ê²Œ í• ë‹¹")

                # ì›Œì»¤ë“¤ì´ ì‘ì—…ì„ ì™„ë£Œí•  ë•Œê¹Œì§€ ëŒ€ê¸°
                while self.active_workers > 0 and not self.should_stop:
                    await asyncio.sleep(1)

                # ì£¼ê¸°ì ìœ¼ë¡œ ì§„í–‰ ìƒí™© ì €ì¥ (5ë¶„ë§ˆë‹¤)
                if (datetime.now() - last_progress_save).total_seconds() > 300:
                    self.save_progress()
                    last_progress_save = datetime.now()

                    # í í†µê³„ ì¶œë ¥
                    queue_stats = self.queue_manager.get_queue_stats()
                    logger.info(f"ğŸ“Š ì§„í–‰ìƒí™©: {queue_stats.get('completed', 0)}ê°œ ì™„ë£Œ, "
                              f"{queue_stats.get('total_pending', 0)}ê°œ ëŒ€ê¸° "
                              f"(ì™„ë£Œìœ¨: {queue_stats.get('completion_rate', 0):.1%})")

                # ì§§ì€ ëŒ€ê¸°
                await asyncio.sleep(2)

        except Exception as e:
            logger.error(f"ì—°ì† í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")

        finally:
            # ì›Œì»¤ë“¤ì—ê²Œ ì¢…ë£Œ ì‹ í˜¸ ì „ì†¡
            for _ in range(self.worker_threads):
                self.worker_queue.put(None)

            # ê²°ê³¼ ì²˜ë¦¬ íƒœìŠ¤í¬ ì¢…ë£Œ
            result_processor.cancel()

            # ìµœì¢… ì§„í–‰ ìƒí™© ì €ì¥
            self.save_progress()

    def save_progress(self):
        """ì§„í–‰ ìƒí™© ì €ì¥"""
        try:
            if not self.progress_tracker:
                return

            uptime_minutes = int((datetime.now() - self.start_time).total_seconds() / 60)
            pages_per_minute = self.total_processed / uptime_minutes if uptime_minutes > 0 else 0

            # í ìƒíƒœ
            queue_stats = self.queue_manager.get_queue_stats()

            # ë„ë©”ì¸ í†µê³„
            domain_stats = self.queue_manager.get_domain_stats()

            stats = {
                'total_processed': self.total_processed,
                'success_count': self.total_successful,
                'error_count': self.total_failed,
                'error_rate': self.total_failed / self.total_processed if self.total_processed > 0 else 0,
                'pages_per_minute': round(pages_per_minute, 2),
                'uptime_minutes': uptime_minutes,
                'queue_pending': queue_stats.get('total_pending', 0),
                'queue_completed': queue_stats.get('completed', 0),
                'completion_rate': queue_stats.get('completion_rate', 0),
                'worker_threads': self.worker_threads,
                'active_workers': self.active_workers,
            }

            # DB ìƒíƒœ
            db_pool_stats = self.db_manager.get_pool_stats() if self.db_manager else {}
            stats.update({
                'db_pages_count': self.db_manager.get_crawled_count() if self.db_manager else 0,
                'db_pool_min': db_pool_stats.get('pool_min', 0),
                'db_pool_max': db_pool_stats.get('pool_max', 0),
            })

            self.progress_tracker.save_progress(stats)
        except Exception as e:
            logger.error(f"ì§„í–‰ ìƒí™© ì €ì¥ ì‹¤íŒ¨: {e}")

    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            logger.info("=== ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œì‘ ===")

            if self.thread_pool:
                self.thread_pool.shutdown(wait=True)

            if self.db_manager:
                self.db_manager.flush_batch()
                self.db_manager.close_all_connections()

            if self.metrics_monitor:
                await self.metrics_monitor.stop_monitoring()

            if self.progress_tracker:
                self.progress_tracker.mark_session_completed()

            logger.info("[OK] ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")

        except Exception as e:
            logger.error(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì˜¤ë¥˜: {e}")

    def print_final_stats(self):
        """ìµœì¢… í†µê³„ ì¶œë ¥"""
        duration = datetime.now() - self.start_time
        duration_minutes = duration.total_seconds() / 60

        logger.info("\n" + "="*60)
        logger.info("            ìµœì¢… ë©€í‹°ìŠ¤ë ˆë”© í¬ë¡¤ë§ í†µê³„")
        logger.info("="*60)
        logger.info(f"ì‹¤í–‰ ì‹œê°„      : {duration_minutes:.1f}ë¶„")
        logger.info(f"ì›Œì»¤ ìŠ¤ë ˆë“œ    : {self.worker_threads}ê°œ")
        logger.info(f"ì´ ì²˜ë¦¬        : {self.total_processed:,}ê°œ")
        logger.info(f"ì„±ê³µ          : {self.total_successful:,}ê°œ")
        logger.info(f"ì‹¤íŒ¨          : {self.total_failed:,}ê°œ")

        if self.total_processed > 0:
            success_rate = self.total_successful / self.total_processed * 100
            pages_per_minute = self.total_processed / duration_minutes if duration_minutes > 0 else 0

            logger.info(f"ì„±ê³µë¥         : {success_rate:.1f}%")
            logger.info(f"ì²˜ë¦¬ì†ë„      : {pages_per_minute:.1f} pages/ë¶„")
            logger.info(f"ì²˜ë¦¬ì†ë„      : {pages_per_minute/60:.2f} pages/ì´ˆ")

        # DB í†µê³„
        if self.db_manager:
            db_count = self.db_manager.get_crawled_count()
            logger.info(f"DB ì €ì¥       : {db_count:,}ê°œ")
            pool_stats = self.db_manager.get_pool_stats()
            logger.info(f"DB í’€ (ìµœëŒ€)  : {pool_stats.get('pool_max', 0)}")

        logger.info("="*60)

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ë©€í‹°ìŠ¤ë ˆë”© í¬ë¡¤ëŸ¬ ë¡œê·¸ëŠ” multithreaded_crawler.log íŒŒì¼ì— ê¸°ë¡ë©ë‹ˆë‹¤.")
    print("ì‹¤ì‹œê°„ ë¡œê·¸ë¥¼ ë³´ë ¤ë©´ 'tail -f multithreaded_crawler.log' ëª…ë ¹ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
    import argparse

    parser = argparse.ArgumentParser(description='Multithreaded Enterprise Crawler')
    parser.add_argument('--count', type=int, default=1000,
                       help='í¬ë¡¤ë§í•  URL ê°œìˆ˜ (ê¸°ë³¸: 1000)')
    parser.add_argument('--batch-size', type=int, default=BATCH_SIZE,
                       help=f'ì›Œì»¤ë³„ ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: {BATCH_SIZE})')
    parser.add_argument('--workers', type=int, default=WORKER_THREADS,
                       help=f'ì›Œì»¤ ìŠ¤ë ˆë“œ ìˆ˜ (ê¸°ë³¸: {WORKER_THREADS})')

    args = parser.parse_args()

    # í¬ë¡¤ëŸ¬ ìƒì„±
    crawler = MultithreadedCrawler(
        initial_url_count=args.count,
        worker_threads=args.workers
    )
    crawler.batch_size = args.batch_size

    try:
        # ì´ˆê¸°í™”
        if not await crawler.initialize():
            logger.error("í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì‹¤íŒ¨")
            sys.exit(1)

        # URL ë°ì´í„°ì…‹ ì¤€ë¹„
        if not await crawler.prepare_url_dataset():
            logger.error("URL ë°ì´í„°ì…‹ ì¤€ë¹„ ì‹¤íŒ¨")
            sys.exit(1)

        # ì—°ì† í¬ë¡¤ë§ ì‹¤í–‰
        await crawler.run_continuous()

    except KeyboardInterrupt:
        logger.info("ì‚¬ìš©ìì— ì˜í•œ ì¤‘ë‹¨")

    except Exception as e:
        logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")

    finally:
        # ì •ë¦¬
        crawler.print_final_stats()
        await crawler.cleanup()

if __name__ == "__main__":
    asyncio.run(main())