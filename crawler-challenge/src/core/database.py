import psycopg2
import psycopg2.extras
from psycopg2 import Error
from urllib.parse import urlparse
import json
from typing import List, Dict, Any
import logging

logger = logging.getLogger(__name__)

import psycopg2
import psycopg2.extras
from psycopg2 import pool
from urllib.parse import urlparse
import json
from typing import List, Dict, Any
import logging

logger = logging.getLogger(__name__)

class DatabaseManager:
    _pool = None

    def __init__(self, min_conn=1, max_conn=10, host="localhost", port="5432", 
                 database="crawler_db", user="postgres", password="postgres"):
        if not DatabaseManager._pool:
            self.connection_params = {
                'host': host,
                'port': port,
                'database': database,
                'user': user,
                'password': password
            }
            try:
                DatabaseManager._pool = pool.ThreadedConnectionPool(
                    min_conn, max_conn, **self.connection_params
                )
                logger.info(f"DB Ïª§ÎÑ•ÏÖò ÌíÄ ÏÉùÏÑ± ÏÑ±Í≥µ (min: {min_conn}, max: {max_conn})")
            except psycopg2.Error as e:
                logger.error(f"DB Ïª§ÎÑ•ÏÖò ÌíÄ ÏÉùÏÑ± Ïã§Ìå®: {e}")
                raise
        
        self.batch_size = 1000
        self.batch_buffer = []

    def get_connection(self):
        """Ïª§ÎÑ•ÏÖò ÌíÄÏóêÏÑú Ïª§ÎÑ•ÏÖò Í∞ÄÏ†∏Ïò§Í∏∞"""
        try:
            return self._pool.getconn()
        except psycopg2.Error as e:
            logger.error(f"Ïª§ÎÑ•ÏÖò ÌíÄÏóêÏÑú Ïª§ÎÑ•ÏÖò Í∞ÄÏ†∏Ïò§Í∏∞ Ïã§Ìå®: {e}")
            raise

    def release_connection(self, conn):
        """Ïª§ÎÑ•ÏÖòÏùÑ ÌíÄÏóê Î∞òÌôò"""
        if conn:
            self._pool.putconn(conn)

    def get_pool_stats(self) -> Dict[str, int]:
        """Ïª§ÎÑ•ÏÖò ÌíÄ ÏÉÅÌÉú Ï°∞Ìöå"""
        if self._pool:
            return {
                'pool_min': self._pool.minconn,
                'pool_max': self._pool.maxconn,
            }
        return {}

    def close_all_connections(self):
        """Î™®Îì† Ïª§ÎÑ•ÏÖò Ï¢ÖÎ£å"""
        if self._pool:
            self._pool.closeall()
            DatabaseManager._pool = None
            logger.info("Î™®Îì† DB Ïª§ÎÑ•ÏÖò Ï¢ÖÎ£å")

    def extract_domain(self, url: str) -> str:
        """URLÏóêÏÑú ÎèÑÎ©îÏù∏ Ï∂îÏ∂ú"""
        try:
            return urlparse(url).netloc
        except:
            return "unknown"

    def extract_title_from_html(self, html_content: str) -> str:
        """HTMLÏóêÏÑú title ÌÉúÍ∑∏ Ï∂îÏ∂ú"""
        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html_content, 'html.parser')
            title_tag = soup.find('title')
            return title_tag.get_text().strip() if title_tag else ""
        except Exception as e:
            logger.warning(f"Title Ï∂îÏ∂ú Ïã§Ìå®: {e}")
            return ""

    def create_metadata(self, html_content: str, url: str) -> Dict[str, Any]:
        """Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±"""
        metadata = {
            'content_length': len(html_content),
            'url_path': urlparse(url).path,
            'has_title': bool(self.extract_title_from_html(html_content)),
        }

        # Ï∂îÍ∞Ä Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ (ÏÑ†ÌÉùÏ†Å)
        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html_content, 'html.parser')

            # Î©îÌÉÄ ÌÉúÍ∑∏ ÏàòÏßë
            meta_description = soup.find('meta', attrs={'name': 'description'})
            if meta_description:
                metadata['description'] = meta_description.get('content', '')[:500]

            # ÎßÅÌÅ¨ Ïàò
            metadata['link_count'] = len(soup.find_all('a'))

            # Ïù¥ÎØ∏ÏßÄ Ïàò
            metadata['image_count'] = len(soup.find_all('img'))

        except Exception as e:
            logger.warning(f"Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± Ï§ë Ïò§Î•ò: {e}")

        return metadata

    def add_to_batch(self, url: str, html_content: str):
        """Î∞∞ÏπòÏóê Îç∞Ïù¥ÌÑ∞ Ï∂îÍ∞Ä"""
        # Ïù¥ÎØ∏ Î∞∞ÏπòÏóê ÏûàÎäî URLÏù∏ÏßÄ ÌôïÏù∏ (Ï§ëÎ≥µ Î∞©ÏßÄ)
        if any(item['url'] == url for item in self.batch_buffer):
            logger.warning(f"Duplicate URL in batch, skipping: {url}")
            return

        domain = self.extract_domain(url)
        title = self.extract_title_from_html(html_content)
        metadata = self.create_metadata(html_content, url)

        # ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú (BeautifulSoup ÏÇ¨Ïö©)
        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html_content, 'html.parser')
            # Ïä§ÌÅ¨Î¶ΩÌä∏ÏôÄ Ïä§ÌÉÄÏùº ÌÉúÍ∑∏ Ï†úÍ±∞
            for script in soup(["script", "style"]):
                script.decompose()
            content_text = soup.get_text()
            # Í≥µÎ∞± Ï†ïÎ¶¨
            content_text = ' '.join(content_text.split())
        except Exception as e:
            logger.warning(f"ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú Ïã§Ìå®: {e}")
            content_text = ""

        if title:
            title = title.replace('\x00', '')
        if content_text:
            content_text = content_text.replace('\x00', '')
        if metadata:
            metadata = metadata.replace('\x00', '')

        self.batch_buffer.append({
            'url': url,
            'domain': domain,
            'title': title,
            'content_text': content_text,
            'metadata': json.dumps(metadata)
        })

        # Î∞∞Ïπò ÌÅ¨Í∏∞Ïóê ÎèÑÎã¨ÌïòÎ©¥ ÏûêÎèôÏúºÎ°ú ÌîåÎü¨Ïãú
        if len(self.batch_buffer) >= self.batch_size:
            self.flush_batch()

    def flush_batch(self):
        """Î∞∞Ïπò Îç∞Ïù¥ÌÑ∞Î•º DBÏóê Ï†ÄÏû•"""
        if not self.batch_buffer:
            return

        conn = self.get_connection()
        try:
            with conn.cursor() as cursor:
                insert_query = """
                    INSERT INTO crawled_pages (url, domain, title, content_text, metadata)
                    VALUES %s
                    ON CONFLICT (url) DO UPDATE SET
                        title = EXCLUDED.title,
                        content_text = EXCLUDED.content_text,
                        metadata = EXCLUDED.metadata,
                        updated_at = CURRENT_TIMESTAMP
                """
                values = [(
                    item['url'], item['domain'], item['title'],
                    item['content_text'], item['metadata']
                ) for item in self.batch_buffer]

                psycopg2.extras.execute_values(cursor, insert_query, values, page_size=100)
                conn.commit()
                logger.info(f"{len(self.batch_buffer)}Í∞ú Î†àÏΩîÎìú Î∞∞Ïπò Ï†ÄÏû• ÏôÑÎ£å")
                self.batch_buffer.clear()
        except psycopg2.Error as e:
            logger.error(f"Î∞∞Ïπò Ï†ÄÏû• Ïã§Ìå® (Îç∞Ïù¥ÌÑ∞ Ïú†Ïã§ Í∞ÄÎä•ÏÑ± ÏûàÏùå): {e}")
            conn.rollback()
            # üö® Ï§ëÏöî: ÏóêÎü¨Í∞Ä ÎÇòÎèÑ Î≤ÑÌçºÎ•º ÎπÑÏõåÏïº Îã§Ïùå Îç∞Ïù¥ÌÑ∞Î•º Î∞õÏùÑ Ïàò ÏûàÏùå!
            # (Ïö¥ÏòÅ ÌôòÍ≤ΩÏóêÏÑúÎäî Ïó¨Í∏∞ÏÑú Ïã§Ìå®Ìïú Îç∞Ïù¥ÌÑ∞Î•º Î≥ÑÎèÑ ÌååÏùºÎ°ú ÎπºÎäî Í≤å Ï¢ãÏùå)
        finally:
            # ÏÑ±Í≥µÌïòÎì† Ïã§Ìå®ÌïòÎì† Î≤ÑÌçºÎäî ÎπÑÏõåÏïº Î¨¥Ìïú Î£®ÌîÑÎ•º Î∞©ÏßÄÌï®
            self.batch_buffer.clear()
            self.release_connection(conn)

    def get_crawled_count(self) -> int:
        """ÌÅ¨Î°§ÎßÅÎêú ÌéòÏù¥ÏßÄ Ïàò Ï°∞Ìöå"""
        conn = self.get_connection()
        try:
            with conn.cursor() as cursor:
                cursor.execute("SELECT COUNT(*) FROM crawled_pages")
                return cursor.fetchone()[0]
        except psycopg2.Error as e:
            logger.error(f"Ïπ¥Ïö¥Ìä∏ Ï°∞Ìöå Ïã§Ìå®: {e}")
            return 0
        finally:
            self.release_connection(conn)

    def get_domain_stats(self) -> List[Dict]:
        """ÎèÑÎ©îÏù∏Î≥Ñ ÌÜµÍ≥Ñ Ï°∞Ìöå"""
        conn = self.get_connection()
        try:
            with conn.cursor() as cursor:
                cursor.execute("""
                    SELECT domain, COUNT(*) as count,
                           AVG(LENGTH(content_text)) as avg_content_length
                    FROM crawled_pages
                    GROUP BY domain
                    ORDER BY count DESC
                """)
                results = cursor.fetchall()
                return [
                    {'domain': row[0], 'count': row[1], 'avg_content_length': float(row[2]) if row[2] else 0}
                    for row in results
                ]
        except psycopg2.Error as e:
            logger.error(f"ÎèÑÎ©îÏù∏ ÌÜµÍ≥Ñ Ï°∞Ìöå Ïã§Ìå®: {e}")
            return []
        finally:
            self.release_connection(conn)

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        # ÎÇ®ÏùÄ Î∞∞Ïπò Ï≤òÎ¶¨
        if self.batch_buffer:
            self.flush_batch()
        self.close_all_connections()