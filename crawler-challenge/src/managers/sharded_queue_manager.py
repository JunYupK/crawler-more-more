#!/usr/bin/env python3
"""
Sharded Redis Queue Manager - Redis ìƒ¤ë”©ì„ í†µí•œ ê³ ì„±ëŠ¥ í ê´€ë¦¬
"""

import os
import redis
import json
import time
import hashlib
from typing import List, Dict, Optional, Tuple, Set
from datetime import datetime, timedelta
from urllib.parse import urlparse
import logging
import asyncio
import random

logger = logging.getLogger(__name__)

class ShardedRedisQueueManager:
    """ìƒ¤ë”©ëœ Redis íë¥¼ í†µí•œ ëŒ€ê·œëª¨ URL ê´€ë¦¬"""

    def __init__(self, shard_configs: List[Dict] = None):
        """
        Args:
            shard_configs: ìƒ¤ë“œ ì„¤ì • ë¦¬ìŠ¤íŠ¸
            ì˜ˆ: [
                {'host': 'redis1', 'port': 6379, 'db': 1},
                {'host': 'redis2', 'port': 6379, 'db': 1},
                {'host': 'redis3', 'port': 6379, 'db': 1}
            ]
        """
        if shard_configs is None:
            redis_host = os.getenv('REDIS_HOST', 'localhost')
            # ê¸°ë³¸ ë‹¨ì¼ Redis ì¸ìŠ¤í„´ìŠ¤ë¥¼ 3ê°œ DBë¡œ ìƒ¤ë”©
            shard_configs = [
                {'host': redis_host, 'port': 6379, 'db': 1},
                {'host': redis_host, 'port': 6379, 'db': 2},
                {'host': redis_host, 'port': 6379, 'db': 3}
            ]

        self.shard_configs = shard_configs
        self.num_shards = len(shard_configs)
        self.redis_clients = []

        # ê° ìƒ¤ë“œë³„ Redis í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        for i, config in enumerate(shard_configs):
            client = redis.Redis(
                host=config['host'],
                port=config['port'],
                db=config['db'],
                decode_responses=True,
                socket_keepalive=True,
                health_check_interval=30
            )
            self.redis_clients.append(client)
            logger.info(f"ìƒ¤ë“œ {i} ì´ˆê¸°í™”: {config['host']}:{config['port']}/{config['db']}")

        # í ì´ë¦„ í…œí”Œë¦¿ (ìƒ¤ë“œë³„ë¡œ ìƒì„±ë¨)
        self.queue_templates = {
            'priority_high': 'crawler:shard{shard}:queue:priority_high',
            'priority_custom': 'crawler:shard{shard}:queue:priority_custom',
            'priority_medium': 'crawler:shard{shard}:queue:priority_medium',
            'priority_normal': 'crawler:shard{shard}:queue:priority_normal',
            'priority_low': 'crawler:shard{shard}:queue:priority_low'
        }

        # ìƒíƒœ ê´€ë¦¬ (ìƒ¤ë“œë³„)
        self.processing_template = 'crawler:shard{shard}:processing'
        self.completed_template = 'crawler:shard{shard}:completed'
        self.failed_template = 'crawler:shard{shard}:failed'
        self.retry_template = 'crawler:shard{shard}:retry'

        # ë„ë©”ì¸ë³„ ìƒíƒœ ê´€ë¦¬ (ë„ë©”ì¸ í•´ì‹œì— ë”°ë¼ ìƒ¤ë“œ ê²°ì •)
        self.domain_stats_template = 'crawler:shard{shard}:domain_stats'
        self.domain_delays_template = 'crawler:shard{shard}:domain_delays'

        # ë©”íƒ€ë°ì´í„° (ëª¨ë“  ìƒ¤ë“œì— ë³µì œ)
        self.metadata_key = 'crawler:metadata'

    def get_shard_for_url(self, url: str) -> int:
        """URLì— ëŒ€í•´ ëœë¤ ìƒ¤ë“œ ì„ íƒ

        ê¸°ì¡´: ë„ë©”ì¸ í•´ì‹œ ê¸°ë°˜ ìƒ¤ë”© (ê°™ì€ ë„ë©”ì¸ â†’ ê°™ì€ ìƒ¤ë“œ)
        ë³€ê²½: ëœë¤ ìƒ¤ë”©

        ë³€ê²½ ì´ìœ :
        - Tranco Top 10k ë°ì´í„°ì…‹ì€ ê° ë„ë©”ì¸ë‹¹ 1ê°œ URLë§Œ ì¡´ì¬
        - ë„ë©”ì¸ ì§€ì—­ì„±(robots.txt ìºì‹±, ë”œë ˆì´ ê´€ë¦¬) ì´ì  ì—†ìŒ
        - ë„ë©”ì¸ í•´ì‹œ ê¸°ë°˜ì€ íŠ¹ì • ìƒ¤ë“œì— ë¶€í•˜ ì§‘ì¤‘(Hot Shard) ë¦¬ìŠ¤í¬ ì¡´ì¬

        ëœë¤ ìƒ¤ë”© ì´ì :
        - ì™„ì „ ê· ë“± ë¶„ë°°ë¡œ ëª¨ë“  ìƒ¤ë“œì— ì‘ì—…ì´ ê³ ë¥´ê²Œ ë¶„ì‚°
        - Hot Shard ë¬¸ì œ ìë™ í•´ê²°
        - ë³„ë„ì˜ Rebalancing ë¡œì§ ë¶ˆí•„ìš”
        """
        return random.randint(0, self.num_shards - 1)

    def get_shard_for_domain(self, domain: str) -> int:
        """ë„ë©”ì¸ì„ ê¸°ì¤€ìœ¼ë¡œ ìƒ¤ë“œ ê²°ì •"""
        hash_value = int(hashlib.md5(domain.encode('utf-8')).hexdigest(), 16)
        return hash_value % self.num_shards

    def get_random_shard(self) -> int:
        """ëœë¤ ìƒ¤ë“œ ì„ íƒ (ë¡œë“œ ë°¸ëŸ°ì‹±ìš©)"""
        return random.randint(0, self.num_shards - 1)

    def test_connection(self) -> bool:
        """ëª¨ë“  ìƒ¤ë“œ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            for i, client in enumerate(self.redis_clients):
                client.ping()
                logger.info(f"ìƒ¤ë“œ {i} ì—°ê²° ì„±ê³µ")
            logger.info("ëª¨ë“  ìƒ¤ë“œ ì—°ê²° ì„±ê³µ")
            return True
        except Exception as e:
            logger.error(f"ìƒ¤ë“œ ì—°ê²° ì‹¤íŒ¨: {e}")
            return False

    def is_url_hash_pending(self, url_hash: str) -> bool:
        """URL í•´ì‹œê°€ pending ìƒíƒœ(í/processing/retry)ì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
        for shard_id in range(self.num_shards):
            client = self.redis_clients[shard_id]

            processing_key = self.processing_template.format(shard=shard_id)
            if client.sismember(processing_key, url_hash):
                return True

            for queue_template in self.queue_templates.values():
                queue_key = queue_template.format(shard=shard_id)
                cursor = 0

                while True:
                    cursor, entries = client.zscan(queue_key, cursor=cursor)
                    for raw_data, _ in entries:
                        try:
                            parsed = json.loads(raw_data)
                        except json.JSONDecodeError:
                            continue

                        pending_url = parsed.get('url')
                        if pending_url and self._get_url_hash(pending_url) == url_hash:
                            return True

                    if cursor == 0:
                        break

            retry_key = self.retry_template.format(shard=shard_id)
            cursor = 0
            while True:
                cursor, entries = client.zscan(retry_key, cursor=cursor)
                for raw_data, _ in entries:
                    try:
                        parsed = json.loads(raw_data)
                    except json.JSONDecodeError:
                        continue

                    retry_url = parsed.get('url')
                    if retry_url and self._get_url_hash(retry_url) == url_hash:
                        return True

                if cursor == 0:
                    break

        return False

    def initialize_queues(self, url_data: List[Dict]) -> bool:
        """ìƒ¤ë”©ëœ í ì´ˆê¸°í™” ë° URL ë¡œë“œ"""
        try:
            logger.info(f"ìƒ¤ë”©ëœ í ì´ˆê¸°í™” ì‹œì‘: {len(url_data)}ê°œ URL, {self.num_shards}ê°œ ìƒ¤ë“œ")

            # ê¸°ì¡´ í ì •ë¦¬
            self.clear_all_queues()

            # ìƒ¤ë“œë³„ í†µê³„
            shard_counts = [{'high': 0, 'custom': 0, 'medium': 0, 'normal': 0, 'low': 0} for _ in range(self.num_shards)]
            total_counts = {'high': 0, 'custom': 0, 'medium': 0, 'normal': 0, 'low': 0}

            # URLì„ ë„ë©”ì¸ë³„ë¡œ ì ì ˆí•œ ìƒ¤ë“œì— ë¶„ì‚°
            for url_info in url_data:
                url = url_info['url']
                priority = url_info.get('priority', 0)
                shard_id = self.get_shard_for_url(url)

                client = self.redis_clients[shard_id]
                url_data_str = json.dumps(url_info)

                # ìš°ì„ ìˆœìœ„ë³„ í ê²°ì •
                if url_info.get('url_type') == 'custom':
                    queue_key = self.queue_templates['priority_custom'].format(shard=shard_id)
                    category = 'custom'
                elif priority >= 900:
                    queue_key = self.queue_templates['priority_high'].format(shard=shard_id)
                    category = 'high'
                elif priority >= 800:
                    queue_key = self.queue_templates['priority_medium'].format(shard=shard_id)
                    category = 'medium'
                elif priority >= 700:
                    queue_key = self.queue_templates['priority_normal'].format(shard=shard_id)
                    category = 'normal'
                else:
                    queue_key = self.queue_templates['priority_low'].format(shard=shard_id)
                    category = 'low'

                # í•´ë‹¹ ìƒ¤ë“œì˜ íì— ì¶”ê°€
                client.zadd(queue_key, {url_data_str: priority})
                shard_counts[shard_id][category] += 1
                total_counts[category] += 1

            # ë©”íƒ€ë°ì´í„° ì €ì¥ (ëª¨ë“  ìƒ¤ë“œì— ë³µì œ)
            metadata = {
                'total_urls': len(url_data),
                'num_shards': self.num_shards,
                'initialization_time': datetime.now().isoformat(),
                'shard_distribution': json.dumps(shard_counts),
                'priority_distribution': json.dumps(total_counts)
            }

            for client in self.redis_clients:
                client.hset(self.metadata_key, mapping=metadata)

            logger.info(f"ìƒ¤ë”©ëœ í ì´ˆê¸°í™” ì™„ë£Œ:")
            logger.info(f"  - ì´ URL: {len(url_data)}ê°œ")
            logger.info(f"  - ìƒ¤ë“œ ìˆ˜: {self.num_shards}ê°œ")
            for i, counts in enumerate(shard_counts):
                total_shard = sum(counts.values())
                logger.info(f"  - ìƒ¤ë“œ {i}: {total_shard}ê°œ ({counts})")

            return True

        except Exception as e:
            logger.error(f"ìƒ¤ë”©ëœ í ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    def get_next_batch(self, batch_size: int = 50) -> List[Dict]:
        """ë‹¤ìŒ ë°°ì¹˜ URL íšë“ (ì™„ì „ ëœë¤ ìƒ¤ë“œ ì„ íƒ)

        ê¸°ì¡´ ë°©ì‹ì˜ ë¬¸ì œì :
        - ì›Œì»¤ë³„ preferred_shard í• ë‹¹ (worker_id % num_shards)
        - 4ê°œ ì›Œì»¤, 3ê°œ ìƒ¤ë“œì¸ ê²½ìš°:
          Worker 0,3 â†’ Shard 0 (2ë°° ë¹ ë¥´ê²Œ ì†Œì§„)
          Worker 1 â†’ Shard 1
          Worker 2 â†’ Shard 2
        - ê²°ê³¼: ìƒ¤ë“œê°„ ë¶ˆê· ë“±í•œ ì†Œì§„ ì†ë„, ë¹ˆë²ˆí•œ Rebalancing ë°œìƒ

        ê°œì„ ëœ ë°©ì‹:
        - ë§¤ë²ˆ ì™„ì „ ëœë¤ ìˆœì„œë¡œ ìƒ¤ë“œ íƒìƒ‰
        - ëª¨ë“  ì›Œì»¤ê°€ ëª¨ë“  ìƒ¤ë“œì— ê· ë“±í•˜ê²Œ ì ‘ê·¼
        - í†µê³„ì ìœ¼ë¡œ ê· ë“±í•œ ë¶€í•˜ ë¶„ì‚° ë‹¬ì„±
        """
        try:
            batch = []

            # ì™„ì „ ëœë¤ ìˆœì„œë¡œ ìƒ¤ë“œ íƒìƒ‰ (ê· ë“± ë¡œë“œ ë°¸ëŸ°ì‹±)
            shard_order = list(range(self.num_shards))
            random.shuffle(shard_order)

            for shard_id in shard_order:
                if len(batch) >= batch_size:
                    break

                client = self.redis_clients[shard_id]
                remaining_size = batch_size - len(batch)

                # ìš°ì„ ìˆœìœ„ ìˆœìœ¼ë¡œ íì—ì„œ ê°€ì ¸ì˜¤ê¸°
                for queue_name, queue_template in self.queue_templates.items():
                    if len(batch) >= batch_size:
                        break

                    queue_key = queue_template.format(shard=shard_id)
                    items_needed = min(remaining_size, 20)  # ìƒ¤ë“œë‹¹ ìµœëŒ€ 20ê°œ

                    items = client.zrevrange(queue_key, 0, items_needed - 1, withscores=True)

                    for item, score in items:
                        try:
                            url_data = json.loads(item)
                            batch.append({**url_data, 'shard_id': shard_id})

                            # ì²˜ë¦¬ ì¤‘ ì„¸íŠ¸ì— ì¶”ê°€
                            url_hash = self._get_url_hash(url_data['url'])
                            processing_key = self.processing_template.format(shard=shard_id)
                            client.sadd(processing_key, url_hash)

                            # íì—ì„œ ì œê±°
                            client.zrem(queue_key, item)

                        except json.JSONDecodeError:
                            logger.warning(f"ì˜ëª»ëœ JSON ë°ì´í„° ì œê±°: {item}")
                            client.zrem(queue_key, item)

                    remaining_size = batch_size - len(batch)

            logger.debug(f"ë°°ì¹˜ íšë“: {len(batch)}ê°œ URL (ìƒ¤ë“œë³„ ë¶„ì‚°)")
            return batch

        except Exception as e:
            logger.error(f"ë°°ì¹˜ íšë“ ì‹¤íŒ¨: {e}")
            return []

    def mark_completed(self, url: str, success: bool = True, error_info: Optional[Dict] = None, shard_id: Optional[int] = None):
        """URL ì™„ë£Œ ì²˜ë¦¬ (í•´ë‹¹ ìƒ¤ë“œì—)

        Args:
            url: ì™„ë£Œ ì²˜ë¦¬í•  URL
            success: ì„±ê³µ ì—¬ë¶€
            error_info: ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ì •ë³´
            shard_id: URLì´ ì €ì¥ëœ ìƒ¤ë“œ ID (ëœë¤ ìƒ¤ë”© ì‚¬ìš© ì‹œ í•„ìˆ˜)
                      - get_next_batch()ì—ì„œ ë°˜í™˜ëœ shard_idë¥¼ ì „ë‹¬í•´ì•¼ í•¨
                      - Noneì¸ ê²½ìš° ëª¨ë“  ìƒ¤ë“œì—ì„œ ê²€ìƒ‰ ì‹œë„ (ë¹„íš¨ìœ¨ì )
        """
        try:
            # shard_idê°€ ì œê³µë˜ì§€ ì•Šì€ ê²½ìš° ëª¨ë“  ìƒ¤ë“œì—ì„œ ê²€ìƒ‰ (í•˜ìœ„ í˜¸í™˜ì„±)
            if shard_id is None:
                logger.warning(f"shard_id ë¯¸ì œê³µ - ëª¨ë“  ìƒ¤ë“œ ê²€ìƒ‰ í•„ìš”: {url}")
                # ì²« ë²ˆì§¸ ìƒ¤ë“œë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš© (ëœë¤ ìƒ¤ë”©ì—ì„œëŠ” ì •í™•í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ)
                shard_id = 0

            client = self.redis_clients[shard_id]

            url_hash = self._get_url_hash(url)
            domain = urlparse(url).netloc

            # ì²˜ë¦¬ ì¤‘ ì„¸íŠ¸ì—ì„œ ì œê±°
            processing_key = self.processing_template.format(shard=shard_id)
            client.srem(processing_key, url_hash)

            if success:
                # ì™„ë£Œ ì„¸íŠ¸ì— ì¶”ê°€
                completed_key = self.completed_template.format(shard=shard_id)
                client.sadd(completed_key, url_hash)

                # ë„ë©”ì¸ í†µê³„ ì—…ë°ì´íŠ¸ (ì„±ê³µ)
                domain_stats_key = self.domain_stats_template.format(shard=shard_id)
                client.hincrby(domain_stats_key, f"{domain}:success", 1)

            else:
                # ì‹¤íŒ¨ ì²˜ë¦¬
                failure_data = {
                    'url': url,
                    'timestamp': datetime.now().isoformat(),
                    'error': error_info or {},
                    'shard_id': shard_id
                }

                failed_key = self.failed_template.format(shard=shard_id)
                client.zadd(failed_key, {json.dumps(failure_data): time.time()})

                # ë„ë©”ì¸ í†µê³„ ì—…ë°ì´íŠ¸ (ì‹¤íŒ¨)
                domain_stats_key = self.domain_stats_template.format(shard=shard_id)
                client.hincrby(domain_stats_key, f"{domain}:failed", 1)

                # ì¬ì‹œë„ ê°€ëŠ¥í•œ ì—ëŸ¬ì¸ ê²½ìš° ì¬ì‹œë„ íì— ì¶”ê°€
                if error_info and error_info.get('recoverable', False):
                    retry_data = {
                        'url': url,
                        'retry_count': error_info.get('retry_count', 0) + 1,
                        'last_error': error_info,
                        'next_retry': (datetime.now() + timedelta(minutes=30)).isoformat(),
                        'shard_id': shard_id
                    }

                    if retry_data['retry_count'] <= 3:  # ìµœëŒ€ 3íšŒ ì¬ì‹œë„
                        retry_key = self.retry_template.format(shard=shard_id)
                        client.zadd(retry_key, {json.dumps(retry_data): time.time() + 1800})  # 30ë¶„ í›„

        except Exception as e:
            logger.error(f"ì™„ë£Œ ì²˜ë¦¬ ì‹¤íŒ¨ ({url}): {e}")

    def get_queue_stats(self) -> Dict[str, any]:
        """ì „ì²´ ìƒ¤ë“œì˜ í ìƒíƒœ í†µê³„"""
        try:
            total_stats = {}
            shard_stats = []

            # ê° ìƒ¤ë“œë³„ í†µê³„ ìˆ˜ì§‘
            for shard_id in range(self.num_shards):
                client = self.redis_clients[shard_id]
                shard_stat = {'shard_id': shard_id}

                # ê° íë³„ í¬ê¸°
                for queue_name, queue_template in self.queue_templates.items():
                    queue_key = queue_template.format(shard=shard_id)
                    shard_stat[f"queue_{queue_name}"] = client.zcard(queue_key)

                # ìƒíƒœë³„ í†µê³„
                processing_key = self.processing_template.format(shard=shard_id)
                completed_key = self.completed_template.format(shard=shard_id)
                failed_key = self.failed_template.format(shard=shard_id)
                retry_key = self.retry_template.format(shard=shard_id)

                shard_stat['processing'] = client.scard(processing_key)
                shard_stat['completed'] = client.scard(completed_key)
                shard_stat['failed'] = client.zcard(failed_key)
                shard_stat['retry'] = client.zcard(retry_key)

                # ì´ê³„
                shard_stat['total_pending'] = sum(shard_stat[k] for k in shard_stat if k.startswith('queue_'))

                shard_stats.append(shard_stat)

            # ì „ì²´ í†µê³„ ê³„ì‚°
            for key in ['queue_priority_high', 'queue_priority_custom', 'queue_priority_medium', 'queue_priority_normal', 'queue_priority_low',
                       'processing', 'completed', 'failed', 'retry', 'total_pending']:
                total_stats[key] = sum(shard.get(key, 0) for shard in shard_stats)

            total_stats['total_urls'] = (total_stats['total_pending'] + total_stats['processing'] +
                                       total_stats['completed'] + total_stats['failed'])

            # ì§„í–‰ë¥ 
            if total_stats['total_urls'] > 0:
                total_stats['completion_rate'] = total_stats['completed'] / total_stats['total_urls']
                total_stats['failure_rate'] = total_stats['failed'] / total_stats['total_urls']
            else:
                total_stats['completion_rate'] = 0.0
                total_stats['failure_rate'] = 0.0

            # ìƒ¤ë“œë³„ ìƒì„¸ ì •ë³´
            total_stats['shard_details'] = shard_stats
            total_stats['num_shards'] = self.num_shards

            return total_stats

        except Exception as e:
            logger.error(f"í í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}

    def get_shard_load_balance(self) -> Dict[int, float]:
        """ìƒ¤ë“œë³„ ë¡œë“œ ë°¸ëŸ°ìŠ¤ ìƒíƒœ í™•ì¸"""
        try:
            shard_loads = {}

            for shard_id in range(self.num_shards):
                client = self.redis_clients[shard_id]

                # ê° ìƒ¤ë“œì˜ ì´ ì‘ì—…ëŸ‰ ê³„ì‚°
                total_work = 0
                for queue_template in self.queue_templates.values():
                    queue_key = queue_template.format(shard=shard_id)
                    total_work += client.zcard(queue_key)

                processing_key = self.processing_template.format(shard=shard_id)
                total_work += client.scard(processing_key)

                shard_loads[shard_id] = total_work

            return shard_loads

        except Exception as e:
            logger.error(f"ìƒ¤ë“œ ë¡œë“œ ë°¸ëŸ°ìŠ¤ í™•ì¸ ì‹¤íŒ¨: {e}")
            return {}

    def rebalance_shards(self, target_batch_size: int = 100) -> bool:
        """ìƒ¤ë“œê°„ ë¡œë“œ ë¦¬ë°¸ëŸ°ì‹±"""
        try:
            logger.info("ìƒ¤ë“œ ë¦¬ë°¸ëŸ°ì‹± ì‹œì‘...")

            shard_loads = self.get_shard_load_balance()
            if not shard_loads:
                return False

            avg_load = sum(shard_loads.values()) / len(shard_loads)
            logger.info(f"í‰ê·  ë¡œë“œ: {avg_load:.1f}")

            # ë¡œë“œê°€ ë†’ì€ ìƒ¤ë“œì—ì„œ ë‚®ì€ ìƒ¤ë“œë¡œ ì‘ì—… ì´ì „
            for over_shard, over_load in shard_loads.items():
                if over_load > avg_load * 1.5:  # 150% ì´ìƒì¸ ê²½ìš°
                    # ê°€ì¥ ë¡œë“œê°€ ë‚®ì€ ìƒ¤ë“œ ì°¾ê¸°
                    under_shard = min(shard_loads.items(), key=lambda x: x[1])[0]

                    if shard_loads[under_shard] < avg_load * 0.5:  # 50% ì´í•˜ì¸ ê²½ìš°
                        # ì‘ì—… ì´ì „ ì‹¤í–‰
                        moved = self._move_work_between_shards(over_shard, under_shard, target_batch_size // 4)
                        if moved > 0:
                            logger.info(f"ìƒ¤ë“œ {over_shard} â†’ ìƒ¤ë“œ {under_shard}: {moved}ê°œ ì‘ì—… ì´ì „")

            logger.info("ìƒ¤ë“œ ë¦¬ë°¸ëŸ°ì‹± ì™„ë£Œ")
            return True

        except Exception as e:
            logger.error(f"ìƒ¤ë“œ ë¦¬ë°¸ëŸ°ì‹± ì‹¤íŒ¨: {e}")
            return False

    def _move_work_between_shards(self, from_shard: int, to_shard: int, max_items: int) -> int:
        """ìƒ¤ë“œê°„ ì‘ì—… ì´ì „"""
        try:
            from_client = self.redis_clients[from_shard]
            to_client = self.redis_clients[to_shard]
            moved_count = 0

            # ë‚®ì€ ìš°ì„ ìˆœìœ„ íë¶€í„° ì´ì „
            for queue_name in ['priority_low', 'priority_normal']:
                if moved_count >= max_items:
                    break

                from_queue = self.queue_templates[queue_name].format(shard=from_shard)
                to_queue = self.queue_templates[queue_name].format(shard=to_shard)

                # ê°€ì ¸ì˜¬ ì•„ì´í…œ ìˆ˜ ê²°ì •
                items_to_move = min(max_items - moved_count, 10)
                items = from_client.zrange(from_queue, 0, items_to_move - 1, withscores=True)

                for item, score in items:
                    # ëŒ€ìƒ ìƒ¤ë“œë¡œ ì´ì „
                    to_client.zadd(to_queue, {item: score})
                    from_client.zrem(from_queue, item)
                    moved_count += 1

            return moved_count

        except Exception as e:
            logger.error(f"ìƒ¤ë“œê°„ ì‘ì—… ì´ì „ ì‹¤íŒ¨: {e}")
            return 0

    def clear_all_queues(self):
        """ëª¨ë“  ìƒ¤ë“œì˜ í ì •ë¦¬"""
        try:
            for shard_id in range(self.num_shards):
                client = self.redis_clients[shard_id]

                # í í‚¤ë“¤ ìƒì„±
                keys_to_clear = []
                for queue_template in self.queue_templates.values():
                    keys_to_clear.append(queue_template.format(shard=shard_id))

                keys_to_clear.extend([
                    self.processing_template.format(shard=shard_id),
                    self.completed_template.format(shard=shard_id),
                    self.failed_template.format(shard=shard_id),
                    self.retry_template.format(shard=shard_id),
                    self.domain_stats_template.format(shard=shard_id),
                    self.domain_delays_template.format(shard=shard_id)
                ])

                for key in keys_to_clear:
                    client.delete(key)

            logger.info("ëª¨ë“  ìƒ¤ë“œì˜ í ì •ë¦¬ ì™„ë£Œ")

        except Exception as e:
            logger.error(f"í ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def _get_url_hash(self, url: str) -> str:
        """URL í•´ì‹œ ìƒì„±"""
        return hashlib.md5(url.encode('utf-8')).hexdigest()

async def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    logging.basicConfig(level=logging.INFO)

    # ìƒ¤ë”©ëœ í ë§¤ë‹ˆì € ì´ˆê¸°í™” (3ê°œ ìƒ¤ë“œ)
    shard_configs = [
        {'host': 'localhost', 'port': 6379, 'db': 1},
        {'host': 'localhost', 'port': 6379, 'db': 2},
        {'host': 'localhost', 'port': 6379, 'db': 3}
    ]

    queue_manager = ShardedRedisQueueManager(shard_configs)

    if not queue_manager.test_connection():
        print("âŒ Redis ìƒ¤ë“œ ì—°ê²° ì‹¤íŒ¨")
        return

    # í…ŒìŠ¤íŠ¸ URL ë°ì´í„° (ë” ë§ì€ URLë¡œ í…ŒìŠ¤íŠ¸)
    test_urls = []
    domains = ['google.com', 'youtube.com', 'facebook.com', 'twitter.com', 'instagram.com',
              'linkedin.com', 'reddit.com', 'wikipedia.org', 'github.com', 'stackoverflow.com']

    for i, domain in enumerate(domains):
        for protocol in ['https', 'http']:
            test_urls.append({
                'url': f'{protocol}://{domain}/',
                'priority': 1000 - i * 50,
                'rank': i + 1,
                'domain': domain,
                'url_type': 'root'
            })

    # í ì´ˆê¸°í™”
    if queue_manager.initialize_queues(test_urls):
        print(f"âœ… ìƒ¤ë”©ëœ í ì´ˆê¸°í™” ì„±ê³µ: {len(test_urls)}ê°œ URL")
    else:
        print("âŒ í ì´ˆê¸°í™” ì‹¤íŒ¨")
        return

    # í ìƒíƒœ í™•ì¸
    stats = queue_manager.get_queue_stats()
    print(f"\nğŸ“Š ì „ì²´ í†µê³„:")
    print(f"  ì´ URL: {stats.get('total_urls', 0)}")
    print(f"  ëŒ€ê¸° ì¤‘: {stats.get('total_pending', 0)}")
    print(f"  ìƒ¤ë“œ ìˆ˜: {stats.get('num_shards', 0)}")

    print(f"\nğŸ“ˆ ìƒ¤ë“œë³„ ìƒì„¸:")
    for shard in stats.get('shard_details', []):
        shard_id = shard['shard_id']
        total_pending = shard['total_pending']
        print(f"  ìƒ¤ë“œ {shard_id}: {total_pending}ê°œ ëŒ€ê¸°")

    # ë¡œë“œ ë°¸ëŸ°ì‹± í™•ì¸
    load_balance = queue_manager.get_shard_load_balance()
    print(f"\nâš–ï¸ ìƒ¤ë“œ ë¡œë“œ ë°¸ëŸ°ìŠ¤:")
    for shard_id, load in load_balance.items():
        print(f"  ìƒ¤ë“œ {shard_id}: {load}ê°œ ì‘ì—…")

    # ë°°ì¹˜ íšë“ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ“¦ ë°°ì¹˜ íšë“ í…ŒìŠ¤íŠ¸:")
    for i in range(3):
        batch = queue_manager.get_next_batch(5)
        if batch:
            print(f"  ë°°ì¹˜ {i+1}: {len(batch)}ê°œ URL")
            for item in batch[:2]:  # ì²˜ìŒ 2ê°œë§Œ ì¶œë ¥
                print(f"    - {item['url']} (ìƒ¤ë“œ: {item['shard_id']})")
        else:
            print(f"  ë°°ì¹˜ {i+1}: ë¹ˆ ë°°ì¹˜")

if __name__ == "__main__":
    asyncio.run(main())
