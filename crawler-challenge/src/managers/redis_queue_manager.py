import redis
import json
import time
import hashlib
from typing import List, Dict, Optional, Tuple, Set
from datetime import datetime, timedelta
from urllib.parse import urlparse
import logging
import asyncio

logger = logging.getLogger(__name__)

class RedisQueueManager:
    """Redis ê¸°ë°˜ ëŒ€ê·œëª¨ URL í ê´€ë¦¬ì"""

    def __init__(self, redis_host="localhost", redis_port=6379, redis_db=1):
        self.redis_client = redis.Redis(
            host=redis_host,
            port=redis_port,
            db=redis_db,
            decode_responses=True,
            socket_keepalive=True,
            health_check_interval=30
        )

        # í ì´ë¦„ ì •ì˜
        self.queues = {
            'priority_high': 'crawler:queue:priority_high',    # ìš°ì„ ìˆœìœ„ 1000-900
            'priority_medium': 'crawler:queue:priority_medium', # ìš°ì„ ìˆœìœ„ 899-800
            'priority_normal': 'crawler:queue:priority_normal', # ìš°ì„ ìˆœìœ„ 799-700
            'priority_low': 'crawler:queue:priority_low'       # ìš°ì„ ìˆœìœ„ 699 ì´í•˜
        }

        # ìƒíƒœ ê´€ë¦¬
        self.processing_set = 'crawler:processing'  # í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ URL
        self.completed_set = 'crawler:completed'    # ì™„ë£Œëœ URL
        self.failed_set = 'crawler:failed'          # ì‹¤íŒ¨í•œ URL
        self.retry_queue = 'crawler:retry'          # ì¬ì‹œë„ í

        # ë„ë©”ì¸ë³„ ìƒíƒœ ê´€ë¦¬
        self.domain_stats = 'crawler:domain_stats'  # ë„ë©”ì¸ë³„ í†µê³„
        self.domain_delays = 'crawler:domain_delays' # ë„ë©”ì¸ë³„ ë”œë ˆì´

        # ë©”íƒ€ë°ì´í„°
        self.metadata_key = 'crawler:metadata'

    def test_connection(self) -> bool:
        """Redis ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            self.redis_client.ping()
            logger.info("Redis í ë§¤ë‹ˆì € ì—°ê²° ì„±ê³µ")
            return True
        except Exception as e:
            logger.error(f"Redis ì—°ê²° ì‹¤íŒ¨: {e}")
            return False

    def initialize_queues(self, url_data: List[Dict]) -> bool:
        """í ì´ˆê¸°í™” ë° URL ë¡œë“œ"""
        try:
            logger.info(f"í ì´ˆê¸°í™” ì‹œì‘: {len(url_data)}ê°œ URL")

            # ê¸°ì¡´ í ì •ë¦¬
            self.clear_all_queues()

            # ìš°ì„ ìˆœìœ„ë³„ë¡œ URL ë¶„ë¥˜ ë° ì ì¬
            priority_counts = {'high': 0, 'medium': 0, 'normal': 0, 'low': 0}

            pipe = self.redis_client.pipeline()

            for url_info in url_data:
                priority = url_info.get('priority', 0)
                url_data_str = json.dumps(url_info)

                # ìš°ì„ ìˆœìœ„ë³„ í ê²°ì •
                if priority >= 900:
                    queue_key = self.queues['priority_high']
                    priority_counts['high'] += 1
                elif priority >= 800:
                    queue_key = self.queues['priority_medium']
                    priority_counts['medium'] += 1
                elif priority >= 700:
                    queue_key = self.queues['priority_normal']
                    priority_counts['normal'] += 1
                else:
                    queue_key = self.queues['priority_low']
                    priority_counts['low'] += 1

                # íì— ì¶”ê°€ (ìš°ì„ ìˆœìœ„ ì ìˆ˜ì™€ í•¨ê»˜)
                pipe.zadd(queue_key, {url_data_str: priority})

            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata = {
                'total_urls': len(url_data),
                'initialization_time': datetime.now().isoformat(),
                'priority_distribution': json.dumps(priority_counts)
            }
            pipe.hset(self.metadata_key, mapping=metadata)

            # ë°°ì¹˜ ì‹¤í–‰
            pipe.execute()

            logger.info(f"í ì´ˆê¸°í™” ì™„ë£Œ: {priority_counts}")
            return True

        except Exception as e:
            logger.error(f"í ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    def get_next_batch(self, batch_size: int = 50) -> List[Dict]:
        """ë‹¤ìŒ ë°°ì¹˜ URL íšë“ (ìš°ì„ ìˆœìœ„ ìˆœ)"""
        try:
            batch = []

            # ìš°ì„ ìˆœìœ„ ìˆœìœ¼ë¡œ íì—ì„œ ê°€ì ¸ì˜¤ê¸°
            for queue_name, queue_key in self.queues.items():
                if len(batch) >= batch_size:
                    break

                remaining_size = batch_size - len(batch)

                # ë†’ì€ ìš°ì„ ìˆœìœ„ë¶€í„° ê°€ì ¸ì˜¤ê¸°
                items = self.redis_client.zrevrange(queue_key, 0, remaining_size - 1, withscores=True)

                for item, score in items:
                    try:
                        url_data = json.loads(item)
                        batch.append(url_data)

                        # ì²˜ë¦¬ ì¤‘ ì„¸íŠ¸ì— ì¶”ê°€
                        url_hash = self._get_url_hash(url_data['url'])
                        self.redis_client.sadd(self.processing_set, url_hash)

                        # íì—ì„œ ì œê±°
                        self.redis_client.zrem(queue_key, item)

                    except json.JSONDecodeError:
                        logger.warning(f"ì˜ëª»ëœ JSON ë°ì´í„° ì œê±°: {item}")
                        self.redis_client.zrem(queue_key, item)

            logger.debug(f"ë°°ì¹˜ íšë“: {len(batch)}ê°œ URL")
            return batch

        except Exception as e:
            logger.error(f"ë°°ì¹˜ íšë“ ì‹¤íŒ¨: {e}")
            return []

    def mark_completed(self, url: str, success: bool = True, error_info: Optional[Dict] = None):
        """URL ì™„ë£Œ ì²˜ë¦¬"""
        try:
            url_hash = self._get_url_hash(url)
            domain = urlparse(url).netloc

            # ì²˜ë¦¬ ì¤‘ ì„¸íŠ¸ì—ì„œ ì œê±°
            self.redis_client.srem(self.processing_set, url_hash)

            if success:
                # ì™„ë£Œ ì„¸íŠ¸ì— ì¶”ê°€
                self.redis_client.sadd(self.completed_set, url_hash)

                # ë„ë©”ì¸ í†µê³„ ì—…ë°ì´íŠ¸ (ì„±ê³µ)
                self.redis_client.hincrby(self.domain_stats, f"{domain}:success", 1)

            else:
                # ì‹¤íŒ¨ ì²˜ë¦¬
                failure_data = {
                    'url': url,
                    'timestamp': datetime.now().isoformat(),
                    'error': error_info or {}
                }

                self.redis_client.zadd(self.failed_set, {json.dumps(failure_data): time.time()})

                # ë„ë©”ì¸ í†µê³„ ì—…ë°ì´íŠ¸ (ì‹¤íŒ¨)
                self.redis_client.hincrby(self.domain_stats, f"{domain}:failed", 1)

                # ì¬ì‹œë„ ê°€ëŠ¥í•œ ì—ëŸ¬ì¸ ê²½ìš° ì¬ì‹œë„ íì— ì¶”ê°€
                if error_info and error_info.get('recoverable', False):
                    retry_data = {
                        'url': url,
                        'retry_count': error_info.get('retry_count', 0) + 1,
                        'last_error': error_info,
                        'next_retry': (datetime.now() + timedelta(minutes=30)).isoformat()
                    }

                    if retry_data['retry_count'] <= 3:  # ìµœëŒ€ 3íšŒ ì¬ì‹œë„
                        self.redis_client.zadd(self.retry_queue,
                                             {json.dumps(retry_data): time.time() + 1800})  # 30ë¶„ í›„

        except Exception as e:
            logger.error(f"ì™„ë£Œ ì²˜ë¦¬ ì‹¤íŒ¨ ({url}): {e}")

    def get_retry_batch(self, batch_size: int = 20) -> List[Dict]:
        """ì¬ì‹œë„í•  URL ë°°ì¹˜ íšë“"""
        try:
            current_time = time.time()
            items = self.redis_client.zrangebyscore(self.retry_queue, 0, current_time,
                                                   start=0, num=batch_size, withscores=True)

            retry_batch = []
            for item, score in items:
                try:
                    retry_data = json.loads(item)
                    retry_batch.append({
                        'url': retry_data['url'],
                        'domain': urlparse(retry_data['url']).netloc,
                        'retry_count': retry_data['retry_count'],
                        'priority': 500,  # ì¬ì‹œë„ëŠ” ì¤‘ê°„ ìš°ì„ ìˆœìœ„
                        'url_type': 'retry'
                    })

                    # ì¬ì‹œë„ íì—ì„œ ì œê±°
                    self.redis_client.zrem(self.retry_queue, item)

                except json.JSONDecodeError:
                    logger.warning(f"ì˜ëª»ëœ ì¬ì‹œë„ ë°ì´í„° ì œê±°: {item}")
                    self.redis_client.zrem(self.retry_queue, item)

            if retry_batch:
                logger.info(f"ì¬ì‹œë„ ë°°ì¹˜ íšë“: {len(retry_batch)}ê°œ")

            return retry_batch

        except Exception as e:
            logger.error(f"ì¬ì‹œë„ ë°°ì¹˜ íšë“ ì‹¤íŒ¨: {e}")
            return []

    def get_queue_stats(self) -> Dict[str, any]:
        """í ìƒíƒœ í†µê³„"""
        try:
            stats = {}

            # ê° íë³„ í¬ê¸°
            for queue_name, queue_key in self.queues.items():
                stats[f"queue_{queue_name}"] = self.redis_client.zcard(queue_key)

            # ìƒíƒœë³„ í†µê³„
            stats['processing'] = self.redis_client.scard(self.processing_set)
            stats['completed'] = self.redis_client.scard(self.completed_set)
            stats['failed'] = self.redis_client.zcard(self.failed_set)
            stats['retry'] = self.redis_client.zcard(self.retry_queue)

            # ì´ê³„
            stats['total_pending'] = sum(stats[k] for k in stats if k.startswith('queue_'))
            stats['total_urls'] = (stats['total_pending'] + stats['processing'] +
                                 stats['completed'] + stats['failed'])

            # ì§„í–‰ë¥ 
            if stats['total_urls'] > 0:
                stats['completion_rate'] = stats['completed'] / stats['total_urls']
                stats['failure_rate'] = stats['failed'] / stats['total_urls']
            else:
                stats['completion_rate'] = 0.0
                stats['failure_rate'] = 0.0

            # ë©”íƒ€ë°ì´í„°
            metadata = self.redis_client.hgetall(self.metadata_key)
            if metadata:
                stats['metadata'] = metadata

            return stats

        except Exception as e:
            logger.error(f"í í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}

    def get_domain_stats(self, limit: int = 20) -> List[Dict]:
        """ë„ë©”ì¸ë³„ í†µê³„ (ìƒìœ„ Nê°œ)"""
        try:
            all_stats = self.redis_client.hgetall(self.domain_stats)

            # ë„ë©”ì¸ë³„ ì§‘ê³„
            domain_summary = {}
            for key, value in all_stats.items():
                if ':' in key:
                    domain, stat_type = key.rsplit(':', 1)
                    if domain not in domain_summary:
                        domain_summary[domain] = {'success': 0, 'failed': 0}
                    domain_summary[domain][stat_type] = int(value)

            # ì´ ì²˜ë¦¬ëŸ‰ ê¸°ì¤€ ì •ë ¬
            sorted_domains = sorted(
                domain_summary.items(),
                key=lambda x: x[1]['success'] + x[1]['failed'],
                reverse=True
            )

            result = []
            for domain, stats in sorted_domains[:limit]:
                total = stats['success'] + stats['failed']
                result.append({
                    'domain': domain,
                    'success': stats['success'],
                    'failed': stats['failed'],
                    'total': total,
                    'success_rate': stats['success'] / total if total > 0 else 0.0
                })

            return result

        except Exception as e:
            logger.error(f"ë„ë©”ì¸ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    def add_urls_to_queue(self, urls: List[Dict]) -> bool:
        """ê¸°ì¡´ íì— URL ì¶”ê°€"""
        try:
            logger.info(f"íì— URL ì¶”ê°€: {len(urls)}ê°œ")

            pipe = self.redis_client.pipeline()

            for url_info in urls:
                # ì´ë¯¸ ì²˜ë¦¬ëœ URLì¸ì§€ í™•ì¸
                url_hash = self._get_url_hash(url_info['url'])
                if (self.redis_client.sismember(self.completed_set, url_hash) or
                    self.redis_client.sismember(self.processing_set, url_hash)):
                    continue

                priority = url_info.get('priority', 0)
                url_data_str = json.dumps(url_info)

                # ìš°ì„ ìˆœìœ„ë³„ í ê²°ì •
                if priority >= 900:
                    queue_key = self.queues['priority_high']
                elif priority >= 800:
                    queue_key = self.queues['priority_medium']
                elif priority >= 700:
                    queue_key = self.queues['priority_normal']
                else:
                    queue_key = self.queues['priority_low']

                pipe.zadd(queue_key, {url_data_str: priority})

            pipe.execute()
            return True

        except Exception as e:
            logger.error(f"URL ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return False

    def clear_all_queues(self):
        """ëª¨ë“  í ì •ë¦¬"""
        try:
            all_keys = (
                list(self.queues.values()) +
                [self.processing_set, self.completed_set, self.failed_set,
                 self.retry_queue, self.domain_stats, self.domain_delays]
            )

            for key in all_keys:
                self.redis_client.delete(key)

            logger.info("ëª¨ë“  í ì •ë¦¬ ì™„ë£Œ")

        except Exception as e:
            logger.error(f"í ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def _get_url_hash(self, url: str) -> str:
        """URL í•´ì‹œ ìƒì„±"""
        return hashlib.md5(url.encode('utf-8')).hexdigest()

    def set_domain_delay(self, domain: str, delay_seconds: int):
        """ë„ë©”ì¸ë³„ ë”œë ˆì´ ì„¤ì •"""
        try:
            self.redis_client.hset(self.domain_delays, domain, delay_seconds)
        except Exception as e:
            logger.error(f"ë„ë©”ì¸ ë”œë ˆì´ ì„¤ì • ì‹¤íŒ¨ ({domain}): {e}")

    def get_domain_delay(self, domain: str) -> int:
        """ë„ë©”ì¸ë³„ ë”œë ˆì´ ì¡°íšŒ"""
        try:
            delay = self.redis_client.hget(self.domain_delays, domain)
            return int(delay) if delay else 1  # ê¸°ë³¸ 1ì´ˆ
        except Exception as e:
            logger.error(f"ë„ë©”ì¸ ë”œë ˆì´ ì¡°íšŒ ì‹¤íŒ¨ ({domain}): {e}")
            return 1

    def cleanup_old_data(self, hours_old: int = 24):
        """ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬"""
        try:
            cutoff_time = time.time() - (hours_old * 3600)

            # ì˜¤ë˜ëœ ì‹¤íŒ¨ ê¸°ë¡ ì œê±°
            removed_failed = self.redis_client.zremrangebyscore(self.failed_set, 0, cutoff_time)

            # ì˜¤ë˜ëœ ì¬ì‹œë„ ê¸°ë¡ ì œê±° (ì‹œê°„ì´ ì§€ë‚œ ê²ƒë“¤)
            removed_retry = self.redis_client.zremrangebyscore(self.retry_queue, 0, cutoff_time - 3600)

            logger.info(f"ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬: ì‹¤íŒ¨ {removed_failed}ê°œ, ì¬ì‹œë„ {removed_retry}ê°œ")

        except Exception as e:
            logger.error(f"ë°ì´í„° ì •ë¦¬ ì‹¤íŒ¨: {e}")

async def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    logging.basicConfig(level=logging.INFO)

    # í ë§¤ë‹ˆì € ì´ˆê¸°í™”
    queue_manager = RedisQueueManager()

    if not queue_manager.test_connection():
        print("âŒ Redis ì—°ê²° ì‹¤íŒ¨")
        return

    # í…ŒìŠ¤íŠ¸ URL ë°ì´í„°
    test_urls = [
        {'url': 'https://google.com/', 'priority': 1000, 'rank': 1, 'domain': 'google.com', 'url_type': 'root'},
        {'url': 'https://youtube.com/', 'priority': 950, 'rank': 2, 'domain': 'youtube.com', 'url_type': 'root'},
        {'url': 'https://facebook.com/', 'priority': 900, 'rank': 3, 'domain': 'facebook.com', 'url_type': 'root'},
        {'url': 'https://twitter.com/', 'priority': 850, 'rank': 4, 'domain': 'twitter.com', 'url_type': 'root'},
        {'url': 'https://instagram.com/', 'priority': 800, 'rank': 5, 'domain': 'instagram.com', 'url_type': 'root'},
    ]

    # í ì´ˆê¸°í™”
    if queue_manager.initialize_queues(test_urls):
        print(f"âœ… í ì´ˆê¸°í™” ì„±ê³µ: {len(test_urls)}ê°œ URL")
    else:
        print("âŒ í ì´ˆê¸°í™” ì‹¤íŒ¨")
        return

    # í ìƒíƒœ í™•ì¸
    stats = queue_manager.get_queue_stats()
    print(f"\nğŸ“Š í í†µê³„:")
    for key, value in stats.items():
        if not key.startswith('metadata'):
            print(f"  {key}: {value}")

    # ë°°ì¹˜ íšë“ í…ŒìŠ¤íŠ¸
    batch = queue_manager.get_next_batch(3)
    print(f"\nğŸ“¦ ë°°ì¹˜ íšë“: {len(batch)}ê°œ")
    for i, url_info in enumerate(batch, 1):
        print(f"  {i}. {url_info['url']} (ìš°ì„ ìˆœìœ„: {url_info['priority']})")

    # ì™„ë£Œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
    if batch:
        queue_manager.mark_completed(batch[0]['url'], success=True)
        print(f"âœ… ì™„ë£Œ ì²˜ë¦¬: {batch[0]['url']}")

    # ìµœì¢… í†µê³„
    final_stats = queue_manager.get_queue_stats()
    print(f"\nğŸ“Š ìµœì¢… í†µê³„:")
    print(f"  ëŒ€ê¸° ì¤‘: {final_stats['total_pending']}")
    print(f"  ì²˜ë¦¬ ì¤‘: {final_stats['processing']}")
    print(f"  ì™„ë£Œ: {final_stats['completed']}")
    print(f"  ì™„ë£Œìœ¨: {final_stats['completion_rate']:.1%}")

if __name__ == "__main__":
    asyncio.run(main())