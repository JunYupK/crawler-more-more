#!/usr/bin/env python3
"""
í¬ë¡¤ë§ ì œì–´ API ì„œë²„
- í¬ë¡¤ë§ ì‹œì‘/ì¤‘ì§€/ìƒíƒœì¡°íšŒ
- worker ìˆ˜, URL ìˆ˜ ë™ì  ì§€ì •
"""

from fastapi import FastAPI, BackgroundTasks, HTTPException
from pydantic import BaseModel
import asyncio
from typing import Optional
import subprocess
import time
import os

app = FastAPI(title="Crawler Control API", version="1.0.0")


# ============================================
# ìƒíƒœ ê´€ë¦¬
# ============================================
class CrawlerState:
    def __init__(self):
        self.is_running = False
        self.workers = 0
        self.target_urls = 0
        self.processed_urls = 0
        self.success_count = 0
        self.fail_count = 0
        self.start_time = None
        self.process = None

crawler_state = CrawlerState()


# ============================================
# ìš”ì²­/ì‘ë‹µ ëª¨ë¸
# ============================================
class CrawlRequest(BaseModel):
    workers: int = 4
    url_count: int = 10000

class CrawlStatus(BaseModel):
    is_running: bool
    workers: int
    target_urls: int
    processed_urls: int
    success_count: int
    fail_count: int
    progress_percent: float
    elapsed_seconds: Optional[float]
    pages_per_second: Optional[float]


# ============================================
# ì—”ë“œí¬ì¸íŠ¸
# ============================================
@app.get("/")
def root():
    return {
        "status": "ready",
        "message": "Crawler Control API",
        "endpoints": {
            "start": "POST /crawl/start",
            "status": "GET /crawl/status",
            "stop": "POST /crawl/stop"
        }
    }


@app.get("/health")
def health_check():
    return {"status": "healthy", "timestamp": time.time()}


@app.post("/crawl/start")
async def start_crawl(request: CrawlRequest, background_tasks: BackgroundTasks):
    """í¬ë¡¤ë§ ì‹œì‘"""
    if crawler_state.is_running:
        raise HTTPException(
            status_code=400,
            detail="í¬ë¡¤ë§ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. ë¨¼ì € /crawl/stopì„ í˜¸ì¶œí•˜ì„¸ìš”."
        )

    background_tasks.add_task(
        run_crawler,
        request.workers,
        request.url_count
    )

    return {
        "status": "started",
        "config": {
            "workers": request.workers,
            "url_count": request.url_count
        }
    }


@app.get("/crawl/status")
def get_status() -> CrawlStatus:
    """í¬ë¡¤ë§ ìƒíƒœ ì¡°íšŒ"""
    elapsed = None
    pps = None

    if crawler_state.start_time:
        elapsed = time.time() - crawler_state.start_time
        if elapsed > 0 and crawler_state.processed_urls > 0:
            pps = round(crawler_state.processed_urls / elapsed, 2)

    progress = 0.0
    if crawler_state.target_urls > 0:
        progress = (crawler_state.processed_urls / crawler_state.target_urls) * 100

    return CrawlStatus(
        is_running=crawler_state.is_running,
        workers=crawler_state.workers,
        target_urls=crawler_state.target_urls,
        processed_urls=crawler_state.processed_urls,
        success_count=crawler_state.success_count,
        fail_count=crawler_state.fail_count,
        progress_percent=round(progress, 2),
        elapsed_seconds=round(elapsed, 1) if elapsed else None,
        pages_per_second=pps
    )


@app.post("/crawl/stop")
async def stop_crawl():
    """í¬ë¡¤ë§ ì¤‘ì§€"""
    if not crawler_state.is_running:
        raise HTTPException(
            status_code=400,
            detail="ì‹¤í–‰ ì¤‘ì¸ í¬ë¡¤ë§ì´ ì—†ìŠµë‹ˆë‹¤."
        )

    if crawler_state.process:
        crawler_state.process.terminate()
        crawler_state.process.wait(timeout=10)

    crawler_state.is_running = False

    return {
        "status": "stopped",
        "summary": {
            "processed": crawler_state.processed_urls,
            "success": crawler_state.success_count,
            "failed": crawler_state.fail_count
        }
    }


# ============================================
# í¬ë¡¤ë§ ì‹¤í–‰ ë¡œì§
# ============================================
async def run_crawler(workers: int, url_count: int):
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰"""

    # ìƒíƒœ ì´ˆê¸°í™”
    crawler_state.is_running = True
    crawler_state.workers = workers
    crawler_state.target_urls = url_count
    crawler_state.processed_urls = 0
    crawler_state.success_count = 0
    crawler_state.fail_count = 0
    crawler_state.start_time = time.time()

    try:
        # sharded_master.py ì‹¤í–‰
        cmd = [
            "python", "-u", "runners/sharded_master.py",
            "--count", str(url_count),
            "--workers", str(workers)
        ]

        print(f"[API] í¬ë¡¤ë§ ì‹œì‘: {' '.join(cmd)}")

        crawler_state.process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1
        )

        # ì¶œë ¥ ëª¨ë‹ˆí„°ë§ (ì§„í–‰ ìƒí™© íŒŒì‹±)
        for line in crawler_state.process.stdout:
            print(f"[Crawler] {line.strip()}")
            # TODO: ì§„í–‰ ìƒí™© íŒŒì‹±í•´ì„œ crawler_state ì—…ë°ì´íŠ¸
            # ì˜ˆ: "Processed: 100/10000" ê°™ì€ í˜•ì‹ íŒŒì‹±

        crawler_state.process.wait()

        print(f"[API] í¬ë¡¤ë§ ì™„ë£Œ, ì¢…ë£Œ ì½”ë“œ: {crawler_state.process.returncode}")

    except Exception as e:
        print(f"[API] í¬ë¡¤ë§ ì—ëŸ¬: {e}")
    finally:
        crawler_state.is_running = False
        crawler_state.process = None

        # ë¦¬í¬íŠ¸ ìë™ ìƒì„±
        await generate_report()


async def generate_report():
    """í¬ë¡¤ë§ ì™„ë£Œ í›„ ë¦¬í¬íŠ¸ ìë™ ìƒì„±"""
    try:
        elapsed = time.time() - crawler_state.start_time if crawler_state.start_time else 0

        print(f"[API] ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        print(f"      ì²˜ë¦¬: {crawler_state.processed_urls}")
        print(f"      ì„±ê³µ: {crawler_state.success_count}")
        print(f"      ì‹¤íŒ¨: {crawler_state.fail_count}")
        print(f"      ì†Œìš”: {elapsed:.1f}ì´ˆ")

        # work_loggerê°€ ìˆë‹¤ë©´ ì‹¤í–‰
        if os.path.exists("src/utils/work_logger.py"):
            subprocess.run([
                "python", "src/utils/work_logger.py",
                "--title", f"í¬ë¡¤ë§ ì™„ë£Œ: {crawler_state.target_urls} URLs",
                "--processed", str(crawler_state.processed_urls),
                "--elapsed", str(int(elapsed))
            ], check=False)

    except Exception as e:
        print(f"[API] ë¦¬í¬íŠ¸ ìƒì„± ì—ëŸ¬: {e}")


# ============================================
# ë©”ì¸ ì‹¤í–‰
# ============================================
if __name__ == "__main__":
    import uvicorn

    print("=" * 50)
    print("ğŸš€ Crawler Control API ì„œë²„ ì‹œì‘")
    print("=" * 50)
    print("Endpoints:")
    print("  GET  /          - API ì •ë³´")
    print("  GET  /health    - í—¬ìŠ¤ì²´í¬")
    print("  POST /crawl/start - í¬ë¡¤ë§ ì‹œì‘")
    print("  GET  /crawl/status - ìƒíƒœ ì¡°íšŒ")
    print("  POST /crawl/stop  - í¬ë¡¤ë§ ì¤‘ì§€")
    print("=" * 50)

    uvicorn.run(app, host="0.0.0.0", port=8080)
