#!/usr/bin/env python3
"""
ì—”í„°í”„ë¼ì´ì¦ˆ í¬ë¡¤ëŸ¬ 1,000ê°œ ì‚¬ì´íŠ¸ í…ŒìŠ¤íŠ¸
- Tranco Top 1M ë°ì´í„° í™œìš©
- ì •ì¤‘í•œ í¬ë¡¤ë§ ì •ì±… ì ìš©
- Redis í ê´€ë¦¬
- ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§
"""

import asyncio
import time
from typing import List, Dict
import logging
from datetime import datetime

from test_tranco import create_sample_tranco_data
from tranco_manager import TrancoManager
from polite_crawler import PoliteCrawler
from work_logger import WorkLogger

logger = logging.getLogger(__name__)


class EnterpriseCrawlerTest:
    """ì—”í„°í”„ë¼ì´ì¦ˆ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.tranco_manager = TrancoManager()
        self.work_logger = WorkLogger()
        self.test_results = []

    async def prepare_test_urls(self, count: int = 1000) -> List[Dict]:
        """í…ŒìŠ¤íŠ¸ìš© URL ì¤€ë¹„"""
        print(f"[PREP] {count}ê°œ í…ŒìŠ¤íŠ¸ URL ì¤€ë¹„ ì¤‘...")

        # 1. ìƒ˜í”Œ Tranco ë°ì´í„° ìƒì„± (ì‹¤ì œ ìƒìœ„ ì‚¬ì´íŠ¸ë“¤)
        create_sample_tranco_data()

        # 2. URL íŒŒì‹± ë° ìš°ì„ ìˆœìœ„ í• ë‹¹
        urls = self.tranco_manager.parse_csv_to_urls(limit=count, add_www=False)

        if not urls:
            print("[ERROR] URL ì¤€ë¹„ ì‹¤íŒ¨")
            return []

        print(f"[OK] {len(urls)}ê°œ URL ì¤€ë¹„ ì™„ë£Œ")

        # 3. ìš°ì„ ìˆœìœ„ë³„ ë¶„í¬ í™•ì¸
        priority_count = {}
        for url_info in urls:
            priority = url_info['priority']
            priority_count[priority] = priority_count.get(priority, 0) + 1

        print("[STATS] ìš°ì„ ìˆœìœ„ë³„ ë¶„í¬:")
        for priority in sorted(priority_count.keys(), reverse=True):
            print(f"  ìš°ì„ ìˆœìœ„ {priority}: {priority_count[priority]}ê°œ")

        return urls

    async def run_crawl_test(self, urls: List[Dict], concurrent_limit: int = 25) -> List[Dict]:
        """ì‹¤ì œ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print(f"[START] {len(urls)}ê°œ URL í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print(f"  ë™ì‹œ ìš”ì²­ ì œí•œ: {concurrent_limit}ê°œ")
        print(f"  robots.txt ì¤€ìˆ˜: í™œì„±í™”")
        print(f"  ë„ë©”ì¸ë³„ ë”œë ˆì´: í™œì„±í™”")

        start_time = time.time()
        results = []

        async with PoliteCrawler() as crawler:
            # ë™ì‹œ ì‹¤í–‰ ì œí•œì„ ìœ„í•œ ì„¸ë§ˆí¬ì–´
            semaphore = asyncio.Semaphore(concurrent_limit)

            async def crawl_single_url(url_info: Dict) -> Dict:
                """ë‹¨ì¼ URL í¬ë¡¤ë§"""
                async with semaphore:
                    url = url_info['url']
                    crawl_start = time.time()

                    try:
                        result = await crawler.fetch_url_politely(url)

                        # ê²°ê³¼ì— ì¶”ê°€ ì •ë³´ í¬í•¨
                        result.update({
                            'rank': url_info.get('rank', 999999),
                            'priority': url_info.get('priority', 600),
                            'url_type': url_info.get('url_type', 'unknown'),
                            'crawl_duration': time.time() - crawl_start
                        })

                        return result

                    except Exception as e:
                        return {
                            'url': url,
                            'success': False,
                            'error': str(e),
                            'rank': url_info.get('rank', 999999),
                            'priority': url_info.get('priority', 600),
                            'crawl_duration': time.time() - crawl_start
                        }

            # ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§
            async def progress_monitor():
                """ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§"""
                while len(results) < len(urls):
                    await asyncio.sleep(5)  # 5ì´ˆë§ˆë‹¤ ì²´í¬

                    completed = len(results)
                    if completed > 0:
                        elapsed = time.time() - start_time
                        rate = completed / elapsed
                        eta = (len(urls) - completed) / rate if rate > 0 else 0

                        successful = sum(1 for r in results if r.get('success', False))
                        success_rate = (successful / completed) * 100

                        print(f"[PROGRESS] {completed}/{len(urls)} ì™„ë£Œ "
                              f"({success_rate:.1f}% ì„±ê³µ) "
                              f"- {rate:.2f} req/sec "
                              f"- ETA: {eta/60:.1f}ë¶„")

            # ëª¨ë‹ˆí„°ë§ íƒœìŠ¤í¬ ì‹œì‘
            monitor_task = asyncio.create_task(progress_monitor())

            try:
                # ëª¨ë“  URLì— ëŒ€í•´ í¬ë¡¤ë§ íƒœìŠ¤í¬ ìƒì„±
                tasks = [crawl_single_url(url_info) for url_info in urls]

                # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì‹¤í–‰ (ë©”ëª¨ë¦¬ ê´€ë¦¬)
                batch_size = 100
                for i in range(0, len(tasks), batch_size):
                    batch = tasks[i:i + batch_size]
                    batch_results = await asyncio.gather(*batch, return_exceptions=True)

                    for result in batch_results:
                        if isinstance(result, Exception):
                            results.append({
                                'url': 'unknown',
                                'success': False,
                                'error': str(result),
                                'crawl_duration': 0
                            })
                        else:
                            results.append(result)

                    print(f"[BATCH] {min(i + batch_size, len(urls))}/{len(urls)} ë°°ì¹˜ ì™„ë£Œ")

            finally:
                monitor_task.cancel()

        total_time = time.time() - start_time
        print(f"[COMPLETE] í¬ë¡¤ë§ ì™„ë£Œ - ì´ ì†Œìš”ì‹œê°„: {total_time:.1f}ì´ˆ")

        return results

    def analyze_results(self, results: List[Dict]) -> Dict:
        """ê²°ê³¼ ë¶„ì„"""
        successful = [r for r in results if r.get('success', False)]
        failed = [r for r in results if not r.get('success', False)]

        # ê¸°ë³¸ í†µê³„
        stats = {
            'total_requests': len(results),
            'successful_requests': len(successful),
            'failed_requests': len(failed),
            'success_rate': (len(successful) / len(results)) * 100 if results else 0,
            'total_time': sum(r.get('crawl_duration', 0) for r in results),
            'average_response_time': 0,
        }

        if successful:
            stats['average_response_time'] = sum(r.get('crawl_duration', 0) for r in successful) / len(successful)

        # ë„ë©”ì¸ë³„ í†µê³„
        domain_stats = {}
        for result in results:
            domain = result.get('domain', 'unknown')
            if domain not in domain_stats:
                domain_stats[domain] = {'total': 0, 'success': 0, 'failed': 0}

            domain_stats[domain]['total'] += 1
            if result.get('success', False):
                domain_stats[domain]['success'] += 1
            else:
                domain_stats[domain]['failed'] += 1

        # ìƒìœ„ ì„±ê³µ/ì‹¤íŒ¨ ë„ë©”ì¸
        top_success_domains = sorted(
            domain_stats.items(),
            key=lambda x: x[1]['success'],
            reverse=True
        )[:10]

        top_failed_domains = sorted(
            domain_stats.items(),
            key=lambda x: x[1]['failed'],
            reverse=True
        )[:10]

        # ìš°ì„ ìˆœìœ„ë³„ ì„±ê³µë¥ 
        priority_stats = {}
        for result in results:
            priority = result.get('priority', 600)
            if priority not in priority_stats:
                priority_stats[priority] = {'total': 0, 'success': 0}

            priority_stats[priority]['total'] += 1
            if result.get('success', False):
                priority_stats[priority]['success'] += 1

        # ì˜¤ë¥˜ ìœ í˜•ë³„ í†µê³„
        error_types = {}
        for result in failed:
            error = result.get('error', 'Unknown')
            error_type = error.split(':')[0] if ':' in error else error
            error_types[error_type] = error_types.get(error_type, 0) + 1

        return {
            'basic_stats': stats,
            'domain_stats': domain_stats,
            'top_success_domains': top_success_domains,
            'top_failed_domains': top_failed_domains,
            'priority_stats': priority_stats,
            'error_types': error_types
        }

    def print_analysis(self, analysis: Dict):
        """ë¶„ì„ ê²°ê³¼ ì¶œë ¥"""
        stats = analysis['basic_stats']

        print(f"\n[ANALYSIS] í¬ë¡¤ë§ ê²°ê³¼ ë¶„ì„")
        print(f"=" * 50)

        print(f"ğŸ“Š ê¸°ë³¸ í†µê³„:")
        print(f"  ì´ ìš”ì²­: {stats['total_requests']:,}")
        print(f"  ì„±ê³µ: {stats['successful_requests']:,}")
        print(f"  ì‹¤íŒ¨: {stats['failed_requests']:,}")
        print(f"  ì„±ê³µë¥ : {stats['success_rate']:.1f}%")
        print(f"  í‰ê·  ì‘ë‹µì‹œê°„: {stats['average_response_time']:.2f}ì´ˆ")

        print(f"\nğŸ† ìƒìœ„ ì„±ê³µ ë„ë©”ì¸:")
        for domain, domain_stat in analysis['top_success_domains'][:5]:
            success_rate = (domain_stat['success'] / domain_stat['total']) * 100
            print(f"  {domain}: {domain_stat['success']}/{domain_stat['total']} ({success_rate:.1f}%)")

        print(f"\nâŒ ìƒìœ„ ì‹¤íŒ¨ ë„ë©”ì¸:")
        for domain, domain_stat in analysis['top_failed_domains'][:5]:
            if domain_stat['failed'] > 0:
                print(f"  {domain}: {domain_stat['failed']} ì‹¤íŒ¨")

        print(f"\nğŸ¯ ìš°ì„ ìˆœìœ„ë³„ ì„±ê³µë¥ :")
        for priority in sorted(analysis['priority_stats'].keys(), reverse=True):
            pstat = analysis['priority_stats'][priority]
            success_rate = (pstat['success'] / pstat['total']) * 100
            print(f"  ìš°ì„ ìˆœìœ„ {priority}: {success_rate:.1f}% ({pstat['success']}/{pstat['total']})")

        print(f"\nâš ï¸ ì˜¤ë¥˜ ìœ í˜•ë³„ í†µê³„:")
        for error_type, count in sorted(analysis['error_types'].items(), key=lambda x: x[1], reverse=True)[:5]:
            print(f"  {error_type}: {count}íšŒ")

    async def run_full_test(self, url_count: int = 1000):
        """ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print(f"[ENTERPRISE] {url_count}ê°œ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ì‹œì‘")

        # 1. URL ì¤€ë¹„
        urls = await self.prepare_test_urls(url_count)
        if not urls:
            return False

        # 2. í¬ë¡¤ë§ ì‹¤í–‰
        results = await self.run_crawl_test(urls, concurrent_limit=25)

        # 3. ê²°ê³¼ ë¶„ì„
        analysis = self.analyze_results(results)
        self.print_analysis(analysis)

        # 4. ê²°ê³¼ ì €ì¥ ë° ë¡œê¹…
        self.test_results = results

        success_rate = analysis['basic_stats']['success_rate']
        avg_response_time = analysis['basic_stats']['average_response_time']

        self.work_logger.log_and_commit(
            title=f"ì—”í„°í”„ë¼ì´ì¦ˆ í¬ë¡¤ëŸ¬ {url_count}ê°œ ì‚¬ì´íŠ¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ",
            description=f"Tranco ìƒìœ„ ì‚¬ì´íŠ¸ {url_count}ê°œë¥¼ ëŒ€ìƒìœ¼ë¡œ ì •ì¤‘í•œ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.",
            details={
                "ì´ ìš”ì²­": f"{len(results):,}",
                "ì„±ê³µë¥ ": f"{success_rate:.1f}%",
                "í‰ê·  ì‘ë‹µì‹œê°„": f"{avg_response_time:.2f}ì´ˆ",
                "ë„ë©”ì¸ ìˆ˜": len(analysis['domain_stats']),
                "robots.txt": "ì¤€ìˆ˜",
                "ë™ì‹œ ìš”ì²­": "25ê°œ ì œí•œ",
                "ìƒíƒœ": "í…ŒìŠ¤íŠ¸ ì™„ë£Œ"
            }
        )

        print(f"\n[SUCCESS] ì—”í„°í”„ë¼ì´ì¦ˆ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"ì„±ê³µë¥ : {success_rate:.1f}%, í‰ê·  ì‘ë‹µì‹œê°„: {avg_response_time:.2f}ì´ˆ")

        return True


async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )

    test_system = EnterpriseCrawlerTest()

    try:
        success = await test_system.run_full_test(url_count=1000)
        return success
    except Exception as e:
        print(f"[ERROR] í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return False


if __name__ == "__main__":
    success = asyncio.run(main())
    if success:
        print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    else:
        print("\nğŸ’¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨!")