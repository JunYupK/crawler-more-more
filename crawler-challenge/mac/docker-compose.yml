version: '3.8'

services:
  # 1. Infrastructure Services
  redis:
    image: redis:7-alpine
    container_name: crawler-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: always

  # 2. Crawler Services
  crawler-master:
    ports:
      - "8000:8000"
    build:
      context: ..
      dockerfile: docker/Dockerfile
    container_name: crawler-master
    # 대화형 입력 활성화 (start 명령 대기)
    stdin_open: true  # docker run -i
    tty: true         # docker run -t
    # 실제 실행 명령어 (--auto-start 없으면 사용자 입력 대기)
    command: python runners/sharded_master.py --count 1000000 --workers 8
    environment:
      # 같은 도커 네트워크라 서비스명(redis)으로 접속 가능
      - REDIS_HOST=redis
      - KAFKA_SERVERS=${KAFKA_SERVERS:-host.docker.internal:9092}
      # AI 리포트 생성용 (Prometheus는 Docker 호스트에서 접근)
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - PROMETHEUS_URL=http://100.105.22.101:9090
    volumes:
      - ../logs:/app/logs
      - ../docs/reports:/app/docs/reports
      - ../.env:/app/.env:ro
    depends_on:
      redis:
        condition: service_started
    restart: "no"  # 사용자 입력 대기 모드에서는 자동 재시작 비활성화

  crawler-worker-1:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    container_name: crawler-worker-1
    command: python runners/sharded_worker.py --worker-id 1
    ports:
      - "8001:8001"
    environment:
      - REDIS_HOST=redis
      - KAFKA_SERVERS=${KAFKA_SERVERS:-host.docker.internal:9092}
    volumes:
      - ../logs:/app/logs
    depends_on:
      crawler-master:
        condition: service_started
    restart: always

  crawler-worker-2:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    container_name: crawler-worker-2
    command: python runners/sharded_worker.py --worker-id 2
    ports:
      - "8002:8002"
    environment:
      - REDIS_HOST=redis
      - KAFKA_SERVERS=${KAFKA_SERVERS:-host.docker.internal:9092}
    volumes:
      - ../logs:/app/logs
    depends_on:
      crawler-master:
        condition: service_started
    restart: always

  crawler-worker-3:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    container_name: crawler-worker-3
    command: python runners/sharded_worker.py --worker-id 3
    ports:
      - "8003:8003"
    environment:
      - REDIS_HOST=redis
      - KAFKA_SERVERS=${KAFKA_SERVERS:-host.docker.internal:9092}
    volumes:
      - ../logs:/app/logs
    depends_on:
      crawler-master:
        condition: service_started
    restart: always

  crawler-worker-4:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    container_name: crawler-worker-4
    command: python runners/sharded_worker.py --worker-id 4
    ports:
      - "8004:8004"
    environment:
      - REDIS_HOST=redis
      - KAFKA_SERVERS=${KAFKA_SERVERS:-host.docker.internal:9092}
    volumes:
      - ../logs:/app/logs
    depends_on:
      crawler-master:
        condition: service_started
    restart: always

  crawler-worker-5:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    container_name: crawler-worker-5
    command: python runners/sharded_worker.py --worker-id 5
    ports:
      - "8005:8005"
    environment:
      - REDIS_HOST=redis
      - KAFKA_SERVERS=${KAFKA_SERVERS:-host.docker.internal:9092}
    volumes:
      - ../logs:/app/logs
    depends_on:
      crawler-master:
        condition: service_started
    restart: always

  crawler-worker-6:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    container_name: crawler-worker-6
    command: python runners/sharded_worker.py --worker-id 6
    ports:
      - "8006:8006"
    environment:
      - REDIS_HOST=redis
      - KAFKA_SERVERS=${KAFKA_SERVERS:-host.docker.internal:9092}
    volumes:
      - ../logs:/app/logs
    depends_on:
      crawler-master:
        condition: service_started
    restart: always

  crawler-worker-7:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    container_name: crawler-worker-7
    command: python runners/sharded_worker.py --worker-id 7
    ports:
      - "8007:8007"
    environment:
      - REDIS_HOST=redis
      - KAFKA_SERVERS=${KAFKA_SERVERS:-host.docker.internal:9092}
    volumes:
      - ../logs:/app/logs
    depends_on:
      crawler-master:
        condition: service_started
    restart: always

  crawler-worker-8:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    container_name: crawler-worker-8
    command: python runners/sharded_worker.py --worker-id 8
    ports:
      - "8008:8008"
    environment:
      - REDIS_HOST=redis
      - KAFKA_SERVERS=${KAFKA_SERVERS:-host.docker.internal:9092}
    volumes:
      - ../logs:/app/logs
    depends_on:
      crawler-master:
        condition: service_started
    restart: always

volumes:
  redis_data:
